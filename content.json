{"meta":{"title":"Enmin`s blog","subtitle":null,"description":null,"author":"Enmin","url":"https://zem12345678.github.io"},"pages":[{"title":"about","date":"2018-10-16T08:50:22.000Z","updated":"2018-10-19T03:00:32.812Z","comments":true,"path":"about/index.html","permalink":"https://zem12345678.github.io/about/index.html","excerpt":"","text":"技术爱玩发与部署环境为 DockerPython 3.6.3前端 Vue + Webpack + ES2015 + axios后端 Django 2.0 + DjangoRestFramework + Celery自动化部署选用工具 Ansible 以及 Docker，K8s后端组件ElasticSearch 用于搜索和推荐PostgreSQL 用于数据持久化Mysql/Mangodb，用于数据存储Redis 缓存RabbitMQ 分布式队列 / 定时任务Nginx 用于反向代理如果你也是追新的 Django 开发者，一起来提 PR"}],"posts":[{"title":"urllib库的使用","slug":"urllib库的使用","date":"2019-05-16T03:50:23.174Z","updated":"2019-05-16T03:51:17.288Z","comments":true,"path":"2019/05/16/urllib库的使用/","link":"","permalink":"https://zem12345678.github.io/2019/05/16/urllib库的使用/","excerpt":"","text":"urllib库的使用所谓网页抓取，就是把URL地址中指定的网络资源从网络流中读取出来，保存到本地。 在Python中有很多库可以用来抓取网页，我们先学习urllib。 在 python2 中，urllib 被分为urllib,urllib2等 urlopen我们先来段代码： 12345678910111213# urllib_request.py# 导入urllib.request 库import urllib.request# 向指定的url发送请求，并返回服务器响应的类文件对象response = urllib.request.urlopen(&quot;http://www.baidu.com&quot;)# 类文件对象支持文件对象的操作方法，如read()方法读取文件全部内容，返回字符串html = response.read()# 打印字符串print (html) 执行写的python代码，将打印结果 Power@PowerMac ~$: python urllib_request.py 实际上，如果我们在浏览器上打开百度主页， 右键选择“查看源代码”，你会发现，跟我们刚才打印出来的是一模一样。也就是说，上面的4行代码就已经帮我们把百度的首页的全部代码爬了下来。 一个基本的url请求对应的python代码真的非常简单。 Request在我们第一个例子里，urlopen()的参数就是一个url地址； 但是如果需要执行更复杂的操作，比如增加HTTP报头，必须创建一个 Request 实例来作为urlopen()的参数；而需要访问的url地址则作为 Request 实例的参数。 我们编辑urllib_request.py12345678910111213# urllib_request.pyimport urllib.request# url 作为Request()方法的参数，构造并返回一个Request对象request = urllib.request.Request(&quot;http://www.baidu.com&quot;)# Request对象作为urlopen()方法的参数，发送给服务器并接收响应response = urllib.request.urlopen(request)html = response.read().decode()print (html) 运行结果是完全一样的： 新建Request实例，除了必须要有 url 参数之外，还可以设置另外两个参数： data（默认空）：是伴随 url 提交的数据（比如要post的数据），同时 HTTP 请求将从 “GET”方式 改为 “POST”方式。 headers（默认空）：是一个字典，包含了需要发送的HTTP报头的键值对。 这两个参数下面会说到。 User-Agent但是这样直接用urllib给一个网站发送请求的话，确实略有些唐突了，就好比，人家每家都有门，你以一个路人的身份直接闯进去显然不是很礼貌。而且有一些站点不喜欢被程序（非人为访问）访问，有可能会拒绝你的访问请求。 但是如果我们用一个合法的身份去请求别人网站，显然人家就是欢迎的，所以我们就应该给我们的这个代码加上一个身份，就是所谓的User-Agent头。 浏览器 就是互联网世界上公认被允许的身份，如果我们希望我们的爬虫程序更像一个真实用户，那我们第一步，就是需要伪装成一个被公认的浏览器。用不同的浏览器在发送请求的时候，会有不同的User-Agent头。 urllib默认的User-Agent头为：Python-urllib/x.y（x和y是Python主版本和次版本号,例如 Python-urllib/2.7） 1234567891011121314151617#urllib_request.pyimport urllib.requesturl = &quot;http://www.itcast.cn&quot;#IE 9.0 的 User-Agent，包含在 ua_header里ua_header = &#123;&quot;User-Agent&quot; : &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;&quot;&#125;# url 连同 headers，一起构造Request请求，这个请求将附带 IE9.0 浏览器的User-Agentrequest = urllib.request.Request(url, headers = ua_header)# 向服务器发送这个请求response = urllib.request.urlopen(request)html = response.read()print (html) 添加更多的Header信息在 HTTP Request 中加入特定的 Header，来构造一个完整的HTTP请求消息。 可以通过调用Request.add_header() 添加/修改一个特定的header 也可以通过调用Request.get_header()来查看已有的header。 添加一个特定的header12345678910111213141516171819202122# urllib_headers.pyimport urllib.requesturl = &quot;http://www.itcast.cn&quot;#IE 9.0 的 User-Agentheader = &#123;&quot;User-Agent&quot; : &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;&quot;&#125;request = urllib.request.Request(url, headers = header)#也可以通过调用Request.add_header() 添加/修改一个特定的headerrequest.add_header(&quot;Connection&quot;, &quot;keep-alive&quot;)# 也可以通过调用Request.get_header()来查看header信息# request.get_header(header_name=&quot;Connection&quot;)response = urllib.request.urlopen(request)print (response.code) #可以查看响应状态码html = response.read().decode()print (html) 随机添加/修改User-Agent12345678910111213141516171819202122232425262728# urllib_add_headers.pyimport urllibimport randomurl = &quot;http://www.itcast.cn&quot;ua_list = [ &quot;Mozilla/5.0 (Windows NT 6.1; ) Apple.... &quot;, &quot;Mozilla/5.0 (X11; CrOS i686 2268.111.0)... &quot;, &quot;Mozilla/5.0 (Macintosh; U; PPC Mac OS X.... &quot;, &quot;Mozilla/5.0 (Macintosh; Intel Mac OS... &quot;]user_agent = random.choice(ua_list)request = urllib.request.Request(url)#也可以通过调用Request.add_header() 添加/修改一个特定的headerrequest.add_header(&quot;User-Agent&quot;, user_agent)# get_header()的字符串参数，第一个字母大写，后面的全部小写request.get_header(&quot;User-agent&quot;)response = urllib.request.urlopen(requestr)html = response.read()print (html) urllib默认只支持HTTP/HTTPS的GET和POST方法urllib.parse.urlencode() 编码工作使用urllib.parse的urlencode()函数，帮我们将key:value这样的键值对转换成”key=value”这样的字符串，解码工作可以使用urllib.parse的unquote()函数。 123456789101112# IPython3 中的测试结果In [1]: import urllib.parseIn [2]: word = &#123;&quot;wd&quot; : &quot;人生苦短&quot;&#125;# 通过urllib.urlencode()方法，将字典键值对按URL编码转换，从而能被web服务器接受。In [3]: urllib.parse.urlencode(word) Out[3]: &quot;wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2&quot;# 通过urllib.unquote()方法，把 URL编码字符串，转换回原先字符串。In [4]: print urllib.parse.unquote(&quot;wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2&quot;)wd=人生苦短 一般HTTP请求提交数据，需要编码成 URL编码格式，然后做为url的一部分，或者作为参数传到Request对象中。 Get方式GET请求一般用于我们向服务器获取数据，比如说，我们用百度搜索传智播客：https://www.baidu.com/s?wd=传智播客 浏览器的url会跳转成如图所示:https://www.baidu.com/s?wd=%E4%BC%A0%E6%99%BA%E6%92%AD%E5%AE%A2 在其中我们可以看到在请求部分里，http://www.baidu.com/s? 之后出现一个长长的字符串，其中就包含我们要查询的关键词传智播客，于是我们可以尝试用默认的Get方式来发送请求。1234567891011121314# urllib_get.pyurl = &quot;http://www.baidu.com/s&quot;word = &#123;&quot;wd&quot;:&quot;传智播客&quot;&#125;word = urllib.parse.urlencode(word) #转换成url编码格式（字符串）newurl = url + &quot;?&quot; + word # url首个分隔符就是 ?headers=&#123; &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36&quot;&#125;request = urllib.request.Request(newurl, headers=headers)response = urllib.request.urlopen(request)print (response.read()) 批量爬取贴吧页面数据首先我们创建一个python文件, tiebaSpider.py，我们要完成的是，输入一个百度贴吧的地址，比如： 百度贴吧LOL吧第一页：http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=0 第二页： http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=50 第三页： http://tieba.baidu.com/f?kw=lol&amp;ie=utf-8&amp;pn=100 发现规律了吧，贴吧中每个页面不同之处，就是url最后的pn的值，其余的都是一样的，我们可以抓住这个规律。 简单写一个小爬虫程序，来爬取百度LOL吧的所有网页。*先写一个main，提示用户输入要爬取的贴吧名，并用urllib.urlencode()进行转码，然后组合url，假设是lol吧，那么组合后的url就是：http://tieba.baidu.com/f?kw=lol1234567891011121314# 模拟 main 函数if __name__ == &quot;__main__&quot;: kw = raw_input(&quot;请输入需要爬取的贴吧:&quot;) # 输入起始页和终止页，str转成int类型 beginPage = int(raw_input(&quot;请输入起始页：&quot;)) endPage = int(raw_input(&quot;请输入终止页：&quot;)) url = &quot;http://tieba.baidu.com/f?&quot; key = urllib.parse.urlencode(&#123;&quot;kw&quot; : kw&#125;) # 组合后的url示例：http://tieba.baidu.com/f?kw=lol url = url + key tiebaSpider(url, beginPage, endPage) 接下来，我们写一个百度贴吧爬虫接口，我们需要传递3个参数给这个接口， 一个是main里组合的url地址，以及起始页码和终止页码，表示要爬取页码的范围。1234567891011121314151617181920def tiebaSpider(url, beginPage, endPage): &quot;&quot;&quot; 作用：负责处理url，分配每个url去发送请求 url：需要处理的第一个url beginPage: 爬虫执行的起始页面 endPage: 爬虫执行的截止页面 &quot;&quot;&quot; for page in range(beginPage, endPage + 1): pn = (page - 1) * 50 filename = &quot;第&quot; + str(page) + &quot;页.html&quot; # 组合为完整的 url，并且pn值每次增加50 fullurl = url + &quot;&amp;pn=&quot; + str(pn) #print fullurl # 调用loadPage()发送请求获取HTML页面 html = loadPage(fullurl, filename) # 将获取到的HTML页面写入本地磁盘文件 writeFile(html, filename) 我们已经之前写出一个爬取一个网页的代码。现在，我们可以将它封装成一个小函数loadPage，供我们使用。12345678910111213def loadPage(url, filename): &apos;&apos;&apos; 作用：根据url发送请求，获取服务器响应文件 url：需要爬取的url地址 filename: 文件名 &apos;&apos;&apos; print (&quot;正在下载&quot; + filename) headers = &#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;&quot;&#125; request = urllib.request.Request(url, headers = headers) response = urllib.request.urlopen(request) return response.read() 最后如果我们希望将爬取到了每页的信息存储在本地磁盘上，我们可以简单写一个存储文件的接口。12345678910def writeFile(html, filename): &quot;&quot;&quot; 作用：保存服务器响应文件到本地磁盘文件里 html: 服务器响应文件 filename: 本地磁盘文件名 &quot;&quot;&quot; print (&quot;正在存储&quot; + filename) with open(filename, &apos;w&apos;) as f: f.write(html) print &quot;-&quot; * 20 其实很多网站都是这样的，同类网站下的html页面编号，分别对应网址后的网页序号，只要发现规律就可以批量爬取页面了。 POST方式：上面我们说了Request请求对象的里有data参数，它就是用在POST里的，我们要传送的数据就是这个参数data，data是一个字典，里面要匹配键值对。 有道词典翻译网站：输入测试数据，再通过使用Fiddler观察，其中有一条是POST请求，而向服务器发送的请求数据并不是在url里，那么我们可以试着模拟这个POST请求。1234567891011121314151617181920212223import urllib# POST请求的目标URLurl = &quot;http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;smartresult=ugc&amp;sessionFrom=null&quot;headers=&#123;&quot;User-Agent&quot;: &quot;Mozilla....&quot;&#125;formdata = &#123; &quot;type&quot;:&quot;AUTO&quot;, &quot;i&quot;:&quot;i love python&quot;, &quot;doctype&quot;:&quot;json&quot;, &quot;xmlVersion&quot;:&quot;1.8&quot;, &quot;keyfrom&quot;:&quot;fanyi.web&quot;, &quot;ue&quot;:&quot;UTF-8&quot;, &quot;action&quot;:&quot;FY_BY_ENTER&quot;, &quot;typoResult&quot;:&quot;true&quot;&#125;data = urllib.parse.urlencode(formdata)request = urllib.request.Request(url, data = data, headers = headers)response = urllib.request.urlopen(request)print (response.read()) 发送POST请求时，需要特别注意headers的一些属性： Content-Length: 144： 是指发送的表单数据长度为144，也就是字符个数是144个。 Content-Type: application/x-www-form-urlencoded ： 表示浏览器提交 Web 表单时使用，表单数据会按照 name1=value1&amp;name2=value2 键值对形式进行编码。 X-Requested-With: XMLHttpRequest ：表示Ajax异步请求。 获取AJAX加载的内容有些网页内容使用AJAX加载，这种数据无法直接对网页url进行获取。只要记得，AJAX一般返回的是JSON，只要对AJAX地址进行post或get，就能返回JSON数据了。 如果非要从HTML页面里获取展现出来的数据，也不是不可以。但是要记住，作为一名爬虫工程师，你更需要关注的是数据的来源。 12345678910111213141516171819202122232425262728293031323334353637383940import urllib# demo1url = &quot;https://movie.douban.com/j/chart/top_list?type=11&amp;interval_id=100%3A90&amp;action&quot;headers=&#123;&quot;User-Agent&quot;: &quot;Mozilla....&quot;&#125;# 变动的是这两个参数，从start开始往后显示limit个formdata = &#123; &apos;start&apos;:&apos;0&apos;, &apos;limit&apos;:&apos;10&apos;&#125;data = urllib.parse.urlencode(formdata)request = urllib.request.Request(url, data = data, headers = headers)response = urllib.request.urlopen(request)print (response.read())# demo2url = &quot;https://movie.douban.com/j/chart/top_list?&quot;headers=&#123;&quot;User-Agent&quot;: &quot;Mozilla....&quot;&#125;# 处理所有参数formdata = &#123; &apos;type&apos;:&apos;11&apos;, &apos;interval_id&apos;:&apos;100:90&apos;, &apos;action&apos;:&apos;&apos;, &apos;start&apos;:&apos;0&apos;, &apos;limit&apos;:&apos;10&apos;&#125;data = urllib.parse.urlencode(formdata)request = urllib.request.Request(url, data = data, headers = headers)response = urllib.request.urlopen(request)print (response.read()) GET方式是直接以链接形式访问，链接中包含了所有的参数，服务器端用Request.QueryString获取变量的值。如果包含了密码的话是一种不安全的选择，不过你可以直观地看到自己提交了什么内容。 POST则不会在网址上显示所有的参数，服务器端用Request.Form获取提交的数据，在Form提交的时候。但是HTML代码里如果不指定 method 属性，则默认为GET请求，Form中提交的数据将会附加在url之后，以?分开与url分开。 表单数据可以作为 URL 字段（method=”get”）或者 HTTP POST （method=”post”）的方式来发送。比如在下面的HTML代码中，表单数据将因为 （method=”get”） 而附加到 URL 上： 12345&lt;form action=&quot;form_action.asp&quot; method=&quot;get&quot;&gt; &lt;p&gt;First name: &lt;input type=&quot;text&quot; name=&quot;fname&quot; /&gt;&lt;/p&gt; &lt;p&gt;Last name: &lt;input type=&quot;text&quot; name=&quot;lname&quot; /&gt;&lt;/p&gt; &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;&lt;/form&gt; 处理HTTPS请求 SSL证书验证现在随处可见 https 开头的网站，urllib可以为 HTTPS 请求验证SSL证书，就像web浏览器一样，如果网站的SSL证书是经过CA认证的，则能够正常访问，如：https://www.baidu.com/等... 如果SSL证书验证不通过，或者操作系统不信任服务器的安全证书，比如浏览器在访问12306网站如：https://www.12306.cn/mormhweb/的时候，会警告用户证书不受信任。（据说 12306 网站证书是自己做的，没有通过CA认证）urllib在访问的时候则会报出SSLError：1234567891011import urlliburl = &quot;https://www.12306.cn/mormhweb/&quot;headers = &#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;&#125;request = urllib.request.Request(url, headers = headers)response = urllib.request.urlopen(request)print (response.read()) 运行结果： URLError: 所以，如果以后遇到这种网站，我们需要单独处理SSL证书，让程序忽略SSL证书验证错误，即可正常访问。1234567891011121314151617import urllib# 1. 导入Python SSL处理模块import ssl# 2. 表示忽略未经核实的SSL证书认证context = ssl._create_unverified_context()url = &quot;https://www.12306.cn/mormhweb/&quot;headers = &#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;&#125;request = urllib.request.Request(url, headers = headers)# 3. 在urlopen()方法里 指明添加 context 参数response = urllib.request.urlopen(request, context = context)print (response.read().decode()) 关于CACA(Certificate Authority)是数字证书认证中心的简称，是指发放、管理、废除数字证书的受信任的第三方机构，如北京数字认证股份有限公司、上海市数字证书认证中心有限公司等… CA的作用是检查证书持有者身份的合法性，并签发证书，以防证书被伪造或篡改，以及对证书和密钥进行管理。 现实生活中可以用身份证来证明身份， 那么在网络世界里，数字证书就是身份证。和现实生活不同的是，并不是每个上网的用户都有数字证书的，往往只有当一个人需要证明自己的身份的时候才需要用到数字证书。 普通用户一般是不需要，因为网站并不关心是谁访问了网站，现在的网站只关心流量。但是反过来，网站就需要证明自己的身份了。 比如说现在钓鱼网站很多的，比如你想访问的是www.baidu.com，但其实你访问的是www.daibu.com”，所以在提交自己的隐私信息之前需要验证一下网站的身份，要求网站出示数字证书。 一般正常的网站都会主动出示自己的数字证书，来确保客户端和网站服务器之间的通信数据是加密安全的。 Handler处理器 和 自定义Openeropener是 urllib.request.OpenerDirector 的实例，我们之前一直都在使用的urlopen，它是一个特殊的opener（也就是模块帮我们构建好的）。 但是基本的urlopen()方法不支持代理、cookie等其他的HTTP/HTTPS高级功能。所以要支持这些功能： 使用相关的 Handler处理器 来创建特定功能的处理器对象； 然后通过 urllib.request.build_opener()方法使用这些处理器对象，创建自定义opener对象； 使用自定义的opener对象，调用open()方法发送请求。 如果程序里所有的请求都使用自定义的opener，可以使用urllib.request.install_opener() 将自定义的 opener 对象 定义为 全局opener，表示如果之后凡是调用urlopen，都将使用这个opener（根据自己的需求来选择） 简单的自定义opener()12345678910111213141516171819import urllib.request# 构建一个HTTPHandler 处理器对象，支持处理HTTP请求http_handler = urllib.request.HTTPHandler()# 构建一个HTTPHandler 处理器对象，支持处理HTTPS请求# http_handler = urllib.request.HTTPSHandler()# 调用urllib.request.build_opener()方法，创建支持处理HTTP请求的opener对象opener = urllib.request.build_opener(http_handler)# 构建 Request请求request = urllib.request.Request(&quot;http://www.baidu.com/&quot;)# 调用自定义opener对象的open()方法，发送request请求response = opener.open(request)# 获取服务器响应内容print (response.read().decode()) 这种方式发送请求得到的结果，和使用urllib.request.urlopen()发送HTTP/HTTPS请求得到的结果是一样的。 如果在 HTTPHandler()增加 debuglevel=1参数，还会将 Debug Log 打开，这样程序在执行的时候，会把收包和发包的报头在屏幕上自动打印出来，方便调试，有时可以省去抓包的工作。1234567# 仅需要修改的代码部分：# 构建一个HTTPHandler 处理器对象，支持处理HTTP请求，同时开启Debug Log，debuglevel 值默认 0http_handler = urllib.request.HTTPHandler(debuglevel=1)# 构建一个HTTPHSandler 处理器对象，支持处理HTTPS请求，同时开启Debug Log，debuglevel 值默认 0https_handler = urllib.request.HTTPSHandler(debuglevel=1) ProxyHandler处理器（代理使用代理IP，这是爬虫/反爬虫的第二大招，通常也是最好用的。很多网站会检测某一段时间某个IP的访问次数(通过流量统计，系统日志等)，如果访问次数多的不像正常人，它会禁止这个IP的访问。 所以我们可以设置一些代理服务器，每隔一段时间换一个代理，就算IP被禁止，依然可以换个IP继续爬取。 urllib.request中通过ProxyHandler来设置使用代理服务器，下面代码说明如何使用自定义opener来使用代理：设置）123456789101112131415161718192021222324252627#urllib_proxy1.pyimport urllib.request# 构建了两个代理Handler，一个有代理IP，一个没有代理IPhttpproxy_handler = urllib.request.ProxyHandler(&#123;&quot;http&quot; : &quot;124.88.67.81:80&quot;&#125;)nullproxy_handler = urllib.request.ProxyHandler(&#123;&#125;)proxySwitch = True #定义一个代理开关# 通过 urllib.request.build_opener()方法使用这些代理Handler对象，创建自定义opener对象# 根据代理开关是否打开，使用不同的代理模式if proxySwitch: opener = urllib.request.build_opener(httpproxy_handler)else: opener = urllib.request.build_opener(nullproxy_handler)request = urllib.request.Request(&quot;http://www.baidu.com/&quot;)# 1. 如果这么写，只有使用opener.open()方法发送请求才使用自定义的代理，而urlopen()则不使用自定义代理。response = opener.open(request)# 2. 如果这么写，就是将opener应用到全局，之后所有的，不管是opener.open()还是urlopen() 发送请求，都将使用自定义代理。# urllib.request.install_opener(opener)# response = urlopen(request)print (response.read().decode()) 免费的开放代理获取基本没有成本，我们可以在一些代理网站上收集这些免费代理，测试后如果可以用，就把它收集起来用在爬虫上面。 免费短期代理网站举例： 西刺免费代理IP https://www.xicidaili.com/ 快代理免费代理 https://www.kuaidaili.com/free/inha/ Proxy360代理 http://www.proxy360.cn/ 全网代IP http://www.goubanjia.com/free/index.shtml 如果代理IP足够多，就可以像随机获取User-Agent一样，随机选择一个代理去访问网站。123456789101112131415161718192021import urllib.requestimport randomproxy_list = [ &#123;&quot;http&quot; : &quot;124.88.67.81:80&quot;&#125;, &#123;&quot;http&quot; : &quot;124.88.67.81:80&quot;&#125;, &#123;&quot;http&quot; : &quot;124.88.67.81:80&quot;&#125;, &#123;&quot;http&quot; : &quot;124.88.67.81:80&quot;&#125;, &#123;&quot;http&quot; : &quot;124.88.67.81:80&quot;&#125;]# 随机选择一个代理proxy = random.choice(proxy_list)# 使用选择的代理构建代理处理器对象httpproxy_handler = urllib.request.ProxyHandler(proxy)opener = urllib.request.build_opener(httpproxy_handler)request = urllib.request.Request(&quot;http://www.baidu.com/&quot;)response = opener.open(request)print (response.read()) 但是，这些免费开放代理一般会有很多人都在使用，而且代理有寿命短，速度慢，匿名度不高，HTTP/HTTPS支持不稳定等缺点（免费没好货）。 所以，专业爬虫工程师或爬虫公司会使用高品质的私密代理，这些代理通常需要找专门的代理供应商购买，再通过用户名/密码授权使用（舍不得孩子套不到狼）。 CookieCookie 是指某些网站服务器为了辨别用户身份和进行Session跟踪，而储存在用户浏览器上的文本文件，Cookie可以保持登录信息到用户下次与服务器的会话。 Cookie原理HTTP是无状态的面向连接的协议, 为了保持连接状态, 引入了Cookie机制 Cookie是http消息头中的一种属性，包括：12345678Cookie名字（Name）Cookie的值（Value）Cookie的过期时间（Expires/Max-Age）Cookie作用路径（Path）Cookie所在域名（Domain），使用Cookie进行安全连接（Secure）。前两个参数是Cookie应用的必要条件，另外，还包括Cookie大小（Size，不同浏览器对Cookie个数及大小限制是有差异的）。 Cookie由变量名和值组成，根据 Netscape公司的规定，Cookie格式如下：1Set－Cookie: NAME=VALUE；Expires=DATE；Path=PATH；Domain=DOMAIN_NAME；SECURE Cookie应用Cookies在爬虫方面最典型的应用是判定注册用户是否已经登录网站，用户可能会得到提示，是否在下一次进入此网站时保留用户信息以便简化登录手续。12345678910111213141516171819202122232425262728# 获取一个有登录信息的Cookie模拟登陆import urllib# 1. 构建一个已经登录过的用户的headers信息headers = &#123; &quot;Host&quot;:&quot;www.renren.com&quot;, &quot;Connection&quot;:&quot;keep-alive&quot;, &quot;Upgrade-Insecure-Requests&quot;:&quot;1&quot;, &quot;User-Agent&quot;:&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;, &quot;Accept&quot;:&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&quot;, &quot;Accept-Language&quot;:&quot;zh-CN,zh;q=0.8,en;q=0.6&quot;, &quot;Referer&quot;:&quot;http://www.renren.com/SysHome.do&quot;, # 便于终端阅读，表示不支持压缩文件 # Accept-Encoding: gzip, deflate, sdch, # 重点：这个Cookie是保存了密码无需重复登录的用户的Cookie，这个Cookie里记录了用户名，密码(通常经过RAS加密) &quot;Cookie&quot;: &quot;anonymid=j3jxk555-nrn0wh; depovince=BJ; _r01_=1; JSESSIONID=abcnLjz9MSvBa-3lJK3Xv; ick=3babfba4-e0ed-4e9f-9312-8e833e4cb826; jebecookies=764bacbd-0e4a-4534-b8e8-37c10560770c|||||; ick_login=84f70f68-7ebd-4c5c-9c0f-d1d9aac778e0; _de=7A7A02E9254501DA6278B9C75EAEEB7A; p=91063de8b39ac5e0d2a57500de7e34077; first_login_flag=1; ln_uact=13146128763; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=39fca09219c06df42604435129960e1f7; societyguester=39fca09219c06df42604435129960e1f7; id=941954027; xnsid=8868df75; ver=7.0; loginfrom=null; XNESSESSIONID=a6da759fe858; WebOnLineNotice_941954027=1; wp_fold=0&quot;&#125;# 2. 通过headers里的报头信息（主要是Cookie信息），构建Request对象urllib.request.Request(&quot;http://www.renren.com/941954027#&quot;, headers = headers)# 3. 直接访问renren主页，服务器会根据headers报头信息（主要是Cookie信息），判断这是一个已经登录的用户，并返回相应的页面response = urllib.request.urlopen(request)# 4. 打印响应内容print (response.read().decode()) 但是这样做太过复杂，我们先需要在浏览器登录账户，并且设置保存密码，并且通过抓包才能获取这个Cookie，那有么有更简单方便的方法呢？ cookiejar库 和 HTTPCookieProcessor处理器在Python处理Cookie，一般是通过cookiejar模块和 urllib模块的HTTPCookieProcessor处理器类一起使用。 cookiejar模块：主要作用是提供用于存储cookie的对象 HTTPCookieProcessor处理器：主要作用是处理这些cookie对象，并构建handler对象。 cookiejar 库该模块主要的对象有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。 CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。 我们来做几个案例： 1）获取Cookie，并保存到CookieJar()对象中123456789101112131415161718192021222324# urllib_cookiejar_test1.pyimport urllibfrom http import cookiejar# 构建一个CookieJar对象实例来保存cookiecookiejar = cookiejar.CookieJar()# 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象handler=urllib.request.HTTPCookieProcessor(cookiejar)# 通过 build_opener() 来构建openeropener = urllib.request.build_opener(handler)# 4. 以get方法访问页面，访问之后会自动保存cookie到cookiejar中opener.open(&quot;http://www.baidu.com&quot;)## 可以按标准格式将保存的Cookie打印出来cookieStr = &quot;&quot;for item in cookiejar: cookieStr = cookieStr + item.name + &quot;=&quot; + item.value + &quot;;&quot;## 舍去最后一位的分号print (cookieStr[:-1]) 我们使用以上方法将Cookie保存到cookiejar对象中，然后打印出了cookie中的值，也就是访问百度首页的Cookie值。 运行结果如下：1BAIDUID=4327A58E63A92B73FF7A297FB3B2B4D0:FG=1;BIDUPSID=4327A58E63A92B73FF7A297FB3B2B4D0;H_PS_PSSID=1429_21115_17001_21454_21409_21554_21398;PSTM=1480815736;BDSVRTM=0;BD_HOME=0 利用cookiejar和post登录人人网1234567891011121314151617181920212223242526272829303132import urllibfrom http import cookiejar# 1. 构建一个CookieJar对象实例来保存cookiecookie = cookiejar.CookieJar()# 2. 使用HTTPCookieProcessor()来创建cookie处理器对象，参数为CookieJar()对象cookie_handler = urllib.request.HTTPCookieProcessor(cookie)# 3. 通过 build_opener() 来构建openeropener = urllib.request.build_opener(cookie_handler)# 4. addheaders 接受一个列表，里面每个元素都是一个headers信息的元祖, opener将附带headers信息opener.addheaders = [(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;)]# 5. 需要登录的账户和密码data = &#123;&quot;email&quot;:&quot;13****46**8763&quot;, &quot;password&quot;:&quot;****&quot;&#125; # 6. 通过urlencode()转码postdata = urllib.parse.urlencode(data).encode()# 7. 构建Request请求对象，包含需要发送的用户名和密码request = urllib.request.Request(&quot;http://www.renren.com/PLogin.do&quot;, data = postdata)# 8. 通过opener发送这个请求，并获取登录后的Cookie值，opener.open(request) # 9. opener包含用户登录后的Cookie值，可以直接访问那些登录后才可以访问的页面response = opener.open(&quot;http://www.renren.com/410043129/profile&quot;) # 10. 打印响应内容print (response.read().decode()) 模拟登录要注意几点： 登录一般都会先有一个HTTP GET，用于拉取一些信息及获得Cookie，然后再HTTP POST登录。 HTTP POST登录的链接有可能是动态的，从GET返回的信息中获取。 password 有些是明文发送，有些是加密后发送。有些网站甚至采用动态加密的，同时包括了很多其他数据的加密信息，只能通过查看JS源码获得加密算法，再去破解加密，非常困难。 大多数网站的登录整体流程是类似的，可能有些细节不一样，所以不能保证其他网站登录成功。 这个测试案例中，为了想让大家快速理解知识点，我们使用的人人网登录接口是人人网改版前的隐藏接口(嘘….)，登录比较方便。当然，我们也可以直接发送账号密码到登录界面模拟登录，但是当网页采用JavaScript动态技术以后，想封锁基于 HttpClient 的模拟登录就太容易了，甚至可以根据你的鼠标活动的特征准确地判断出是不是真人在操作。所以，想做通用的模拟登录还得选别的技术，比如用内置浏览器引擎的爬虫(关键词：Selenium ，PhantomJS)","categories":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://zem12345678.github.io/categories/数据挖掘/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zem12345678.github.io/tags/爬虫/"},{"name":"Urllib","slug":"Urllib","permalink":"https://zem12345678.github.io/tags/Urllib/"}]},{"title":"Requests的使用","slug":"Requests的使用","date":"2019-05-16T03:40:27.991Z","updated":"2019-05-16T03:49:10.317Z","comments":true,"path":"2019/05/16/Requests的使用/","link":"","permalink":"https://zem12345678.github.io/2019/05/16/Requests的使用/","excerpt":"","text":"Requests的使用Requests: 让 HTTP 服务人类虽然Python的标准库中 urllib 模块已经包含了平常我们使用的大多数功能，但是它的 API 使用起来让人感觉不太好，而 Requests 自称 “HTTP for Humans”，说明使用更简洁方便。 Requests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用：） Requests 继承了urllib的所有特性。Requests支持HTTP连接保持和连接池，支持使用cookie保持会话，支持文件上传，支持自动确定响应内容的编码，支持国际化的 URL 和 POST 数据自动编码。 requests 的底层实现其实就是 urllibRequests的文档非常完备，中文文档也相当不错。Requests能完全满足当前网络的需求，支持Python 2.6–3.5，而且能在PyPy下完美运行。 开源地址：https://github.com/kennethreitz/requests 中文文档 API： http://docs.python-requests.org/zh_CN/latest/index.html 安装方式利用 pip 安装 或者利用 easy_install 都可以完成安装：123$ pip install requests$ easy_install requests 基本GET请求（headers参数 和 parmas参数）1. 最基本的GET请求可以直接用get方法1234response = requests.get(&quot;http://www.baidu.com/&quot;)# 也可以这么写# response = requests.request(&quot;get&quot;, &quot;http://www.baidu.com/&quot;) 2. 添加 headers 和 查询参数如果想添加 headers，可以传入headers参数来增加请求头中的headers信息。如果要将参数放在url中传递，可以利用 params 参数。1234567891011121314151617181920212223import requestskw = &#123;&apos;wd&apos;:&apos;长城&apos;&#125;headers = &#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;&#125;# params 接收一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要urlencode()response = requests.get(&quot;http://www.baidu.com/s?&quot;, params = kw, headers = headers)# 查看响应内容，response.text 返回的是Unicode格式的数据print (response.text)# 查看响应内容，response.content返回的字节流数据print (respones.content)# 查看完整url地址print (response.url)# 查看响应头部字符编码print (response.encoding)# 查看响应码print (response.status_code) 运行结果123456789............&apos;http://www.baidu.com/s?wd=%E9%95%BF%E5%9F%8E&apos;&apos;utf-8&apos;200 使用response.text 时，Requests 会基于 HTTP 响应的文本编码自动解码响应内容，大多数 Unicode 字符集都能被无缝地解码。 使用response.content 时，返回的是服务器响应数据的原始二进制字节流，可以用来保存图片等二进制文件。 小栗子通过requests获取新浪首页12345#coding=utf-8import requestsresponse = requests.get(&quot;http://www.sina.com&quot;)print(response.request.headers)print(response.content.decode()) 结果12345678910&#123;&apos;User-Agent&apos;: &apos;python-requests/2.12.4&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate&apos;, &apos;Accept&apos;: &apos;*/*&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;&#125;&lt;!DOCTYPE html&gt;&lt;!-- [ published at 2017-06-09 15:15:23 ] --&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot; /&gt; &lt;title&gt;新浪首页&lt;/title&gt; &lt;meta name=&quot;keywords&quot; content=&quot;新浪,新浪网,SINA,sina,sina.com.cn,新浪首页,门户,资讯&quot; /&gt; ... 产生问题的原因分析 requests默认自带的Accept-Encoding导致或者新浪默认发送的就是压缩之后的网页 但是为什么content.read()没有问题，因为requests，自带解压压缩网页的功能 当收到一个响应时，Requests 会猜测响应的编码方式，用于在你调用response.text 方法时对响应进行解码。Requests 首先在 HTTP 头部检测是否存在指定的编码方式，如果不存在，则会使用 chardet.detect来尝试猜测编码方式（存在误差） 更推荐使用response.content.deocde()通过requests获取网络上图片的大小12345678from io import BytesIO,StringIOimport requestsfrom PIL import Imageimg_url = &quot;http://imglf1.ph.126.net/pWRxzh6FRrG2qVL3JBvrDg==/6630172763234505196.png&quot;response = requests.get(img_url)f = BytesIO(response.content)img = Image.open(f)print(img.size) 输出结果：1(500, 262) 理解一下 BytesIO 和StringIO 很多时候，数据读写不一定是文件，也可以在内存中读写。StringIO顾名思义就是在内存中读写str。BytesIO 就是在内存中读写bytes类型的二进制数据 例子中如果使用StringIO 即f = StringIO(response.text)会产生”cannot identify image file”的错误当然上述例子也可以把图片存到本地之后再使用Image打开来获取图片大小 基本POST请求（data参数）1. 最基本post方法1response = requests.post(&quot;http://www.baidu.com/&quot;, data = data) 2. 传入data数据对于 POST 请求来说，我们一般需要为它增加一些参数。那么最基本的传参方法可以利用 data 这个参数。1234567891011121314151617181920212223import requestsformdata = &#123; &quot;type&quot;:&quot;AUTO&quot;, &quot;i&quot;:&quot;i love python&quot;, &quot;doctype&quot;:&quot;json&quot;, &quot;xmlVersion&quot;:&quot;1.8&quot;, &quot;keyfrom&quot;:&quot;fanyi.web&quot;, &quot;ue&quot;:&quot;UTF-8&quot;, &quot;action&quot;:&quot;FY_BY_ENTER&quot;, &quot;typoResult&quot;:&quot;true&quot;&#125;url = &quot;http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&amp;smartresult=ugc&amp;sessionFrom=null&quot;headers=&#123; &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36&quot;&#125;response = requests.post(url, data = formdata, headers = headers)print (response.text)# 如果是json文件可以直接显示print (response.json()) 运行结果123&#123;&quot;type&quot;:&quot;EN2ZH_CN&quot;,&quot;errorCode&quot;:0,&quot;elapsedTime&quot;:3,&quot;translateResult&quot;:[[&#123;&quot;src&quot;:&quot;i love python&quot;,&quot;tgt&quot;:&quot;我喜欢python&quot;&#125;]],&quot;smartResult&quot;:&#123;&quot;type&quot;:1,&quot;entries&quot;:[&quot;&quot;,&quot;肆文&quot;,&quot;&quot;,&quot;&quot;,&quot;高德纳&quot;,&quot;&quot;,&quot;&quot;]&#125;&#125;&#123;&apos;type&apos;: &apos;EN2ZH_CN&apos;, &apos;errorCode&apos;: 0, &apos;elapsedTime&apos;: 3, &apos;translateResult&apos;: [[&#123;&apos;src&apos;: &apos;i love python&apos;, &apos;tgt&apos;: &apos;我喜欢python&apos;&#125;]], &apos;smartResult&apos;: &#123;&apos;type&apos;: 1, &apos;entries&apos;: [&apos;&apos;, &apos;肆文&apos;, &apos;&apos;, &apos;&apos;, &apos;高德纳&apos;, &apos;&apos;, &apos;&apos;]&#125;&#125; 代理（proxies参数）如果需要使用代理，你可以通过为任意请求方法提供 proxies 参数来配置单个请求：12345678910mport requests# 根据协议类型，选择不同的代理proxies = &#123; &quot;http&quot;: &quot;http://12.34.56.79:9527&quot;, &quot;https&quot;: &quot;http://12.34.56.79:9527&quot;,&#125;response = requests.get(&quot;http://www.baidu.com&quot;, proxies = proxies)print response.text 也可以通过本地环境变量 HTTP_PROXY 和 HTTPS_PROXY 来配置代理：12export HTTP_PROXY=&quot;http://12.34.56.79:9527&quot;export HTTPS_PROXY=&quot;https://12.34.56.79:9527&quot; 私密代理验证（特定格式） 和 Web客户端验证（auth 参数）私密代理12345678import requests# 如果代理需要使用HTTP Basic Auth，可以使用下面这种格式：proxy = &#123; &quot;http&quot;: &quot;mr_mao_hacker:sffqry9r@61.158.163.130:16816&quot; &#125;response = requests.get(&quot;http://www.baidu.com&quot;, proxies = proxy)print (response.text) web客户端验证如果是Web客户端验证，需要添加 auth = (账户名, 密码)1234567import requestsauth=(&apos;test&apos;, &apos;123456&apos;)response = requests.get(&apos;http://192.168.199.107&apos;, auth = auth)print (response.text) Cookies 和 SessionCookies如果一个响应中包含了cookie，那么我们可以利用 cookies参数拿到：12345678910111213import requestsresponse = requests.get(&quot;http://www.baidu.com/&quot;)# 7\\. 返回CookieJar对象:cookiejar = response.cookies# 8\\. 将CookieJar转为字典：cookiedict = requests.utils.dict_from_cookiejar(cookiejar)print (cookiejar)print (cookiedict) 运行结果：123&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;&#123;&apos;BDORZ&apos;: &apos;27315&apos;&#125; session在 requests 里，session对象是一个非常常用的对象，这个对象代表一次用户会话：从客户端浏览器连接服务器开始，到客户端浏览器与服务器断开。 会话能让我们在跨请求时候保持某些参数，比如在同一个 Session 实例发出的所有请求之间保持 cookie 。 实现人人网登录12345678910111213141516171819import requests# 1\\. 创建session对象，可以保存Cookie值ssion = requests.session()# 2\\. 处理 headersheaders = &#123;&quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;&#125;# 3\\. 需要登录的用户名和密码data = &#123;&quot;email&quot;:&quot;mr_mao_hacker@163.com&quot;, &quot;password&quot;:&quot;alarmchime&quot;&#125; # 4\\. 发送附带用户名和密码的请求，并获取登录后的Cookie值，保存在ssion里ssion.post(&quot;http://www.renren.com/PLogin.do&quot;, data = data)# 5\\. ssion包含用户登录后的Cookie值，可以直接访问那些登录后才可以访问的页面response = ssion.get(&quot;http://www.renren.com/410043129/profile&quot;)# 6\\. 打印响应内容print (response.text) 处理HTTPS请求 SSL证书验证Requests也可以为HTTPS请求验证SSL证书： 要想检查某个主机的SSL证书，你可以使用 verify 参数（也可以不写）123456import requestsresponse = requests.get(&quot;https://www.baidu.com/&quot;, verify=True)# 也可以省略不写# response = requests.get(&quot;https://www.baidu.com/&quot;)print (r.text) 运行结果：12345&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-typecontent=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatiblecontent=IE=Edge&gt;百度一下，你就知道 .... 如果SSL证书验证不通过，或者不信任服务器的安全证书，则会报出SSLError，据说 12306 证书是自己做的： 来测试一下：123import requestsresponse = requests.get(&quot;https://www.12306.cn/mormhweb/&quot;)print (response.text) 果然：1SSLError: (&quot;bad handshake: Error([(&apos;SSL routines&apos;, &apos;ssl3_get_server_certificate&apos;, &apos;certificate verify failed&apos;)],)&quot;,) 如果我们想跳过 12306 的证书验证，把 verify 设置为 False 就可以正常请求了。1r = requests.get(&quot;https://www.12306.cn/mormhweb/&quot;, verify = False)","categories":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://zem12345678.github.io/categories/数据挖掘/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zem12345678.github.io/tags/爬虫/"},{"name":"Requests","slug":"Requests","permalink":"https://zem12345678.github.io/tags/Requests/"}]},{"title":"HTTP/HTTPS抓包工具-Fiddler","slug":"HTTPS抓包工具-Fiddler","date":"2019-05-16T03:40:10.523Z","updated":"2019-05-16T03:47:34.774Z","comments":true,"path":"2019/05/16/HTTPS抓包工具-Fiddler/","link":"","permalink":"https://zem12345678.github.io/2019/05/16/HTTPS抓包工具-Fiddler/","excerpt":"","text":"HTTP/HTTPS抓包工具-FiddlerHTTP代理神器FiddlerFiddler是一款强大Web调试工具，它能记录所有客户端和服务器的HTTP请求。 Fiddler启动的时候，默认IE的代理设为了127.0.0.1:8888，而其他浏览器是需要手动设置。 工作原理Fiddler 是以代理web服务器的形式工作的，它使用代理地址：127.0.0.1，端口：8888 Fiddler抓取HTTPS设置 启动Fiddler，打开菜单栏中的 Tools &gt; Telerik Fiddler Options，打开“Fiddler Options”对话框。 对Fiddler进行设置： 打开工具栏-&gt;Tools-&gt;Fiddler Options-&gt;HTTPS，选中Capture HTTPS CONNECTs (捕捉HTTPS连接)，选中Decrypt HTTPS traffic（解密HTTPS通信）另外我们要用Fiddler获取本机所有进程的HTTPS请求，所以中间的下拉菜单中选中…from all processes （从所有进程）选中下方Ignore server certificate errors（忽略服务器证书错误） 为 Fiddler 配置Windows信任这个根证书解决安全警告：Trust Root Certificate（受信任的根证书）。 Fiddler 主菜单 Tools -&gt; Fiddler Options…-&gt; Connections 选中Allow remote computers to connect（允许远程连接）Act as system proxy on startup（作为系统启动代理） 重启Fiddler，使配置生效（这一步很重要，必须做） Fiddler 如何捕获Chrome的会话 安装SwitchyOmega 代理管理 Chrome 浏览器插件 如图所示，设置代理服务器为127.0.0.1:8888 通过浏览器插件切换为设置好的代理。Fiddler界面设置好后，本机HTTP通信都会经过127.0.0.1:8888代理，也就会被Fiddler拦截到。请求 (Request) 部分详解 Headers —— 显示客户端发送到服务器的 HTTP 请求的 header，显示为一个分级视图，包含了 Web 客户端信息、Cookie、传输状态等。 Textview —— 显示 POST 请求的 body 部分为文本。 WebForms —— 显示请求的 GET 参数 和 POST body 内容。 HexView —— 用十六进制数据显示请求。 Auth —— 显示响应 header 中的 Proxy-6. Authorization(代理身份验证) 和 Authorization(授权) 信息. Raw —— 将整个请求显示为纯文本。 JSON - 显示JSON格式文件。 XML —— 如果请求的 body 是 XML 格式，就是用分级&gt;的 XML 树来显示它。 响应 (Response) 部分详解 Transformer —— 显示响应的编码信息。 Headers —— 用分级视图显示响应的 header。 TextView —— 使用文本显示相应的 body。 ImageVies —— 如果请求是图片资源，显示响应的图片。 HexView —— 用十六进制数据显示响应。 WebView —— 响应在 Web 浏览器中的预览效果。 Auth —— 显示响应 header 中的 Proxy-Authorization(代理身份验证) 和 Authorization(授权) 信&gt;息。 Caching —— 显示此请求的缓存信息。 Privacy —— 显示此请求的私密 (P3P) 信息。 Raw —— 将整个响应显示为纯文本。 JSON - 显示JSON格式文件。 XML —— 如果响应的 body 是 XML 格式，就是用分级的 XML 树来显示它 。","categories":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://zem12345678.github.io/categories/数据挖掘/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zem12345678.github.io/tags/爬虫/"},{"name":"HTTP/HTTPS","slug":"HTTP-HTTPS","permalink":"https://zem12345678.github.io/tags/HTTP-HTTPS/"}]},{"title":"HTTP/HTTPS的请求与响应","slug":"HTTPS的请求与响应","date":"2019-05-16T03:39:46.146Z","updated":"2019-05-16T03:46:33.816Z","comments":true,"path":"2019/05/16/HTTPS的请求与响应/","link":"","permalink":"https://zem12345678.github.io/2019/05/16/HTTPS的请求与响应/","excerpt":"","text":"HTTP/HTTPS的请求与响应HTTP和HTTPSHTTP协议（HyperText Transfer Protocol，超文本传输协议）：是一种发布和接收 HTML页面的方法。 HTTPS（Hypertext Transfer Protocol over Secure Socket Layer）简单讲是HTTP的安全版，在HTTP下加入SSL层。 SSL（Secure Sockets Layer 安全套接层）主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全。 HTTP的端口号为80，HTTPS的端口号为443 HTTP的请求与响应HTTP通信由两部分组成： 客户端请求消息 与 服务器响应消息 浏览器发送HTTP请求的过程： 当用户在浏览器的地址栏中输入一个URL并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。HTTP请求主要分为“Get”和“Post”两种方法。 当我们在浏览器输入URL http://www.baidu.com 的时候，浏览器发送一个Request请求去获取 http://www.baidu.com 的html文件，服务器把Response文件对象发送回给浏览器。 浏览器分析Response中的 HTML，发现其中引用了很多其他文件，比如Images文件，CSS文件，JS文件。 浏览器会自动再次发送Request去获取图片，CSS文件，或者JS文件。 当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了。 URL（Uniform / Universal Resource Locator的缩写）：统一资源定位符，是用于完整地描述Internet上网页和其他资源的地址的一种标识方法。 基本格式：scheme://host[:port#]/path/…/[?query-string][#anchor] scheme：协议(例如：http, https, ftp)host：服务器的IP地址或者域名port#：服务器的端口（如果是走协议默认端口，缺省端口80）path：访问资源的路径query-string：参数，发送给http服务器的数据anchor：锚（跳转到网页的指定锚点位置） 例如： ftp://192.168.0.116:8080/index http://www.baidu.com http://item.jd.com/11936238.html#product-detail 客户端HTTP请求URL只是标识资源的位置，而HTTP是用来提交和获取资源。客户端发送一个HTTP请求到服务器的请求消息，包括以下格式： 请求行、请求头部、空行、请求数据 四个部分组成，下图给出了请求报文的一般格式。 一个典型的HTTP请求示例 GET https://www.baidu.com/ HTTP/1.1Host: www.baidu.comConnection: keep-aliveUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8Referer: http://www.baidu.com/Accept-Encoding: gzip, deflate, sdch, brAccept-Language: zh-CN,zh;q=0.8,en;q=0.6Cookie: BAIDUID=04E4001F34EA74AD4601512DD3C41A7B:FG=1; BIDUPSID=04E4001F34EA74AD4601512DD3C41A7B; PSTM=1470329258; MCITY=-343%3A340%3A; BDUSS=nF0MVFiMTVLcUh-Q2MxQ0M3STZGQUZ4N2hBa1FFRkIzUDI3QlBCZjg5cFdOd1pZQVFBQUFBJCQAAAAAAAAAAAEAAADpLvgG0KGyvLrcyfrG-AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFaq3ldWqt5XN; H_PS_PSSID=1447_18240_21105_21386_21454_21409_21554; BD_UPN=12314753; sug=3; sugstore=0; ORIGIN=0; bdime=0; H_PS_645EC=7e2ad3QHl181NSPbFbd7PRUCE1LlufzxrcFmwYin0E6b%2BW8bbTMKHZbDP0g; BDSVRTM=0 请求方法 GET https://www.baidu.com/ HTTP/1.1 根据HTTP标准，HTTP请求可以使用多种请求方法。 HTTP 0.9：只有基本的文本 GET 功能。 HTTP 1.0：完善的请求/响应模型，并将协议补充完整，定义了三种请求方法： GET, POST 和 HEAD方法。 HTTP 1.1：在 1.0 基础上进行更新，新增了五种请求方法：OPTIONS, PUT, DELETE, TRACE 和 CONNECT 方法。 HTTP 2.0（未普及）：请求/响应首部的定义基本没有改变，只是所有首部键必须全部小写，而且请求行要独立为 :method、:scheme、:host、:path这些键值对。| 序号 | 方法 | 描述 || :——– | ——–:| :–: || 1 | GET | 请求指定的页面信息，并返回实体主体。 || 2 | HEAD | 类似于get请求，只不过返回的响应中没有具体的内容，用于获取报头 || 3 | POST | 向指定资源提交数据进行处理请求（例如提交表单或者上传文件），数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 || 4 | PUT | 从客户端向服务器传送的数据取代指定的文档的内容。 || 5 | DELETE | 请求服务器删除指定的页面。|| 6 | CONNECT | HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 || 7 | OPTIONS | 允许客户端查看服务器的性能。 || 8 | TRACE | 回显服务器收到的请求，主要用于测试或诊断。 | HTTP请求主要分为Get和Post两种方法 GET是从服务器上获取数据，POST是向服务器传送数据 GET请求参数显示，都显示在浏览器网址上，HTTP服务器根据该请求所包含URL中的参数来产生响应内容，即“Get”请求的参数是URL的一部分。 例如： http://www.baidu.com/s?wd=Chinese POST请求参数在请求体当中，消息长度没有限制而且以隐式的方式进行发送，通常用来向HTTP服务器提交量比较大的数据（比如请求中包含许多参数或者文件上传操作等），请求的参数包含在“Content-Type”消息头里，指明该消息体的媒体类型和编码， 注意：避免使用Get方式提交表单，因为有可能会导致安全问题。 比如说在登陆表单中用Get方式，用户输入的用户名和密码将在地址栏中暴露无遗。 常用的请求报头1. Host (主机和端口号)Host：对应网址URL中的Web名称和端口号，用于指定被请求资源的Internet主机和端口号，通常属于URL的一部分。 2. Connection (链接类型)Connection：表示客户端与服务连接类型 Client 发起一个包含 Connection:keep-alive 的请求，HTTP/1.1使用 keep-alive 为默认值。 Server收到请求后： 如果 Server 支持 keep-alive，回复一个包含 Connection:keep-alive 的响应，不关闭连接；如果 Server 不支持 keep-alive，回复一个包含 Connection:close 的响应，关闭连接。如果client收到包含 Connection:keep-alive 的响应，向同一个连接发送下一个请求，直到一方主动关闭连接。 keep-alive在很多情况下能够重用连接，减少资源消耗，缩短响应时间，比如当浏览器需要多个文件时(比如一个HTML文件和相关的图形文件)，不需要每次都去请求建立连接。 3. Upgrade-Insecure-Requests (升级为HTTPS请求)Upgrade-Insecure-Requests：升级不安全的请求，意思是会在加载 http 资源时自动替换成 https 请求，让浏览器不再显示https页面中的http请求警报。 HTTPS 是以安全为目标的 HTTP 通道，所以在 HTTPS 承载的页面上不允许出现 HTTP 请求，一旦出现就是提示或报错。 4. User-Agent (浏览器名称)User-Agent：是客户浏览器的名称，以后会详细讲。 5. Accept (传输文件类型)Accept：指浏览器或其他客户端可以接受的MIME（Multipurpose Internet Mail Extensions（多用途互联网邮件扩展））文件类型，服务器可以根据它判断并返回适当的文件格式。举例： Accept: /：表示什么都可以接收。 Accept：image/gif：表明客户端希望接受GIF图像格式的资源； Accept：text/html：表明客户端希望接受html文本。 Accept: text/html, application/xhtml+xml;q=0.9, image/*;q=0.8：表示浏览器支持的 MIME 类型分别是 html文本、xhtml和xml文档、所有的图像格式资源。 **q是权重系数，范围 0 =&lt; q &lt;= 1，q 值越大，请求越倾向于获得其“;”之前的类型表示的内容。若没有指定q值，则默认为1，按从左到右排序顺序；若被赋值为0，则用于表示浏览器不接受此内容类型。 Text：用于标准化地表示的文本信息，文本消息可以是多种字符集和或者多种格式的；Application：用于传输应用程序数据或者二进制数据** 6. Referer (页面跳转处)Referer：表明产生请求的网页来自于哪个URL，用户是从该 Referer页面访问到当前请求的页面。这个属性可以用来跟踪Web请求来自哪个页面，是从什么网站来的等。 有时候遇到下载某网站图片，需要对应的referer，否则无法下载图片，那是因为人家做了防盗链，原理就是根据referer去判断是否是本网站的地址，如果不是，则拒绝，如果是，就可以下载； 7. Accept-Encoding（文件编解码格式）Accept-Encoding：指出浏览器可以接受的编码方式。编码方式不同于文件格式，它是为了压缩文件并加速文件传递速度。浏览器在接收到Web响应之后先解码，然后再检查文件格式，许多情形下这可以减少大量的下载时间。举例：Accept-Encoding:gzip;q=1.0, identity; q=0.5, *;q=0如果有多个Encoding同时匹配, 按照q值顺序排列，本例中按顺序支持 gzip, identity压缩编码，支持gzip的浏览器会返回经过gzip编码的HTML页面。 如果请求消息中没有设置这个域服务器假定客户端对各种内容编码都可以接受。 8. Accept-Language（语言种类）Accept-Langeuage：指出浏览器可以接受的语言种类，如en或en-us指英语，zh或者zh-cn指中文，当服务器能够提供一种以上的语言版本时要用到。 9. Accept-Charset（字符编码）Accept-Charset：指出浏览器可以接受的字符编码。 举例：Accept-Charset:iso-8859-1,gb2312,utf-8 ISO8859-1：通常叫做Latin-1。Latin-1包括了书写所有西方欧洲语言不可缺少的附加字符，英文浏览器的默认值是ISO-8859-1.gb2312：标准简体中文字符集;utf-8：UNICODE 的一种变长字符编码，可以解决多种语言文本显示问题，从而实现应用国际化和本地化。 如果在请求消息中没有设置这个域，缺省是任何字符集都可以接受。 10. Cookie （Cookie）Cookie：浏览器用这个属性向服务器发送Cookie。Cookie是在浏览器中寄存的小型数据体，它可以记载和服务器相关的用户信息，也可以用来实现会话功能，以后会详细讲。 11. Content-Type (POST数据类型)Content-Type：POST请求里用来表示的内容类型。 举例：Content-Type = Text/XML; charset=gb2312：指明该请求的消息体中包含的是纯文本的XML类型的数据，字符编码采用“gb2312”。 服务端HTTP响应HTTP响应也由四个部分组成，分别是：状态行、消息报头、空行、响应正文 12345678910111213HTTP/1.1 200 OKServer: TengineConnection: keep-aliveDate: Wed, 30 Nov 2016 07:58:21 GMTCache-Control: no-cacheContent-Type: text/html;charset=UTF-8Keep-Alive: timeout=20Vary: Accept-EncodingPragma: no-cacheX-NWS-LOG-UUID: bd27210a-24e5-4740-8f6c-25dbafa9c395Content-Length: 180945&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; .... 常用的响应报头理论上所有的响应头信息都应该是回应请求头的。但是服务端为了效率，安全，还有其他方面的考虑，会添加相对应的响应头信息，从上图可以看到： 1. Cache-Control：must-revalidate, no-cache, private。这个值告诉客户端，服务端不希望客户端缓存资源，在下次请求资源时，必须要从新请求服务器，不能从缓存副本中获取资源。 Cache-Control是响应头中很重要的信息，当客户端请求头中包含Cache-Control:max-age=0请求，明确表示不会缓存服务器资源时,Cache-Control作为作为回应信息，通常会返回no-cache，意思就是说，”那就不缓存呗”。 当客户端在请求头中没有包含Cache-Control时，服务端往往会定,不同的资源不同的缓存策略，比如说oschina在缓存图片资源的策略就是Cache-Control：max-age=86400,这个意思是，从当前时间开始，在86400秒的时间内，客户端可以直接从缓存副本中读取资源，而不需要向服务器请求。 2. Connection：keep-alive这个字段作为回应客户端的Connection：keep-alive，告诉客户端服务器的tcp连接也是一个长连接，客户端可以继续使用这个tcp连接发送http请求。 3. Content-Encoding:gzip告诉客户端，服务端发送的资源是采用gzip编码的，客户端看到这个信息后，应该采用gzip对资源进行解码。 4. Content-Type：text/html;charset=UTF-8告诉客户端，资源文件的类型，还有字符编码，客户端通过utf-8对资源进行解码，然后对资源进行html解析。通常我们会看到有些网站是乱码的，往往就是服务器端没有返回正确的编码。 5. Date：Sun, 21 Sep 2016 06:18:21 GMT这个是服务端发送资源时的服务器时间，GMT是格林尼治所在地的标准时间。http协议中发送的时间都是GMT的，这主要是解决在互联网上，不同时区在相互请求资源的时候，时间混乱问题。 6. Expires:Sun, 1 Jan 2000 01:00:00 GMT这个响应头也是跟缓存有关的，告诉客户端在这个时间前，可以直接访问缓存副本，很显然这个值会存在问题，因为客户端和服务器的时间不一定会都是相同的，如果时间不同就会导致问题。所以这个响应头是没有Cache-Control：max-age=*这个响应头准确的，因为max-age=date中的date是个相对时间，不仅更好理解，也更准确。 7. Pragma:no-cache这个含义与Cache-Control等同。 8. Server：Tengine/1.4.6这个是服务器和相对应的版本，只是告诉客户端服务器的信息。 9. Transfer-Encoding：chunked这个响应头告诉客户端，服务器发送的资源的方式是分块发送的。一般分块发送的资源都是服务器动态生成的，在发送时还不知道发送资源的大小，所以采用分块发送，每一块都是独立的，独立的块都能标示自己的长度，最后一块是0长度的，当客户端读到这个0长度的块时，就可以确定资源已经传输完了。 10. Vary: Accept-Encoding告诉缓存服务器，缓存压缩文件和非压缩文件两个版本，现在这个字段用处并不大，因为现在的浏览器都是支持压缩的。 Cookie 和 Session：服务器和客户端的交互仅限于请求/响应过程，结束之后便断开，在下一次请求时，服务器会认为新的客户端。 为了维护他们之间的链接，让服务器知道这是前一个用户发送的请求，必须在一个地方保存客户端的信息。 Cookie：通过在 客户端 记录的信息确定用户的身份。 Session：通过在 服务器端 记录的信息确定用户的身份。 响应状态码响应状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值。常见状态码： 100~199：表示服务器成功接收部分请求，要求客户端继续提交其余请求才能完成整个处理过程。 200~299：表示服务器成功接收请求并已完成整个处理过程。常用200（OK 请求成功）。 300~399：为完成请求，客户需进一步细化请求。例如：请求的资源已经移动一个新地址、常用302（所请求的页面已经临时转移至新的url）、307和304（使用缓存资源）。400~499：客户端的请求有错误，常用404（服务器无法找到被请求的页面）、403（服务器拒绝访问，权限不够）。500~599：服务器端出现错误，常用500（请求未完成。服务器遇到不可预知的情况）。 HTTP响应状态码参考：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821xx:信息100 Continue服务器仅接收到部分请求，但是一旦服务器并没有拒绝该请求，客户端应该继续发送其余的请求。101 Switching Protocols服务器转换协议：服务器将遵从客户的请求转换到另外一种协议。2xx:成功200 OK请求成功（其后是对GET和POST请求的应答文档）201 Created请求被创建完成，同时新的资源被创建。202 Accepted供处理的请求已被接受，但是处理未完成。203 Non-authoritative Information文档已经正常地返回，但一些应答头可能不正确，因为使用的是文档的拷贝。204 No Content没有新文档。浏览器应该继续显示原来的文档。如果用户定期地刷新页面，而Servlet可以确定用户文档足够新，这个状态代码是很有用的。205 Reset Content没有新文档。但浏览器应该重置它所显示的内容。用来强制浏览器清除表单输入内容。206 Partial Content客户发送了一个带有Range头的GET请求，服务器完成了它。3xx:重定向300 Multiple Choices多重选择。链接列表。用户可以选择某链接到达目的地。最多允许五个地址。301 Moved Permanently所请求的页面已经转移至新的url。302 Moved Temporarily所请求的页面已经临时转移至新的url。303 See Other所请求的页面可在别的url下被找到。304 Not Modified未按预期修改文档。客户端有缓冲的文档并发出了一个条件性的请求（一般是提供If-Modified-Since头表示客户只想比指定日期更新的文档）。服务器告诉客户，原来缓冲的文档还可以继续使用。305 Use Proxy客户请求的文档应该通过Location头所指明的代理服务器提取。306 Unused此代码被用于前一版本。目前已不再使用，但是代码依然被保留。307 Temporary Redirect被请求的页面已经临时移至新的url。4xx:客户端错误400 Bad Request服务器未能理解请求。401 Unauthorized被请求的页面需要用户名和密码。401.1登录失败。401.2服务器配置导致登录失败。401.3由于 ACL 对资源的限制而未获得授权。401.4筛选器授权失败。401.5ISAPI/CGI 应用程序授权失败。401.7访问被 Web 服务器上的 URL 授权策略拒绝。这个错误代码为 IIS 6.0 所专用。402 Payment Required此代码尚无法使用。403 Forbidden对被请求页面的访问被禁止。403.1执行访问被禁止。403.2读访问被禁止。403.3写访问被禁止。403.4要求 SSL。403.5要求 SSL 128。403.6IP 地址被拒绝。403.7要求客户端证书。403.8站点访问被拒绝。403.9用户数过多。403.10配置无效。403.11密码更改。403.12拒绝访问映射表。403.13客户端证书被吊销。403.14拒绝目录列表。403.15超出客户端访问许可。403.16客户端证书不受信任或无效。403.17客户端证书已过期或尚未生效。403.18在当前的应用程序池中不能执行所请求的 URL。这个错误代码为 IIS 6.0 所专用。403.19不能为这个应用程序池中的客户端执行 CGI。这个错误代码为 IIS 6.0 所专用。403.20Passport 登录失败。这个错误代码为 IIS 6.0 所专用。404 Not Found服务器无法找到被请求的页面。404.0没有找到文件或目录。404.1无法在所请求的端口上访问 Web 站点。404.2Web 服务扩展锁定策略阻止本请求。404.3MIME 映射策略阻止本请求。405 Method Not Allowed请求中指定的方法不被允许。406 Not Acceptable服务器生成的响应无法被客户端所接受。407 Proxy Authentication Required用户必须首先使用代理服务器进行验证，这样请求才会被处理。408 Request Timeout请求超出了服务器的等待时间。409 Conflict由于冲突，请求无法被完成。410 Gone被请求的页面不可用。411 Length Required&quot;Content-Length&quot; 未被定义。如果无此内容，服务器不会接受请求。412 Precondition Failed请求中的前提条件被服务器评估为失败。413 Request Entity Too Large由于所请求的实体的太大，服务器不会接受请求。414 Request-url Too Long由于url太长，服务器不会接受请求。当post请求被转换为带有很长的查询信息的get请求时，就会发生这种情况。415 Unsupported Media Type由于媒介类型不被支持，服务器不会接受请求。416 Requested Range Not Satisfiable服务器不能满足客户在请求中指定的Range头。417 Expectation Failed执行失败。423锁定的错误。5xx:服务器错误500 Internal Server Error请求未完成。服务器遇到不可预知的情况。500.12应用程序正忙于在 Web 服务器上重新启动。500.13Web 服务器太忙。500.15不允许直接请求 Global.asa。500.16UNC 授权凭据不正确。这个错误代码为 IIS 6.0 所专用。500.18URL 授权存储不能打开。这个错误代码为 IIS 6.0 所专用。500.100内部 ASP 错误。501 Not Implemented请求未完成。服务器不支持所请求的功能。502 Bad Gateway请求未完成。服务器从上游服务器收到一个无效的响应。502.1CGI 应用程序超时。 ·502.2CGI 应用程序出错。503 Service Unavailable请求未完成。服务器临时过载或当机。504 Gateway Timeout网关超时。505 HTTP Version Not Supported服务器不支持请求中指明的HTTP协议版本","categories":[{"name":"网络原理","slug":"网络原理","permalink":"https://zem12345678.github.io/categories/网络原理/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zem12345678.github.io/tags/爬虫/"},{"name":"HTTP/HTTPS","slug":"HTTP-HTTPS","permalink":"https://zem12345678.github.io/tags/HTTP-HTTPS/"}]},{"title":"爬虫原理与数据抓取","slug":"爬虫原理与数据抓取","date":"2019-05-16T03:38:25.774Z","updated":"2019-05-16T03:42:13.366Z","comments":true,"path":"2019/05/16/爬虫原理与数据抓取/","link":"","permalink":"https://zem12345678.github.io/2019/05/16/爬虫原理与数据抓取/","excerpt":"","text":"爬虫原理与数据抓取通用爬虫和聚焦爬虫根据使用场景，网络爬虫可分为 通用爬虫 和 聚焦爬虫 两种. 通用爬虫通用网络爬虫 是 捜索引擎抓取系统（Baidu、Google、Yahoo等）的重要组成部分。主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。 通用搜索引擎（Search Engine）工作原理通用网络爬虫 从互联网中搜集网页，采集信息，这些网页信息用于为搜索引擎建立索引从而提供支持，它决定着整个引擎系统的内容是否丰富，信息是否即时，因此其性能的优劣直接影响着搜索引擎的效果。 第一步：抓取网页 首先选取一部分的种子URL，将这些URL放入待抓取URL队列； 取出待抓取URL，解析DNS得到主机的IP，并将URL对应的网页下载下来，存储进已下载网页库中，并且将这些URL放进已抓取URL队列。 分析已抓取URL队列中的URL，分析其中的其他URL，并且将URL放入待抓取URL队列，从而进入下一个循环…. 搜索引擎如何获取一个新网站的URL： 新网站向搜索引擎主动提交网址：（如百度http://zhanzhang.baidu.com/linksubmit/url） 在其他网站上设置新网站外链（尽可能处于搜索引擎爬虫爬取范围） 搜索引擎和DNS解析服务商(如DNSPod等）合作，新网站域名将被迅速抓取。 但是搜索引擎蜘蛛的爬行是被输入了一定的规则的，它需要遵从一些命令或文件的内容，如标注为nofollow的链接，或者是Robots协议。 Robots协议（也叫爬虫协议、机器人协议等），全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取，例如： 淘宝网：https://www.taobao.com/robots.txt 腾讯网： http://www.qq.com/robots.txt 第二步：数据存储搜索引擎通过爬虫爬取到的网页，将数据存入原始页面数据库。其中的页面数据与用户浏览器得到的HTML是完全一样的。 搜索引擎蜘蛛在抓取页面时，也做一定的重复内容检测，一旦遇到访问权重很低的网站上有大量抄袭、采集或者复制的内容，很可能就不再爬行。 第三步：预处理搜索引擎将爬虫抓取回来的页面，进行各种步骤的预处理。 提取文字中文分词消除噪音（比如版权声明文字、导航条、广告等……）索引处理链接关系计算特殊文件处理…. 除了HTML文件外，搜索引擎通常还能抓取和索引以文字为基础的多种文件类型，如 PDF、Word、WPS、XLS、PPT、TXT 文件等。我们在搜索结果中也经常会看到这些文件类型。 但搜索引擎还不能处理图片、视频、Flash 这类非文字内容，也不能执行脚本和程序。 第四步：提供检索服务，网站排名搜索引擎在对信息进行组织和处理后，为用户提供关键字检索服务，将用户检索相关的信息展示给用户。 同时会根据页面的PageRank值（链接的访问量排名）来进行网站排名，这样Rank值高的网站在搜索结果中会排名较前，当然也可以直接使用 Money 购买搜索引擎网站排名，简单粗暴。 但是，这些通用性搜索引擎也存在着一定的局限性： 通用搜索引擎所返回的结果都是网页，而大多情况下，网页里90%的内容对用户来说都是无用的。 不同领域、不同背景的用户往往具有不同的检索目的和需求，搜索引擎无法提供针对具体某个用户的搜索结果。 万维网数据形式的丰富和网络技术的不断发展，图片、数据库、音频、视频多媒体等不同数据大量出现，通用搜索引擎对这些文件无能为力，不能很好地发现和获取。 通用搜索引擎大多提供基于关键字的检索，难以支持根据语义信息提出的查询，无法准确理解用户的具体需求。 针对这些情况，聚焦爬虫技术得以广泛使用。 聚焦爬虫聚焦爬虫，是”面向特定主题需求”的一种网络爬虫程序，它与通用搜索引擎爬虫的区别在于：聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息。 而我们今后要学习的网络爬虫，就是聚焦爬虫","categories":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://zem12345678.github.io/categories/数据挖掘/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"https://zem12345678.github.io/tags/爬虫/"},{"name":"python","slug":"python","permalink":"https://zem12345678.github.io/tags/python/"}]},{"title":"python编码故事","slug":"python编码故事","date":"2019-05-16T03:30:08.423Z","updated":"2019-05-16T03:47:51.806Z","comments":true,"path":"2019/05/16/python编码故事/","link":"","permalink":"https://zem12345678.github.io/2019/05/16/python编码故事/","excerpt":"","text":"python编码故事很久很久以前，有一群人，他们决定用8个可以开合的晶体管来组合成不同的状态，以表示世界上的万物。他们看到8个开关状态是好的，于是他们把这称为”字节”。 再后来，他们又做了一些可以处理这些字节的机器，机器开动了，可以用字节来组合出很多状态，状态开始变来变去。他们看到这样是好的，于是它们就这机器称为”计算机”。 开始计算机只在美国用。八位的字节一共可以组合出256(2的8次方)种不同的状态。 他们把其中的编号从0开始的32种状态分别规定了特殊的用途，一但终端、打印机遇上约定好的这些字节被传过来时，就要做一些约定的动作。遇上00x10, 终端就换行，遇上0x07, 终端就向人们嘟嘟叫，例好遇上0x1b, 打印机就打印反白的字，或者终端就用彩色显示字母。他们看到这样很好，于是就把这些0x20以下的字节状态称为”控制码”。 他们又把所有的空格、标点符号、数字、大小写字母分别用连续的字节状态表示，一直编到了第127号，这样计算机就可以用不同字节来存储英语的文字了。大家看到这样，都感觉很好，于是大家都把这个方案叫做 ANSI 的”Ascii”编码（American Standard Code for Information Interchange，美国信息互换标准代码）。当时世界上所有的计算机都用同样的ASCII方案来保存英文文字。 后来，就像建造巴比伦塔一样，世界各地的都开始使用计算机，但是很多国家用的不是英文，他们的字母里有许多是ASCII里没有的，为了可以在计算机保存他们的文字，他们决定采用127号之后的空位来表示这些新的字母、符号，还加入了很多画表格时需要用下到的横线、竖线、交叉等形状，一直把序号编到了最后一个状态255。从128到255这一页的字符集被称”扩展字符集”。从此之后，贪婪的人类再没有新的状态可以用了，美帝国主义可能没有想到还有第三世界国家的人们也希望可以用到计算机吧！ 等中国人们得到计算机时，已经没有可以利用的字节状态来表示汉字，况且有6000多个常用汉字需要保存呢。但是这难不倒智慧的中国人民，我们不客气地把那些127号之后的奇异符号们直接取消掉, 规定：一个小于127的字符的意义与原来相同，但两个大于127的字符连在一起时，就表示一个汉字，前面的一个字节（他称之为高字节）从0xA1用到0xF7，后面一个字节（低字节）从0xA1到0xFE，这样我们就可以组合出大约7000多个简体汉字了。在这些编码里，我们还把数学符号、罗马希腊的字母、日文的假名们都编进去了，连在 ASCII 里本来就有的数字、标点、字母都统统重新编了两个字节长的编码，这就是常说的”全角”字符，而原来在127号以下的那些就叫”半角”字符了。 中国人民看到这样很不错，于是就把这种汉字方案叫做 “GB2312”。GB2312 是对 ASCII 的中文扩展。 但是中国的汉字太多了，我们很快就就发现有许多人的人名没有办法在这里打出来，特别是某些很会麻烦别人的国家领导人。于是我们不得不继续把 GB2312 没有用到的码位找出来老实不客气地用上。 后来还是不够用，于是干脆不再要求低字节一定是127号之后的内码，只要第一个字节是大于127就固定表示这是一个汉字的开始，不管后面跟的是不是扩展字符集里的内容。结果扩展之后的编码方案被称为 GBK 标准，GBK 包括了 GB2312 的所有内容，同时又增加了近20000个新的汉字（包括繁体字）和符号。 后来少数民族也要用电脑了，于是我们再扩展，又加了几千个新的少数民族的字，GBK 扩成了 GB18030。从此之后，中华民族的文化就可以在计算机时代中传承了。 中国的程序员们看到这一系列汉字编码的标准是好的，于是通称他们叫做 “DBCS”（Double Byte Charecter Set 双字节字符集）。在DBCS系列标准里，最大的特点是两字节长的汉字字符和一字节长的英文字符并存于同一套编码方案里，因此他们写的程序为了支持中文处理，必须要注意字串里的每一个字节的值，如果这个值是大于127的，那么就认为一个双字节字符集里的字符出现了。那时候凡是受过加持，会编程的计算机僧侣们都要每天念下面这个咒语数百遍： “一个汉字算两个英文字符！一个汉字算两个英文字符……” 因为当时各个国家都像中国这样搞出一套自己的编码标准，结果互相之间谁也不懂谁的编码，谁也不支持别人的编码，连大陆和台湾这样只相隔了150海里，使用着同一种语言的兄弟地区，也分别采用了不同的 DBCS 编码方案——当时的中国人想让电脑显示汉字，就必须装上一个”汉字系统”，专门用来处理汉字的显示、输入的问题，但是那个台湾的愚昧封建人士写的算命程序就必须加装另一套支持 BIG5 编码的什么”倚天汉字系统”才可以用，装错了字符系统，显示就会乱了套！这怎么办？而且世界民族之林中还有那些一时用不上电脑的穷苦人民，他们的文字又怎么办？ 真是计算机的巴比伦塔命题啊！ 正在这时，大天使加百列及时出现了——一个叫 ISO （国际标谁化组织）的国际组织决定着手解决这个问题。他们采用的方法很简单：废了所有的地区性编码方案，重新搞一个包括了地球上所有文化、所有字母和符号的编码！他们打算叫它”Universal Multiple-Octet Coded Character Set”，简称 UCS, 俗称 “UNICODE”。 UNICODE 开始制订时，计算机的存储器容量极大地发展了，空间再也不成为问题了。于是 ISO 就直接规定必须用两个字节，也就是16位来统一表示所有的字符，对于ascii里的那些“半角”字符，UNICODE 包持其原编码不变，只是将其长度由原来的8位扩展为16位，而其他文化和语言的字符则全部重新统一编码。由于”半角”英文符号只需要用到低8位，所以其高8位永远是0，因此这种大气的方案在保存英文文本时会多浪费一倍的空间。 这时候，从旧社会里走过来的程序员开始发现一个奇怪的现象：他们的strlen函数靠不住了，一个汉字不再是相当于两个字符了，而是一个！是的，从 UNICODE 开始，无论是半角的英文字母，还是全角的汉字，它们都是统一的”一个字符”！同时，也都是统一的”两个字节”，请注意”字符”和”字节”两个术语的不同，“字节”是一个8位的物理存贮单元，而“字符”则是一个文化相关的符号。在UNICODE 中，一个字符就是两个字节。一个汉字算两个英文字符的时代已经快过去了。 从前多种字符集存在时，那些做多语言软件的公司遇上过很大麻烦，他们为了在不同的国家销售同一套软件，就不得不在区域化软件时也加持那个双字节字符集咒语，不仅要处处小心不要搞错，还要把软件中的文字在不同的字符集中转来转去。UNICODE 对于他们来说是一个很好的一揽子解决方案，于是从 Windows NT 开始，MS 趁机把它们的操作系统改了一遍，把所有的核心代码都改成了用 UNICODE 方式工作的版本，从这时开始，WINDOWS 系统终于无需要加装各种本土语言系统，就可以显示全世界上所有文化的字符了。 但是，UNICODE 在制订时没有考虑与任何一种现有的编码方案保持兼容，这使得 GBK 与UNICODE 在汉字的内码编排上完全是不一样的，没有一种简单的算术方法可以把文本内容从UNICODE编码和另一种编码进行转换，这种转换必须通过查表来进行。 如前所述，UNICODE 是用两个字节来表示为一个字符，他总共可以组合出65535不同的字符，这大概已经可以覆盖世界上所有文化的符号。如果还不够也没有关系，ISO已经准备了UCS-4方案，说简单了就是四个字节来表示一个字符，这样我们就可以组合出21亿个不同的字符出来（最高位有其他用途），这大概可以用到银河联邦成立那一天吧！ UNICODE 来到时，一起到来的还有计算机网络的兴起，UNICODE 如何在网络上传输也是一个必须考虑的问题，于是面向传输的众多 UTF（UCS Transfer Format）标准出现了，顾名思义，UTF8就是每次8个位传输数据，而UTF16就是每次16个位，只不过为了传输时的可靠性，从UNICODE到UTF时并不是直接的对应，而是要过一些算法和规则来转换。 总结 字符(Character)是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等。字符集(Character set)是多个字符的集合字符集包括：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集等ASCII编码是1个字节，而Unicode编码通常是2个字节。UTF-8是Unicode的实现方式之一，UTF-8是它是一种变长的编码方式，可以是1，2，3个字节","categories":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://zem12345678.github.io/tags/python/"},{"name":"编码","slug":"编码","permalink":"https://zem12345678.github.io/tags/编码/"}]},{"title":"str和bytes的区别","slug":"str和bytes的区别","date":"2019-05-16T02:29:36.683Z","updated":"2019-05-16T03:50:08.704Z","comments":true,"path":"2019/05/16/str和bytes的区别/","link":"","permalink":"https://zem12345678.github.io/2019/05/16/str和bytes的区别/","excerpt":"","text":"str和bytes的区别bytesbytes对象只负责以二进制字节序列的形式记录所需记录的对象，至于该对象到底表示什么（比如到底是什么字符）则由相应的编码格式解码所决定 Python2 中1234&gt;&gt;&gt; type(b&apos;xxxxx&apos;)&lt;type &apos;str&apos;&gt;&gt;&gt;&gt; type(&apos;xxxxx&apos;)&lt;type &apos;str&apos;&gt; Python3 中1234&gt;&gt;&gt; type(b&apos;xxxxx&apos;)&lt;class &apos;bytes&apos;&gt;&gt;&gt;&gt; type(&apos;xxxxx&apos;)&lt;class &apos;str&apos;&gt; bytes是Python 3中特有的，Python 2 里不区分bytes和str。 python3中：str 使用encode方法转化为 bytesbytes通过decode转化为str12345678910111213In [9]: str1=&apos;人生苦短，我用Python!&apos;In [10]: type(str1)Out[10]: strIn [11]: b=str1.encode()In [12]: bOut[12]: b&apos;\\xe4\\xba\\xba\\xe7\\x94\\x9f\\xe8\\x8b\\xa6\\xe7\\x9f\\xad\\xef\\xbc\\x8c\\xe6\\x88\\x91\\xe7\\x94\\xa8Python!&apos;In [13]: type(str1.encode())Out[13]: bytes bytes转换成str：123456789101112In [22]: bOut[22]: b&apos;\\xe4\\xba\\xba\\xe7\\x94\\x9f\\xe8\\x8b\\xa6\\xe7\\x9f\\xad\\xef\\xbc\\x8c\\xe6\\x88\\x91\\xe7\\x94\\xa8Python!&apos;In [23]: type(b)Out[23]: bytesIn [24]: b.decode()Out[24]: &apos;人生苦短，我用Python!&apos;In [25]: type(b.decode())Out[25]: str 在Python 2中由于不区分str和bytes所以可以直接通过encode()和decode()方法进行编码解码。 而在Python 3中把两者给分开了这个在使用中需要注意。实际应用中在互联网上是通过二进制进行传输，所以就需要将str转换成bytes进行传输，而在接收中通过decode()解码成我们需要的编码进行处理数据这样不管对方是什么编码而本地是我们使用的编码这样就不会乱码。 bytearraybytearray和bytes不一样的地方在于，bytearray是可变的。123456789101112131415161718In [26]: str1Out[26]: &apos;人生苦短，我用Python!&apos;In [28]: b1=bytearray(str1.encode())In [29]: b1Out[29]: bytearray(b&apos;\\xe4\\xba\\xba\\xe7\\x94\\x9f\\xe8\\x8b\\xa6\\xe7\\x9f\\xad\\xef\\xbc\\x8c\\xe6\\x88\\x91\\xe7\\x94\\xa8Python!&apos;)In [30]: b1.decode()Out[30]: &apos;人生苦短，我用Python!&apos;In [31]: b1[:6]=bytearray(&apos;生命&apos;.encode())In [32]: b1Out[32]: bytearray(b&apos;\\xe7\\x94\\x9f\\xe5\\x91\\xbd\\xe8\\x8b\\xa6\\xe7\\x9f\\xad\\xef\\xbc\\x8c\\xe6\\x88\\x91\\xe7\\x94\\xa8Python!&apos;)In [33]: b1.decode()Out[33]: &apos;生命苦短，我用Python!&apos;","categories":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/categories/Python/"}],"tags":[{"name":"编码","slug":"编码","permalink":"https://zem12345678.github.io/tags/编码/"}]},{"title":"MySQL数据库集群-PXC方案","slug":"MySQL数据库集群-PXC方案","date":"2019-03-18T14:28:06.853Z","updated":"2019-03-18T14:32:36.792Z","comments":true,"path":"2019/03/18/MySQL数据库集群-PXC方案/","link":"","permalink":"https://zem12345678.github.io/2019/03/18/MySQL数据库集群-PXC方案/","excerpt":"","text":"一、安装Percona数据库1. 离线安装Percona 进入RPM安装文件目录，执行下面的脚本 1yum localinstall *.rpm 管理MySQL服务 123systemctl start mysqldsystemctl stop mysqldsystemctl restart mysqld 2. 在线安装Percona 使用yum命令安装 12yum install http://www.percona.com/downloads/percona-release/redhat/0.1-3/percona-release-0.1-3.noarch.rpmyum install Percona-Server-server-57 管理MySQL服务 123service mysql startservice mysql stopservice mysql restart 3. 开放防火墙端口12firewall-cmd --zone=public --add-port=3306/tcp --permanentfirewall-cmd --reload 4. 修改MySQL配置文件1vi /etc/my.cnf 12345[mysqld]character_set_server = utf8bind-address = 0.0.0.0#跳过DNS解析skip-name-resolve 1service mysql restart 5. 禁止开机启动MySQL1chkconfig mysqld off 6. 初始化MySQL数据库 查看MySQL初始密码 1cat /var/log/mysqld.log | grep \"A temporary password\" 修改MySQL密码 1mysql_secure_installation 创建远程管理员账户 1mysql -u root -p 123CREATE USER 'admin'@'%' IDENTIFIED BY 'Abc_123456';GRANT all privileges ON *.* TO 'admin'@'%';FLUSH PRIVILEGES; 二、创建PXC集群1. 删除MariaDB程序包1yum -y remove mari* 2. 开放防火墙端口1234firewall-cmd --zone=public --add-port=3306/tcp --permanentfirewall-cmd --zone=public --add-port=4444/tcp --permanentfirewall-cmd --zone=public --add-port=4567/tcp --permanentfirewall-cmd --zone=public --add-port=4568/tcp --permanent 3. 关闭SELINUX1vi /etc/selinux/config 把SELINUX属性值设置成disabled 1reboot 4. 离线安装PXC 进入RPM文件目录，执行安装命令 1yum localinstall *.rpm 参考第一章内容，修改MySQL配置文件、创建账户等操作 5. 创建PXC集群 停止MySQL服务 修改每个PXC节点的/etc/my.cnf文件（在不同节点上，注意调整文件内容） 123456789101112server-id=1 #PXC集群中MySQL实例的唯一ID，不能重复，必须是数字wsrep_provider=/usr/lib64/galera3/libgalera_smm.sowsrep_cluster_name=pxc-cluster #PXC集群的名称wsrep_cluster_address=gcomm://192.168.99.151,192.168.99.159,192.168.99.215wsrep_node_name=pxc1 #当前节点的名称wsrep_node_address=192.168.99.151 #当前节点的IPwsrep_sst_method=xtrabackup-v2 #同步方法（mysqldump、rsync、xtrabackup）wsrep_sst_auth= admin:Abc_123456 #同步使用的帐户pxc_strict_mode=ENFORCING #同步严厉模式binlog_format=ROW #基于ROW复制（安全可靠）default_storage_engine=InnoDB #默认引擎innodb_autoinc_lock_mode=2 #主键自增长不锁表 主节点的管理命令（第一个启动的PXC节点） 123systemctl start mysql@bootstrap.servicesystemctl stop mysql@bootstrap.servicesystemctl restart mysql@bootstrap.service 非主节点的管理命令（非第一个启动的PXC节点） 123service start mysqlservice stop mysqlservice restart mysql 查看PXC集群状态信息 1show status like &apos;wsrep_cluster%&apos; ; 按照上述配置方法，创建两组PXC集群 6. PXC节点启动与关闭 如果最后关闭的PXC节点是安全退出的，那么下次启动要最先启动这个节点，而且要以主节点启动 如果最后关闭的PXC节点不是安全退出的，那么要先修改/var/lib/mysql/grastate.dat 文件，把其中的safe_to_bootstrap属性值设置为1，再安装主节点启动 三、安装MyCat1. JDK安装与配置 安装JDK 1234#搜索JDK版本yum search jdk #安装JDK1.8开发版yum install java-1.8.0-openjdk-devel.x86_64 配置环境变量 12345#查看JDK安装路径ls -lrt /etc/alternatives/javavi /etc/profile#在文件结尾加上JDK路径，例如export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-8.b10.el7_5.x86_64/source /etc/profile 2. 创建数据表 在两组PXC集群中分别创建t_user数据表 123456789CREATE TABLE t_user( id INT UNSIGNED PRIMARY KEY, username VARCHAR(200) NOT NULL, password VARCHAR(2000) NOT NULL, tel CHAR(11) NOT NULL, locked TINYINT(1) UNSIGNED NOT NULL DEFAULT 0, INDEX idx_username(username) USING BTREE, UNIQUE INDEX unq_username(username) USING BTREE); 3. MyCat安装与配置 下载MyCat http://dl.mycat.io/1.6.5/Mycat-server-1.6.5-release-20180122220033-linux.tar.gz 上传MyCat压缩包到虚拟机 安装unzip程序包，解压缩MyCat 12yum install unzipunzip MyCAT压缩包名称 开放防火墙8066和9066端口，关闭SELINUX 修改MyCat的bin目录中所有.sh文件的权限 1chmod -R 777 ./*.sh MyCat启动与关闭 1234#cd MyCat的bin目录./startup_nowrap.sh #启动MyCatps -aux #查看系统进程kill -9 MyCat进程编号 修改server.xml文件，设置MyCat帐户和虚拟逻辑库 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mycat:server SYSTEM \"server.dtd\"&gt;&lt;mycat:server xmlns:mycat=\"http://io.mycat/\"&gt; &lt;system&gt; &lt;property name=\"nonePasswordLogin\"&gt;0&lt;/property&gt; &lt;property name=\"useHandshakeV10\"&gt;1&lt;/property&gt; &lt;property name=\"useSqlStat\"&gt;0&lt;/property&gt; &lt;property name=\"useGlobleTableCheck\"&gt;0&lt;/property&gt; &lt;property name=\"sequnceHandlerType\"&gt;2&lt;/property&gt; &lt;property name=\"subqueryRelationshipCheck\"&gt;false&lt;/property&gt; &lt;property name=\"processorBufferPoolType\"&gt;0&lt;/property&gt; &lt;property name=\"handleDistributedTransactions\"&gt;0&lt;/property&gt; &lt;property name=\"useOffHeapForMerge\"&gt;1&lt;/property&gt; &lt;property name=\"memoryPageSize\"&gt;64k&lt;/property&gt; &lt;property name=\"spillsFileBufferSize\"&gt;1k&lt;/property&gt; &lt;property name=\"useStreamOutput\"&gt;0&lt;/property&gt; &lt;property name=\"systemReserveMemorySize\"&gt;384m&lt;/property&gt; &lt;property name=\"useZKSwitch\"&gt;false&lt;/property&gt; &lt;/system&gt; &lt;!--这里是设置的admin用户和虚拟逻辑库--&gt; &lt;user name=\"admin\" defaultAccount=\"true\"&gt; &lt;property name=\"password\"&gt;Abc_123456&lt;/property&gt; &lt;property name=\"schemas\"&gt;test&lt;/property&gt; &lt;/user&gt;&lt;/mycat:server&gt; 修改schema.xml文件，设置数据库连接和虚拟数据表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=\"1.0\"?&gt;&lt;!DOCTYPE mycat:schema SYSTEM \"schema.dtd\"&gt;&lt;mycat:schema xmlns:mycat=\"http://io.mycat/\"&gt; &lt;!--配置数据表--&gt; &lt;schema name=\"test\" checkSQLschema=\"false\" sqlMaxLimit=\"100\"&gt; &lt;table name=\"t_user\" dataNode=\"dn1,dn2\" rule=\"mod-long\" /&gt; &lt;/schema&gt; &lt;!--配置分片关系--&gt; &lt;dataNode name=\"dn1\" dataHost=\"cluster1\" database=\"test\" /&gt; &lt;dataNode name=\"dn2\" dataHost=\"cluster2\" database=\"test\" /&gt; &lt;!--配置连接信息--&gt; &lt;dataHost name=\"cluster1\" maxCon=\"1000\" minCon=\"10\" balance=\"2\" writeType=\"1\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=\"W1\" url=\"192.168.99.151:3306\" user=\"admin\" password=\"Abc_123456\"&gt; &lt;readHost host=\"W1R1\" url=\"192.168.99.159:3306\" user=\"admin\" password=\"Abc_123456\" /&gt; &lt;readHost host=\"W1R2\" url=\"192.168.99.215:3306\" user=\"admin\" password=\"Abc_123456\" /&gt; &lt;/writeHost&gt; &lt;writeHost host=\"W2\" url=\"192.168.99.159:3306\" user=\"admin\" password=\"Abc_123456\"&gt; &lt;readHost host=\"W2R1\" url=\"192.168.99.151:3306\" user=\"admin\" password=\"Abc_123456\" /&gt; &lt;readHost host=\"W2R2\" url=\"192.168.99.215:3306\" user=\"admin\" password=\"Abc_123456\" /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; &lt;dataHost name=\"cluster2\" maxCon=\"1000\" minCon=\"10\" balance=\"2\" writeType=\"1\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=\"W1\" url=\"192.168.99.121:3306\" user=\"admin\" password=\"Abc_123456\"&gt; &lt;readHost host=\"W1R1\" url=\"192.168.99.122:3306\" user=\"admin\" password=\"Abc_123456\" /&gt; &lt;readHost host=\"W1R2\" url=\"192.168.99.123:3306\" user=\"admin\" password=\"Abc_123456\" /&gt; &lt;/writeHost&gt; &lt;writeHost host=\"W2\" url=\"192.168.99.122:3306\" user=\"admin\" password=\"Abc_123456\"&gt; &lt;readHost host=\"W2R1\" url=\"192.168.99.121:3306\" user=\"admin\" password=\"Abc_123456\" /&gt; &lt;readHost host=\"W2R2\" url=\"192.168.99.123:3306\" user=\"admin\" password=\"Abc_123456\" /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt;&lt;/mycat:schema&gt; 修改rule.xml文件，把mod-long的count值修改成2 123&lt;function name=\"mod-long\" class=\"io.mycat.route.function.PartitionByMod\"&gt; &lt;property name=\"count\"&gt;2&lt;/property&gt;&lt;/function&gt; 重启MyCat 向t_user表写入数据，感受数据的切分 12345USE test;#第一条记录被切分到第二个分片INSERT INTO t_user(id,username,password,tel,locked) VALUES(1,&quot;A&quot;,HEX(AES_ENCRYPT(&apos;123456&apos;,&apos;HelloWorld&apos;)));#第二条记录被切分到第一个分片INSERT INTO t_user(id,username,password,tel,locked) VALUES(2,&quot;B&quot;,HEX(AES_ENCRYPT(&apos;123456&apos;,&apos;HelloWorld&apos;))); 4. 配置父子表 在conf目录下创建customer-hash-int文件，内容如下： 123456101=0102=0103=0104=1105=1106=1 在rule.xml文件中加入自定义和 1234&lt;function name=\"customer-hash-int\" class=\"io.mycat.route.function.PartitionByFileMap\"&gt; &lt;property name=\"mapFile\"&gt;customer-hash-int.txt&lt;/property&gt;&lt;/function&gt; 123456&lt;tableRule name=\"sharding-customer\"&gt; &lt;rule&gt; &lt;columns&gt;sharding_id&lt;/columns&gt; &lt;algorithm&gt;customer-hash-int&lt;/algorithm&gt; &lt;/rule&gt;&lt;/tableRule&gt; 修改schema.xml文件，添加父子表定义 1234&lt;table name=\"t_customer\" dataNode=\"dn1,dn2\" rule=\"sharding-customer\"&gt; &lt;childTable name=\"t_orders\" primaryKey=\"ID\" joinKey=\"customer_id\" parentKey=\"id\"/&gt;&lt;/table&gt; 在MyCat上执行如下SQL： 1234567891011USE test;CREATE TABLE t_customer( id INT UNSIGNED PRIMARY KEY, username VARCHAR(200) NOT NULL, sharding_id INT NOT NULL);CREATE TABLE t_orders( id INT UNSIGNED PRIMARY KEY, customer_id INT NOT NULL, datetime TIMESTAMP DEFAULT CURRENT_TIMSTAMP); 向t_customer表和t_orders表写入数据，查看字表数据跟随父表切分到同一个分片 5. 创建双机热备的MyCat集群 用两个虚拟机实例，各自部署MyCat 用一个虚拟机实例部署Haproxy 安装Haproxy 1yum install -y haproxy 编辑配置文件 1vi /etc/haproxy/haproxy.cfg 12345678910111213141516171819202122232425262728293031323334353637383940414243global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/statsdefaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000listen admin_stats bind 0.0.0.0:4001 mode http stats uri /dbs stats realm Global\\ statistics stats auth admin:abc123456listen proxy-mysql bind 0.0.0.0:3306 mode tcp balance roundrobin option tcplog #日志格式 server mycat_1 192.168.99.131:3306 check port 8066 maxconn 2000 server mycat_2 192.168.99.132:3306 check port 8066 maxconn 2000 option tcpka #使用keepalive检测死链 启动Haproxy 1service haproxy start 访问Haproxy监控画面 http://192.168.99.131:4001/dbs 用另外一个虚拟机同样按照上述操作安装Haproxy 在某个Haproxy虚拟机实例上部署Keepalived 开启防火墙的VRRP协议 1234#开启VRRPfirewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --protocol vrrp -j ACCEPT#应用设置firewall-cmd --reload 安装Keepalived 1yum install -y keepalived 编辑配置文件 1vim /etc/keepalived/keepalived.conf 1234567891011121314vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 123456 &#125; virtual_ipaddress &#123; 192.168.99.133 &#125;&#125; 启动Keepalived 1service keepalived start ping 192.168.99.133 在另外一个Haproxy虚拟机上，按照上述方法部署Keepalived 使用MySQL客户端连接192.168.99.133，执行增删改查数据 四、Sysbench基准测试1. 安装Sysbench 在线安装 12curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash 1yum -y install sysbench 本地安装 下载压缩文件 https://codeload.github.com/akopytov/sysbench/zip/1.0 安装依赖包 12yum install -y automake libtoolyum install -y mysql-devel 执行安装 123456#cd sysbench./autogen.sh ./configure makemake installsysbench --version 2. 执行测试 准备测试库 1sysbench /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-host=192.168.99.131 --mysql-port=3306 --mysql-user=admin --mysql-password=Abc_123456 --oltp-tables-count=10 --oltp-table-size=100000 prepare 执行测试 1sysbench /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-host=192.168.99.131 --mysql-port=3306 --mysql-user=admin --mysql-password=Abc_123456 --oltp-test-mode=complex --threads=10 --time=300 --report-interval=10 run &gt;&gt; /home/mysysbench.log 清理数据 1sysbench /usr/share/sysbench/tests/include/oltp_legacy/oltp.lua --mysql-host=192.168.99.131 --mysql-port=3306 --mysql-user=admin --mysql-password=Abc_123456 --oltp-tables-count=10 cleanup 五、tpcc-mysql 压力测试1. 准备工作 修改my.cnf配置文件 1vi /etc/my.cnf pxc_strict_mode=DISABLED 修改某个Haproxy的配置文件 123server mysql_1 192.168.99.151:3306 check port 3306 weight 1 maxconn 2000server mysql_2 192.168.99.159:3306 check port 3306 weight 1 maxconn 2000server mysql_3 192.168.99.215:3306 check port 3306 weight 1 maxconn 2000 重新启动Haproxy 安装依赖程序包 12yum install -y gccyum install -y mysql-devel 2. 安装tpcc-mysql 下载压缩包 https://codeload.github.com/Percona-Lab/tpcc-mysql/zip/master 执行安装 12#cd tpcc的src目录make 执行create_table.sql和add_fkey_idx.sql两个文件 执行数据初始化 1./tpcc_load -h 192.168.99.131 -d tpcc -u admin -p Abc_123456 -w 执行压力测试 1./tpcc_start -h 192.168.99.131 -d tpcc -u admin -p Abc_123456 -w 1 -c 5 -r 300 -l 600 -&gt;tpcc-output-log 六、导入数据1. 生成1000万条数据1234567891011121314import java.io.FileWriterimport java.io.BufferedWriterclass Test &#123; def static void main(String[] args) &#123; var writer=new FileWriter(\"D:/data.txt\") var buff=new BufferedWriter(writer) for(i:1..10000000)&#123; buff.write(i+\",测试数据\\n\") &#125; buff.close writer.close &#125;&#125; 2. 执行文件切分 上传data.txt文件到linux 执行文件切分 1split -l 1000000 -d data.txt 3. 准备数据库 每个PXC分片只开启一个节点 修改PXC节点文件，然后重启PXC服务 123innodb_flush_log_at_trx_commit = 0innodb_flush_method = O_DIRECTinnodb_buffer_pool_size = 200M 创建t_test数据表 1234CREATE TABLE t_test( id INT UNSIGNED PRIMARY KEY, name VARCHAR(200) NOT NULL); 配置MyCat 1&lt;table name=\"t_test\" dataNode=\"dn1,dn2\" rule=\"mod-long\" /&gt; 123456789101112&lt;dataHost name=\"cluster1\" maxCon=\"1000\" minCon=\"10\" balance=\"0\" writeType=\"1\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=\"W1\" url=\"192.168.99.151:3306\" user=\"admin\" password=\"Abc_123456\"/&gt;&lt;/dataHost&gt;&lt;dataHost name=\"cluster2\" maxCon=\"1000\" minCon=\"10\" balance=\"0\" writeType=\"1\" dbType=\"mysql\" dbDriver=\"native\" switchType=\"1\" slaveThreshold=\"100\"&gt; &lt;heartbeat&gt;select user()&lt;/heartbeat&gt; &lt;writeHost host=\"W1\" url=\"192.168.99.121:3306\" user=\"admin\" password=\"Abc_123456\"/&gt;&lt;/dataHost&gt; 4. 执行Java程序，多线程导入数据12345678910111213141516171819202122232425import org.eclipse.xtend.lib.annotations.Accessorsimport java.io.Fileimport java.sql.DriverManagerclass Task implements Runnable&#123; @Accessors File file; override run() &#123; var url=\"jdbc:mysql://192.168.99.131:8066/test\" var username=\"admin\" var password=\"Abc_123456\" var con=DriverManager.getConnection(url,username,password) var sql=''' load data local intfile '/home/data/«file.name»' ignore into table t_test character set 'utf8' fields terminated by ',' optionally enclosed by '\\\"' lines terminated by '\\n' (id,name); ''' var pst=con.prepareStatement(sql); pst.execute con.close LoadData.updateNum(); &#125;&#125; 123456789101112131415161718192021222324252627282930import com.mysql.jdbc.Driverimport java.sql.DriverManagerimport java.util.concurrent.LinkedBlockingQueueimport java.util.concurrent.ThreadPoolExecutorimport java.util.concurrent.TimeUnitimport java.io.Fileclass LoadData &#123; var static int num=0; var static int end=0; var static pool=new ThreadPoolExecutor(1,5,60,TimeUnit.SECONDS,new LinkedBlockingQueue(200)) def static void main(String[] args) &#123; DriverManager.registerDriver(new Driver) var folder=new File(\"/home/data\") var files=folder.listFiles end=files.length //线程池结束条件 files.forEach[one| var task=new Task(); task.file=one; pool.execute(task) ] &#125; synchronized def static updateNum()&#123; num++; if(num==end)&#123; pool.shutdown(); println(\"执行结束\") &#125; &#125;&#125; 七、大数据归档1. 安装TokuDB 安装jemlloc 1yum install -y jemalloc 编辑配置文件 1vi /etc/my.cnf 1234……[mysqld_safe]malloc-lib=/usr/lib64/libjemalloc.so.1…… 重启MySQL 开启Linux大页内存 12echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &gt; /sys/kernel/mm/transparent_hugepage/defrag 安装TokuDB 1234yum install -y Percona-Server-tokudb-57.x86_64ps-admin --enable -uroot -pservice mysql restartps-admin --enable -uroot -p 查看安装结果 1show engines ; 2. 配置Replication集群 在两个TokuDB数据库上创建用户 1CREATE USER &apos;backup&apos;@&apos;%&apos; IDENTIFIED BY &apos;Abc_123456&apos; ; 1GRANT super, reload, replication slave ON *.* TO &apos;backup&apos;@&apos;%&apos; ; 1FLUSH PRIVILEGES ; 修改两个TokuDB的配置文件，如下： 12345[mysqld]server_id = 101log_bin = mysql_binrelay_log = relay_bin…… 1234[mysqld]server_id = 102log_bin = mysql_binrelay_log = relay_bin 重新启动两个TokuDB节点 分别在两个TokuDB上执行下面4句SQL 123456789#关闭同步服务stop slave;#设置同步的Master节点change master to master_host=&quot;192.168.99.155&quot;,master_port=3306,master_user=&quot;backup&quot;,master_password=&quot;Abc_123456&quot;;#启动同步服务start slave;#查看同步状态show slave status; 123456789#关闭同步服务stop slave;#设置同步的Master节点change master to master_host=&quot;192.168.99.102&quot;,master_port=3306,master_user=&quot;backup&quot;,master_password=&quot;Abc_123456&quot;;#启动同步服务start slave;#查看同步状态show slave status; 3. 创建归档表123456789101112CREATE TABLE t_purchase ( id INT UNSIGNED PRIMARY KEY, purchase_price DECIMAL(10,2) NOT NULL, purchase_num INT UNSIGNED NOT NULL, purchase_sum DECIMAL (10,2) NOT NULL, purchase_buyer INT UNSIGNED NOT NULL, purchase_date TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, company_id INT UNSIGNED NOT NULL, goods_id INT UNSIGNED NOT NULL, KEY idx_company_id(company_id), KEY idx_goods_id(goods_id))engine=TokuDB; 4. 配置Haproxy+Keepalived双机热备 在两个节点上安装Haproxy 1yum install -y haproxy 修改配置文件 1vi /etc/haproxy/haproxy.cfg 12345678910111213141516171819202122232425262728293031323334353637383940414243global log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/statsdefaults mode http log global option httplog option dontlognull option http-server-close option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 1m timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s maxconn 3000listen admin_stats bind 0.0.0.0:4001 mode http stats uri /dbs stats realm Global\\ statistics stats auth admin:abc123456listen proxy-mysql bind 0.0.0.0:4002 mode tcp balance roundrobin option tcplog #日志格式 server backup_1 192.168.99.102:3306 check port 3306 maxconn 2000 server backup_2 192.168.99.155:3306 check port 3306 maxconn 2000 option tcpka #使用keepalive检测死链 重启Haproxy 开启防火墙的VRRP协议 1firewall-cmd --direct --permanent --add-rule ipv4 filter INPUT 0 --protocol vrrp -j ACCEPT 1firewall-cmd --reload 在两个节点上安装Keepalived 1yum install -y keepalived 编辑Keepalived配置文件 1vim /etc/keepalived/keepalived.conf 1234567891011121314vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 123456 &#125; virtual_ipaddress &#123; 192.168.99.211 &#125;&#125; 重启Keepalived 5. 准备归档数据 在两个PXC分片上创建进货表 123456789101112CREATE TABLE t_purchase ( id INT UNSIGNED PRIMARY KEY, purchase_price DECIMAL(10,2) NOT NULL, purchase_num INT UNSIGNED NOT NULL, purchase_sum DECIMAL (10,2) NOT NULL, purchase_buyer INT UNSIGNED NOT NULL, purchase_date TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, company_id INT UNSIGNED NOT NULL, goods_id INT UNSIGNED NOT NULL, KEY idx_company_id(company_id), KEY idx_goods_id(goods_id)) 配置MyCat的schema.xml文件，并重启MyCat 1&lt;table name=\"t_purchase\" dataNode=\"dn1,dn2\" rule=\"mod-long\" /&gt; 6. 执行数据归档 安装pt-archiver 123yum install percona-toolkitpt-archiver --versionpt-archiver --help 执行数据归档 1pt-archiver --source h=192.168.99.102,P=8066,u=admin,p=Abc_123456,D=test,t=t_purchase --dest h=192.168.99.102,P=3306,u=admin,p=Abc_123456,D=test,t=t_purchase --no-check-charset --where 'purchase_date&lt;\"2018-09\"' --progress 5000 --bulk-delete --bulk-insert --limit=10000 --statistics","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/categories/数据库/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zem12345678.github.io/tags/Mysql/"},{"name":"数据库集群","slug":"数据库集群","permalink":"https://zem12345678.github.io/tags/数据库集群/"}]},{"title":"elasticsearch与MySQL数据同步","slug":"elasticsearch与MySQL数据同步","date":"2019-03-18T14:21:01.447Z","updated":"2019-03-18T14:24:52.797Z","comments":true,"path":"2019/03/18/elasticsearch与MySQL数据同步/","link":"","permalink":"https://zem12345678.github.io/2019/03/18/elasticsearch与MySQL数据同步/","excerpt":"","text":"elasticsearch与MySQL数据同步Logstash 什么是LogstashLogstash是一款轻量级的日志搜集处理框架，可以方便的把分散的、多样化的日志搜集起来，并进行自定义的处理，然后传输到指定的位置，比如某个服务器或者文件。 Logstash安装与测试解压，进入bin目录logstash ‐e ‘input { stdin { } } output { stdout {} }’控制台输入字符，随后就有日志输出注意：双引号不能改成单引号否则可能会报 ERROR: Unknown command ‘{‘stdin，表示输入流，指从键盘输入stdout，表示输出流，指从显示器输出命令行参数:-e 执行–config 或 -f 配置文件，后跟参数类型可以是一个字符串的配置或全路径文件名或全路径路径(如：/etc/logstash.d/，logstash会自动读取/etc/logstash.d/目录下所有*.conf 的文本文件，然后在自己内存里拼接成一个完整的大配置文件再去执行) MySQL数据导入Elasticsearch（1）在logstash-5.6.8安装目录下创建文件夹mysqletc （名称随意）（2）文件夹下创建mysql.conf （名称随意） ，内容如下：1234567891011121314151617181920212223242526272829303132333435363738input &#123;jdbc &#123;# mysql jdbc connection string to our backup databse 后面的test对应mysql中的test数据库jdbc_connection_string =&gt;&quot;jdbc:mysql://127.0.0.1:3306/tensquare_article?characterEncoding=UTF8&quot;# the user we wish to excute our statement asjdbc_user =&gt; &quot;root&quot;jdbc_password =&gt; &quot;123456&quot;# the path to our downloaded jdbc driverjdbc_driver_library =&gt; &quot;D:/logstash‐5.6.8/mysqletc/mysql‐connector‐java‐5.1.46.jar&quot;# the name of the driver class for mysqljdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot;jdbc_paging_enabled =&gt; &quot;true&quot;jdbc_page_size =&gt; &quot;50000&quot;#以下对应着要执行的sql的绝对路径。statement =&gt; &quot;select id,title,content from tb_article&quot;#定时字段 各字段含义（由左至右）分、时、天、月、年，全部为*默认含义为每分钟都更新schedule =&gt; &quot;* * * * *&quot;&#125;&#125; output &#123;elasticsearch &#123;#ESIP地址与端口hosts =&gt; &quot;localhost:9200&quot;#ES索引名称（自己定义的）index =&gt; &quot;tensquare&quot;#自增ID编号document_id =&gt; &quot;%&#123;id&#125;&quot;document_type =&gt; &quot;article&quot;&#125; stdout &#123;#以JSON格式输出codec =&gt; json_lines&#125;&#125; （3）将mysql驱动包mysql-connector-java-5.1.46.jar拷贝至D:/logstash-5.6.8/mysqletc/ 下 。D:/logstash-5.6.8是你的安装目录（4）命令行下执行 logstash ‐f ../mysqletc/mysql.conf 观察控制台输出，每间隔1分钟就执行一次sql查询。再次刷新elasticsearch-head的数据显示，看是否也更新了数据。 Elasticsearch Docker环境下安装容器的创建与远程连接（1）下载镜像（此步省略） docker pull elasticsearch:5.6.8 （2）创建容器 docker run ‐di ‐‐name=tensquare_elasticsearch ‐p 9200:9200 ‐p 9300:9300elasticsearch:5.6.8 （3）浏览器输入地址：http://192.168.184.134:9200/ 即可看到如下信息 12345678910111213&#123;&quot;name&quot; : &quot;WmBn0H‐&quot;,&quot;cluster_name&quot; : &quot;elasticsearch&quot;,&quot;cluster_uuid&quot; : &quot;2g‐VVbm9Rty7J4sksZNJEg&quot;,&quot;version&quot; : &#123;&quot;number&quot; : &quot;5.6.8&quot;,&quot;build_hash&quot; : &quot;688ecce&quot;,&quot;build_date&quot; : &quot;2018‐02‐16T16:46:30.010Z&quot;,&quot;build_snapshot&quot; : false,&quot;lucene_version&quot; : &quot;6.6.1&quot;&#125;,&quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; （4）我们修改demo的application.yml1234spring:data:elasticsearch:cluster‐nodes: 192.168.184.135:9300 （5）运行测试程序，发现会报如下错误12345678910111213NoNodeAvailableException[None of the configured nodes are available:[&#123;#transport#‐1&#125;&#123;exvgJLR‐RlCNMJy‐hzKtnA&#125;&#123;192.168.184.135&#125;&#123;192.168.184.135:9300&#125;]]atorg.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:347)atorg.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:245)atorg.elasticsearch.client.transport.TransportProxyClient.execute(TransportProxyClient.java:59) 这是因为elasticsearch从5版本以后默认不开启远程连接，需要修改配置文件（6）我们进入容器 docker exec ‐it tensquare_elasticsearch /bin/bash 此时，我们看到elasticsearch所在的目录为/usr/share/elasticsearch ,进入config看到了配置文件elasticsearch.yml我们通过vi命令编辑此文件，尴尬的是容器并没有vi命令 ，咋办？我们需要以文件挂载的方式创建容器才行，这样我们就可以通过修改宿主机中的某个文件来实现对容器内配置文件的修改（7）拷贝配置文件到宿主机首先退出容器，然后执行命令： docker cptensquare_elasticsearch:/usr/share/elasticsearch/config/elasticsearch.yml/usr/share/elasticsearch.yml （8）停止和删除原来创建的容器 docker stop tensquare_elasticsearchdocker rm tensquare_elasticsearch （9）重新执行创建容器命令 docker run ‐di ‐‐name=tensquare_elasticsearch ‐p 9200:9200 ‐p 9300:9300 ‐v/usr/share/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml elasticsearch:5.6.8 （10）修改/usr/share/elasticsearch.yml 将 transport.host: 0.0.0.0 前的#去掉后保存文件退出。其作用是允许任何ip地址访问elasticsearch .开发测试阶段可以这么做，生产环境下指定具体的IP（11）重启启动 docker restart tensquare_elasticsearch 重启后发现重启启动失败了，这时什么原因呢？这与我们刚才修改的配置有关，因为elasticsearch在启动的时候会进行一些检查，比如最多打开的文件的个数以及虚拟内存区域数量等等，如果你放开了此配置，意味着需要打开更多的文件以及虚拟内存，所以我们还需要系统调优。（12）系统调优我们一共需要修改两处修改/etc/security/limits.conf ，追加内容 soft nofile 65536 hard nofile 65536 nofile是单个进程允许打开的最大文件个数 soft nofile 是软限制 hard nofile是硬限制 修改/etc/sysctl.conf，追加内容vm.max_map_count=655360 限制一个进程可以拥有的VMA(虚拟内存区域)的数量执行下面命令 修改内核参数马上生效 sysctl ‐p （13）重新启动虚拟机，再次启动容器，发现已经可以启动并远程访问 IK分词器安装（1）快捷键alt+p进入sftp , 将ik文件夹上传至宿主机 sftp&gt; put ‐r d:\\setup\\ik （2）在宿主机中将ik文件夹拷贝到容器内 /usr/share/elasticsearch/plugins 目录下。 docker cp ik tensquare_elasticsearch:/usr/share/elasticsearch/plugins/ （3）重新启动，即可加载IK分词器 docker restart tensquare_elasticsearch HEAD插件安装（1）修改/usr/share/elasticsearch.yml ,添加允许跨域配置 http.cors.enabled: truehttp.cors.allow‐origin: “*” （2）重新启动elasticseach容器 （3）下载head镜像（此步省略） docker pull mobz/elasticsearch‐head:5 （4）创建head容器 docker run ‐di ‐‐name=myhead ‐p 9100:9100 docker pull mobz/elasticsearch‐head:5","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/categories/数据库/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://zem12345678.github.io/tags/Mysql/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://zem12345678.github.io/tags/ElasticSearch/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://zem12345678.github.io/tags/搜索引擎/"}]},{"title":"分布式搜索引擎ElasticSearch","slug":"分布式搜索引擎ElasticSearch","date":"2019-03-18T14:10:07.878Z","updated":"2019-03-18T14:24:02.266Z","comments":true,"path":"2019/03/18/分布式搜索引擎ElasticSearch/","link":"","permalink":"https://zem12345678.github.io/2019/03/18/分布式搜索引擎ElasticSearch/","excerpt":"","text":"分布式搜索引擎ElasticSearchElasticSearch简介什么是ElasticSearch​ Elasticsearch是一个实时的分布式搜索和分析引擎。它可以帮助你用前所未有的速度去处理大规模数据。ElasticSearch是一个基于Lucene的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。Elasticsearch是用Java开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 ElasticSearch特点（1）可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上（2）将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的ES；（3）开箱即用的，部署简单（4）全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理 ElasticSearch体系结构表格 Elasticsearch 关系型数据库Mysql 索引(index) 数据库(databases) 类型(type) 表(table) 文档(document) 行(row) 走进ElasticSearchElasticSearch部署与启动下载ElasticSearch 5.6.8版本https://www.elastic.co/downloads/past-releases/elasticsearch-5-6-8在命令提示符下，进入ElasticSearch安装目录下的bin目录,执行命令即可启动。我们打开浏览器，在地址栏输入http://127.0.0.1:9200/ 即可看到输出结果 {“name” : “uV2glMR”,“cluster_name” : “elasticsearch”,“cluster_uuid” : “RdV7UTQZT1‐Jnka9dDPsFg”,“version” : {“number” : “5.6.8”,“build_hash” : “688ecce”,“build_date” : “2018‐02‐16T16:46:30.010Z”,“build_snapshot” : false,“lucene_version” : “6.6.1”},“tagline” : “You Know, for Search”} Postman调用RestAPI新建索引例如我们要创建一个叫articleindex的索引 ,就以put方式提交http://127.0.0.1:9200/articleindex/ 新建文档新建文档：以post方式提交 http://127.0.0.1:9200/articleindex/articlebody: {“title”:”SpringBoot2.0”,“content”:”发布啦”} 返回结果如下： {“_index”: “articleindex”,“_type”: “article”,“_id”: “AWPKsdh0FdLZnId5S_F9”,“_version”: 1,“result”: “created”,“_shards”: {“total”: 2,“successful”: 1,“failed”: 0},“created”: true} _id是由系统自动生成的。 为了方便之后的演示，我们再次录入几条测试数据 ####查询全部文档查询某索引某类型的全部数据，以get方式请求http://127.0.0.1:9200/articleindex/article/_search 返回结果如下：123456789101112131415161718192021222324252627282930313233343536&#123;&quot;took&quot;: 5,&quot;timed_out&quot;: false,&quot;_shards&quot;: &#123;&quot;total&quot;: 5,&quot;successful&quot;: 5,&quot;skipped&quot;: 0,&quot;failed&quot;: 0&#125;,&quot;hits&quot;: &#123;&quot;total&quot;: 2,&quot;max_score&quot;: 1,&quot;hits&quot;: [&#123;&quot;_index&quot;: &quot;articleindex&quot;,&quot;_type&quot;: &quot;article&quot;,&quot;_id&quot;: &quot;AWPKrI4pFdLZnId5S_F7&quot;,&quot;_score&quot;: 1,&quot;_source&quot;: &#123;&quot;title&quot;: &quot;SpringBoot2.0&quot;,&quot;content&quot;: &quot;发布啦&quot;&#125;&#125;,&#123;&quot;_index&quot;: &quot;articleindex&quot;,&quot;_type&quot;: &quot;article&quot;,&quot;_id&quot;: &quot;AWPKsdh0FdLZnId5S_F9&quot;,&quot;_score&quot;: 1,&quot;_source&quot;: &#123;&quot;title&quot;: &quot;elasticsearch入门&quot;,&quot;content&quot;: &quot;零基础入门&quot;&#125;&#125;]&#125;&#125; 修改文档以put形式提交以下地址http://192.168.184.134:9200/articleindex/article/AWPKrI4pFdLZnId5S_F7body:1234&#123;&quot;title&quot;:&quot;SpringBoot2.0正式版&quot;,&quot;content&quot;:&quot;发布了吗&quot;&#125; 返回结果：12345678910111213&#123;&quot;_index&quot;: &quot;articleindex&quot;,&quot;_type&quot;: &quot;article&quot;,&quot;_id&quot;: &quot;AWPKsdh0FdLZnId5S_F9&quot;,&quot;_version&quot;: 2,&quot;result&quot;: &quot;updated&quot;,&quot;_shards&quot;: &#123;&quot;total&quot;: 2,&quot;successful&quot;: 1,&quot;failed&quot;: 0&#125;,&quot;created&quot;: false&#125; 如果我们在地址中的ID不存在，则会创建新文档以put形式提交以下地址：http://192.168.184.134:9200/articleindex/article/1body:1234 &#123;&quot;title&quot;:&quot;AI牛逼&quot;,&quot;content&quot;:&quot;备注&quot;&#125; 返回信息：12345678910111213&#123;&quot;_index&quot;: &quot;articleindex&quot;,&quot;_type&quot;: &quot;article&quot;,&quot;_id&quot;: &quot;1&quot;,&quot;_version&quot;: 1,&quot;result&quot;: &quot;created&quot;,&quot;_shards&quot;: &#123;&quot;total&quot;: 2,&quot;successful&quot;: 1,&quot;failed&quot;: 0&#125;,&quot;created&quot;: true&#125; 再次查询，看是否有新增的这条文档 按ID查询文档GET方式请求http://192.168.184.134:9200/articleindex/article/12.2.6 基本匹配查询根据某列进行查询 get方式提交下列地址：http://192.168.184.134:9200/articleindex/article/_search?q=title:好给力以上为按标题查询，返回结果如下1234567891011121314151617181920212223242526&#123;&quot;took&quot;: 10,&quot;timed_out&quot;: false,&quot;_shards&quot;: &#123;&quot;total&quot;: 5,&quot;successful&quot;: 5,&quot;skipped&quot;: 0,&quot;failed&quot;: 0&#125;,&quot;hits&quot;: &#123;&quot;total&quot;: 1,&quot;max_score&quot;: 2.0649285,&quot;hits&quot;: [&#123;&quot;_index&quot;: &quot;articleindex&quot;,&quot;_type&quot;: &quot;article&quot;,&quot;_id&quot;: &quot;1&quot;,&quot;_score&quot;: 2.0649285,&quot;_source&quot;: &#123;&quot;title&quot;: &quot;好给力&quot;,&quot;content&quot;: &quot;备注&quot;&#125;&#125;]&#125;&#125; 模糊查询我们可以用*代表任意字符： http://192.168.184.134:9200/articleindex/article/_search?q=title:*s* 删除文档根据ID删除文档,删除ID为1的文档 DELETE方式提交 http://192.168.184.134:9200/articleindex/article/1 返回结果如下： {“found”: true,“_index”: “articleindex”,“_type”: “article”,“_id”: “1”,“_version”: 2,“result”: “deleted”,“_shards”: {“total”: 2,“successful”: 1,“failed”: 0}}再次查看全部是否还存在此记录 Head插件的安装与使用Head插件安装如果都是通过rest请求的方式使用Elasticsearch，未免太过麻烦，而且也不够人性化。我们一般都会使用图形化界面来实现Elasticsearch的日常管理，最常用的就是Head插件步骤1： 下载head插件：https://github.com/mobz/elasticsearch-head 步骤2： 解压到任意目录，但是要和elasticsearch的安装目录区别开。 步骤3：安装node js ,安装cnpm npm install ‐g cnpm ‐‐registry=https://registry.npm.taobao.org 步骤4： 将grunt安装为全局命令 。Grunt是基于Node.js的项目构建工具。它可以自动运行你所设定的任务 步骤5：安装依赖 cnpm install 步骤6： 进入head目录启动head，在命令提示符下输入命令 步骤7： 打开浏览器，输入 http://localhost:9100 步骤8： 点击连接按钮没有任何相应，按F12发现有如下错误No ‘Access-Control-Allow-Origin’ header is present on the requested resource 这个错误是由于elasticsearch默认不允许跨域调用，而elasticsearch-head是属于前端工程，所以报错。我们这时需要修改elasticsearch的配置，让其允许跨域访问。修改elasticsearch配置文件：elasticsearch.yml，增加以下两句命令： http.cors.enabled: truehttp.cors.allow‐origin: “*”此步为允许elasticsearch跨越访问 点击连接即可看到相关信息 Head插件操作新建索引选择“索引”选项卡，点击“新建索引”按钮 输入索引名称点击OK 新建或修改文档在复合查询中提交地址，输入内容，提交方式为PUT点击数据浏览 ,点击要查询的索引名称，右侧窗格中显示文档信息点击文档信息：修改数据后重新提交请求 , 此时因为ID已经存在，所以执行的是修改操作。重新查询此记录，发现版本为2 。也就是说每次修改后版本都会增加1. 搜索文档 删除文档IK分词器什么是IK分词器我们在浏览器地址栏输入http://127.0.0.1:9200/_analyze?analyzer=chinese&amp;pretty=true&amp;text=我是程序员，浏览器显示效果如下123456789101112131415161718192021222324252627282930313233343536373839&#123;&quot;tokens&quot; : [&#123;&quot;token&quot; : &quot;我&quot;,&quot;start_offset&quot; : 0,&quot;end_offset&quot; : 1,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 0&#125;,&#123;&quot;token&quot; : &quot;是&quot;,&quot;start_offset&quot; : 1,&quot;end_offset&quot; : 2,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 1&#125;,&#123;&quot;token&quot; : &quot;程&quot;,&quot;start_offset&quot; : 2,&quot;end_offset&quot; : 3,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 2&#125;,&#123;&quot;token&quot; : &quot;序&quot;,&quot;start_offset&quot; : 3,&quot;end_offset&quot; : 4,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 3&#125;,&#123;&quot;token&quot; : &quot;员&quot;,&quot;start_offset&quot; : 4,&quot;end_offset&quot; : 5,&quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;,&quot;position&quot; : 4&#125;]&#125; 默认的中文分词是将每个字看成一个词，这显然是不符合要求的，所以我们需要安装中文分词器来解决这个问题。IK分词是一款国人开发的相对简单的中文分词器。虽然开发者自2012年之后就不在维护了，但在工程应用中IK算是比较流行的一款！我们今天就介绍一下IK中文分词器的使用。 IK分词器安装下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases 下载5.6.8版本 （1）先将其解压，将解压后的elasticsearch文件夹重命名文件夹为ik（2）将ik文件夹拷贝到elasticsearch/plugins 目录下。（3）重新启动，即可加载IK分词器 IK分词器测试IK提供了两个分词算法ik_smart 和 ik_max_word其中 ik_smart 为最少切分，ik_max_word为最细粒度划分我们分别来试一下（1）最小切分：在浏览器地址栏输入地址http://127.0.0.1:9200/_analyze?analyzer=ik_smart&amp;pretty=true&amp;text=我是程序员输出的结果为：12345678910111213141516171819202122232425&#123;&quot;tokens&quot; : [&#123;&quot;token&quot; : &quot;我&quot;,&quot;start_offset&quot; : 0,&quot;end_offset&quot; : 1,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 0&#125;,&#123;&quot;token&quot; : &quot;是&quot;,&quot;start_offset&quot; : 1,&quot;end_offset&quot; : 2,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 1&#125;,&#123;&quot;token&quot; : &quot;程序员&quot;,&quot;start_offset&quot; : 2,&quot;end_offset&quot; : 5,&quot;type&quot; : &quot;CN_WORD&quot;,&quot;position&quot; : 2&#125;]&#125; （2）最细切分：在浏览器地址栏输入地址http://127.0.0.1:9200/_analyze?analyzer=ik_max_word&amp;pretty=true&amp;text=我是程序员 输出的结果为：123456789101112131415161718192021222324252627282930313233343536373839&#123;&quot;tokens&quot; : [&#123;&quot;token&quot; : &quot;我&quot;,&quot;start_offset&quot; : 0,&quot;end_offset&quot; : 1,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 0&#125;,&#123;&quot;token&quot; : &quot;是&quot;,&quot;start_offset&quot; : 1,&quot;end_offset&quot; : 2,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 1&#125;,&#123;&quot;token&quot; : &quot;程序员&quot;,&quot;start_offset&quot; : 2,&quot;end_offset&quot; : 5,&quot;type&quot; : &quot;CN_WORD&quot;,&quot;position&quot; : 2&#125;,&#123;&quot;token&quot; : &quot;程序&quot;,&quot;start_offset&quot; : 2,&quot;end_offset&quot; : 4,&quot;type&quot; : &quot;CN_WORD&quot;,&quot;position&quot; : 3&#125;,&#123;&quot;token&quot; : &quot;员&quot;,&quot;start_offset&quot; : 4,&quot;end_offset&quot; : 5,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 4&#125;]&#125; 4.4 自定义词库我们现在测试”品如秀儿”，浏览器的测试效果如下：http://127.0.0.1:9200/_analyze?analyzer=ik_smart&amp;pretty=true&amp;text=品如秀儿1234567891011121314151617181920212223242526272829303132&#123;&quot;tokens&quot; : [&#123;&quot;token&quot; : &quot;品&quot;,&quot;start_offset&quot; : 0,&quot;end_offset&quot; : 1,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 0&#125;,&#123;&quot;token&quot; : &quot;如&quot;,&quot;start_offset&quot; : 1,&quot;end_offset&quot; : 2,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 1&#125;,&#123;&quot;token&quot; : &quot;秀&quot;,&quot;start_offset&quot; : 2,&quot;end_offset&quot; : 3,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 2&#125;,&#123;&quot;token&quot; : &quot;儿&quot;,&quot;start_offset&quot; : 3,&quot;end_offset&quot; : 4,&quot;type&quot; : &quot;CN_CHAR&quot;,&quot;position&quot; : 3&#125;]&#125; 默认的分词并没有识别“传智播客”是一个词。如果我们想让系统识别“传智播客”是一个词，需要编辑自定义词库。步骤：（1）进入elasticsearch/plugins/ik/config目录（2）新建一个my.dic文件，编辑内容： 品如秀儿 修改IKAnalyzer.cfg.xml（在ik/config目录下）1234567&lt;properties&gt;&lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt;&lt;!‐‐用户可以在这里配置自己的扩展字典 ‐‐&gt;&lt;entry key=&quot;ext_dict&quot;&gt;my.dic&lt;/entry&gt;&lt;!‐‐用户可以在这里配置自己的扩展停止词字典‐‐&gt;&lt;entry key=&quot;ext_stopwords&quot;&gt;&lt;/entry&gt;&lt;/properties&gt; 重新启动elasticsearch,通过浏览器测试分词效果1234567891011&#123;&quot;tokens&quot; : [&#123;&quot;token&quot; : &quot;品如秀儿&quot;,&quot;start_offset&quot; : 0,&quot;end_offset&quot; : 4,&quot;type&quot; : &quot;CN_WORD&quot;,&quot;position&quot; : 0&#125;]&#125;","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/categories/数据库/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://zem12345678.github.io/tags/分布式/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"https://zem12345678.github.io/tags/ElasticSearch/"},{"name":"搜索引擎","slug":"搜索引擎","permalink":"https://zem12345678.github.io/tags/搜索引擎/"}]},{"title":"文档型数据库MongoDB","slug":"文档型数据库MongoDB","date":"2019-03-18T13:42:15.732Z","updated":"2019-03-18T14:22:57.263Z","comments":true,"path":"2019/03/18/文档型数据库MongoDB/","link":"","permalink":"https://zem12345678.github.io/2019/03/18/文档型数据库MongoDB/","excerpt":"","text":"文档型数据库MongoDBMongoDB简介MongoDB 是一个跨平台的，面向文档的数据库，是当前 NoSQL 数据库产品中最热门 的一种。它介于关系数据库和非关系数据库之间，是非关系数据库当中功能最丰富，最像关系数据库的产品。它支持的数据结构非常松散，是类似 JSON 的 BSON 格式，因此可以存 储比较复杂的数据类型。​ MongoDB 的官方网站地址是：http://www.mongodb.org/ MongoDB特点MongoDB 最大的特点是他支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。它是一个面向集合的,模式自由的文档型数据库。具体特点总结如下： （1）面向集合存储，易于存储对象类型的数据（2）模式自由（3）支持动态查询（4）支持完全索引，包含内部对象（5）支持复制和故障恢复（6）使用高效的二进制数据存储，包括大型对象（如视频等）（7）自动处理碎片，以支持云计算层次的扩展性（8）支持 Python，PHP，Ruby，Java，C，C#，Javascript，Perl 及 C++语言的驱动程序，社区中也提供了对 Erlang 及.NET 等平台的驱动程序（9） 文件存储格式为 BSON（一种 JSON 的扩展） MongoDB体系结构MongoDB 的逻辑结构是一种层次结构。主要由：文档(document)、集合(collection)、数据库(database)这三部分组成的。逻辑结构是面向用户的，用户使用 MongoDB 开发应用程序使用的就是逻辑结构。 （1）MongoDB 的文档（document），相当于关系数据库中的一行记录。（2）多个文档组成一个集合（collection），相当于关系数据库的表。（3）多个集合（collection），逻辑上组织在一起，就是数据库（database）。（4）一个 MongoDB 实例支持多个数据库（database）。文档(document)、集合(collection)、数据库(database)的层次结构如下图:北京市昌平区建材城西路金燕龙办公楼一层 电话| MongoDb | 关系型数据库Mysql || :——– | ——–:| :–: || 数据库(databases) | 数据库(databases) || 集合(collections) | 表(table) || 文档(document) | 行(row) | 数据类型基本数据类型null：用于表示空值或者不存在的字段，{“x”:null} 布尔型：布尔类型有两个值true和false，{“x”:true} 数值：shell默认使用64为浮点型数值。{“x”：3.14}或{“x”：3}。对于整型值，可以使 NumberInt（4字节符号整数）或NumberLong（8字节符号整数）{“x”:NumberInt(“3”)}{“x”:NumberLong(“3”)} 字符串：UTF-8字符串都可以表示为字符串类型的数据，{“x”：“呵呵”} 日期：日期被存储为自新纪元依赖经过的毫秒数，不存储时区，{“x”:new Date()} 正则表达式：查询时，使用正则表达式作为限定条件，语法与JavaScript的正则表达式相同，{“x”:/[abc]/} 数组：数据列表或数据集可以表示为数组，{“x”： [“a“，“b”,”c”]} 内嵌文档：文档可以嵌套其他文档，被嵌套的文档作为值来处理，{“x”:{“y”:3 }} 对象Id：对象id是一个12字节的字符串，是文档的唯一标识，{“x”: objectId() } 二进制数据：二进制数据是一个任意字节的字符串。它不能直接在shell中使用。如果要将非utf-字符保存到数据库中，二进制数据是唯一的方式。代码：查询和文档中可以包括任何JavaScript代码，{“x”:function(){/…/}} 走进MongoDBMongoDB安装与启动window系统MongoDB安装安装双击“mongodb-win32-x86_64-2008plusssl-3.2.10-signed.msi” 按照提示步骤安装即可。安装完成后，软件会安装在C:\\ProgramFiles\\MongoDB 目录中。我们要启动的服务程序就是C:\\Program Files\\MongoDB\\Server\\3.2\\bin目录下的mongod.exe，为了方便我们每次启动，我将C:\\ProgramFiles\\MongoDB\\Server\\3.2\\bin 设置到环境变量path中。启动（1）首先打开命令提示符，创建一个用于存放数据的目录 md d:\\data （2）启动服务 mongod ‐‐dbpath=d:\\data 我们在启动信息中可以看到，mongoDB的默认端口是27017如果我们想改变默认的启动端口，可以通过–port来指定端口在命令提示符输入以下命令即可完成登陆 mongo 退出mongodb exit 2 Docker 环境下MongoDB安装在宿主机创建mongo容器 docker run ‐di ‐‐name=tensquare_mongo ‐p 27017:27017 mongo 远程登陆 mongo 192.168.184.134 常用命令选择和创建数据库选择和创建数据库的语法格式： use 数据库名称 如果数据库不存在则自动创建以下语句创建spit数据库 use spitdb 插入与查询文档插入文档的语法格式： db.集合名称.insert(数据); 我们这里可以插入以下测试数据： db.spit.insert({content:”听说Python很给力呀”,userid:”1011”,nickname:”小雅”,visits:NumberInt(902)}) 查询集合的语法格式： db.集合名称.find() 如果我们要查询spit集合的所有文档，我们输入以下命令 db.spit.find() 这里你会发现每条文档会有一个叫_id的字段，这个相当于我们原来关系数据库中表的主键，当你在插入文档记录时没有指定该字段，MongoDB会自动创建，其类型是ObjectID类型。如果我们在插入文档记录时指定该字段也可以，其类型可以是ObjectID类型，也可以是MongoDB支持的任意类型。输入以下测试语句: db.spit.insert({_id:”1”,content:”我还是没有想明白到底为啥出错”,userid:”1012”,nickname:”小明”,visits:NumberInt(2020)});db.spit.insert({_id:”2”,content:”加班到半夜”,userid:”1013”,nickname:”凯撒”,visits:NumberInt(1023)});db.spit.insert({_id:”3”,content:”手机流量超了咋办？”,userid:”1013”,nickname:”凯撒”,visits:NumberInt(111)});db.spit.insert({_id:”4”,content:”坚持就是胜利”,userid:”1014”,nickname:”诺诺”,visits:NumberInt(1223)}); 如果我想按一定条件来查询，比如我想查询userid为1013的记录，怎么办？很简单！只要在find()中添加参数即可，参数也是json格式，如下： db.spit.find({userid:’1013’})如果你只需要返回符合条件的第一条数据，我们可以使用findOne命令来实现db.spit.findOne({userid:’1013’}) 如果你想返回指定条数的记录，可以在find方法后调用limit来返回结果，例如： db.spit.find().limit(3) 修改与删除文档修改文档的语法结构： db.集合名称.update(条件,修改后的数据) 如果我们想修改_id为1的记录，浏览量为1000，输入以下语句： db.spit.update({_id:”1”},{visits:NumberInt(1000)}) 执行后，我们会发现，这条文档除了visits字段其它字段都不见了，为了解决这个问题，我们需要使用修改器$set来实现，命令如下： db.spit.update({_id:”1”},{visits:NumberInt(1000)}) 这样就OK啦。删除文档的语法结构： db.集合名称.remove(条件) 以下语句可以将数据全部删除，请慎用 db.spit.remove({}) 如果删除visits=1000的记录，输入以下语句 db.spit.remove({visits:1000}) 统计条数统计记录条件使用count()方法。以下语句统计spit集合的记录数 db.spit.count() 如果按条件统计 ，例如：统计userid为1013的记录条数 db.spit.count({userid:”1013”}) 模糊查询MongoDB的模糊查询是通过正则表达式的方式实现的。格式为： /模糊查询字符串/ 例如，我要查询吐槽内容包含“流量”的所有文档，代码如下： db.spit.find({content:/流量/}) 如果要查询吐槽内容中以“加班”开头的，代码如下： db.spit.find({content:/^加班/}) 大于 小于 不等于&lt;, &lt;=, &gt;, &gt;= 这个操作符也是很常用的，格式如下: db.集合名称.find({ “field” : { $gt: value }}) // 大于: field &gt; valuedb.集合名称.find({ “field” : { $lt: value }}) // 小于: field &lt; valuedb.集合名称.find({ “field” : { $gte: value }}) // 大于等于: field &gt;= valuedb.集合名称.find({ “field” : { $lte: value }}) // 小于等于: field &lt;= valuedb.集合名称.find({ “field” : { $ne: value }}) // 不等于 示例：查询吐槽浏览量大于1000的记录 db.spit.find({visits:{$gt:1000}}) 包含与不包含包含使用$in操作符。示例：查询吐槽集合中userid字段包含1013和1014的文档 db.spit.find({userid:{$in:[“1013”,”1014”]}}) 不包含使用$nin操作符。示例：查询吐槽集合中userid字段不包含1013和1014的文档 db.spit.find({userid:{$nin:[“1013”,”1014”]}}) 条件连接我们如果需要查询同时满足两个以上条件，需要使用$and操作符将条件进行关联。（相当于SQL的and）格式为： $and:[ { },{ },{ } ] 示例：查询吐槽集合中visits大于等于1000 并且小于2000的文档 db.spit.find({$and:[ {visits:{$gte:1000}} ,{visits:{$lt:2000} }]}) 如果两个以上条件之间是或者的关系，我们使用 操作符进行关联，与前面and的使用方式相同格式为：$or:[ { },{ },{ } ]示例：查询吐槽集合中userid为1013，或者浏览量小于2000的文档记录 db.spit.find({$or:[ {userid:”1013”} ,{visits:{$lt:2000} }]}) 列值增长如果我们想实现对某列值在原有值的基础上进行增加或减少，可以使用$inc运算符来实现 db.spit.update({_id:”2”},{$inc:{visits:NumberInt(1)}} )","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/categories/数据库/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://zem12345678.github.io/tags/MongoDB/"},{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/tags/数据库/"}]},{"title":"LeedCode15：三数之和","slug":"LeedCode15：三数之和","date":"2019-03-18T09:26:28.175Z","updated":"2019-03-18T09:28:42.965Z","comments":true,"path":"2019/03/18/LeedCode15：三数之和/","link":"","permalink":"https://zem12345678.github.io/2019/03/18/LeedCode15：三数之和/","excerpt":"","text":"LeedCode15：三数之和给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。注意：答案中不可以包含重复的三元组。 例如, 给定数组 nums = [-1, 0, 1, 2, -1, -4]，满足要求的三元组集合为：[ [-1, 0, 1], [-1, -1, 2]] 解题思路一：暴力算法（不应该称之为算法）:三层循环找到符合条件的数时间复杂度为 $ O(N^3) $ ##解题思路二：$ c = -(a+b) $ =&gt; 申请一个一个set集合将nums中所有的元素存进去，用双层loops很容易找a和b，再在set集合中便利找到c复杂度为 $ O(N^2) +O(1) = O(N^2)$ ，但是这样会多开辟一块空间。 ##解题思路三：先将整个数组排序python中可以直接用sort()方法进行排序，基于快排时间复杂度为$ O(N\\log N）$ 例如nums = [-1, 0, 1, 2, -1, -4] [-4, -1, -1 , 0, 1, 2 ] 先枚举找打a 一层loop在从剩下的元素中[-1, -1 , 0, 1, 2 ] 找b和c 因a之后的元素是有序的那么我们就可以同时头和尾开始找假设b从头开始，c从尾开始，如果a+b+c大于零，那么c需要变小c往左移如果a+b+c大于零，那么b需要变大b往右移c复杂度为 $O(N^2)$完整代码：123456789101112131415161718192021222324252627class Solution(object): def threeSum(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; res = [] nums.sort() for i in range(len(nums)-2): if i &gt; 0 and nums[i] == nums[i-1]: continue l,r = i+1,len(nums)-1 while l &lt; r: s = nums[i] + nums[l] + nums[r] if s &lt; 0: l += 1 elif s &gt; 0: r -= 1 else: res.append([nums[i],nums[l],nums[r]]) while l &lt; r and nums[l] == nums[l+1]: l += 1 while l &gt; r and nums[r] == nums[r-1]: r -=1 l += 1 r -= 1 return res","categories":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/categories/算法/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/tags/算法/"},{"name":"LeetCood","slug":"LeetCood","permalink":"https://zem12345678.github.io/tags/LeetCood/"},{"name":"数据结构","slug":"数据结构","permalink":"https://zem12345678.github.io/tags/数据结构/"}]},{"title":"使用随机森林对鸾尾花进行分类","slug":"使用随机森林对鸾尾花进行分类","date":"2019-03-15T06:49:26.147Z","updated":"2019-03-15T06:54:35.224Z","comments":true,"path":"2019/03/15/使用随机森林对鸾尾花进行分类/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/使用随机森林对鸾尾花进行分类/","excerpt":"","text":"使用随机森林对鸾尾花进行分类导入所需的包1234import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.ensemble import RandomForestClassifier ##对种类进行需处理三种花分别为0,1,212345def iris_type(s): it = &#123;b&apos;Iris-setosa&apos;: 0, b&apos;Iris-versicolor&apos;: 1, b&apos;Iris-virginica&apos;: 2&#125; return it[s] 建立随机森林学习模型‘花萼长度’,’花萼宽度’,’花瓣长度’,’花瓣宽度’，四种特种进行两两匹配 iris_feature = ‘花萼长度’,’花萼宽度’,’花瓣长度’,’花瓣宽度’1234567891011121314151617181920212223242526272829303132333435mpl.rcParams[&apos;font.sans-serif&apos;] = [u&apos;simHei&apos;] mpl.rcParams[&apos;axes.unicode_minus&apos;] = False path = &apos;iris.csv&apos; data = np.loadtxt(path,dtype=float,delimiter=&apos;,&apos;,converters=&#123;4:iris_type&#125;) x_prime,y = np.split(data,(4,),axis=1) features_paris = [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)] plt.figure(figsize=(10,9),facecolor=&apos;#FFFFFF&apos;) for i,pair in enumerate(features_paris): #准备数据 x = x_prime[:,pair] #随机森林学习 clf = RandomForestClassifier(n_estimators=200,criterion=&apos;entropy&apos;,max_depth=3) dt_clf = clf.fit(x,y.ravel()) #画图 N,M = 500,500 x1_min,x1_max = x[:,0].min(),x[:,0].max()# 第0列的范围 x2_min, x2_max = x[:, 1].min(), x[:, 1].max() # 第1列的范围 t1 = np.linspace(x1_min, x1_max, N) t2 = np.linspace(x2_min, x2_max, M) x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点 x_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点 #训练集上的预测结果 y_hat = dt_clf.predict(x) y = y.reshape(-1) c = np.count_nonzero(y_hat==y)# 统计预测正确的个数 print(&apos;特征： &apos;, iris_feature[pair[0]], &apos; + &apos;, iris_feature[pair[1]],) print(&apos;\\t预测正确数目：&apos;, c,) print(&apos;\\t准确率: %.2f%%&apos; % (100 * float(c) / float(len(y)))) 特征： 花萼长度 + 花萼宽度 预测正确数目： 123 准确率: 82.00%特征： 花萼长度 + 花瓣长度 预测正确数目： 142 准确率: 94.67%特征： 花萼长度 + 花瓣宽度 预测正确数目： 145 准确率: 96.67%特征： 花萼宽度 + 花瓣长度 预测正确数目： 143 准确率: 95.33%特征： 花萼宽度 + 花瓣宽度 预测正确数目： 144 准确率: 96.00%特征： 花瓣长度 + 花瓣宽度 预测正确数目： 145 准确率: 96.67% 学习结果可视化1234567891011121314151617#显示 cm_light = mpl.colors.ListedColormap([&apos;#A0FFA0&apos;, &apos;#FFA0A0&apos;, &apos;#A0A0FF&apos;]) cm_dark = mpl.colors.ListedColormap([&apos;g&apos;, &apos;r&apos;, &apos;b&apos;]) y_hat = dt_clf.predict(x_test) # 预测值 y_hat = y_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.subplot(2, 3, i + 1) plt.pcolormesh(x1, x2, y_hat, cmap=cm_light) # 预测值 plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors=&apos;k&apos;, cmap=cm_dark) # 样本 plt.xlabel(iris_feature[pair[0]], fontsize=14) plt.ylabel(iris_feature[pair[1]], fontsize=14) plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.grid() plt.suptitle(u&apos;随机森林对鸢尾花数据的两特征组合的分类结果&apos;, fontsize=18) plt.tight_layout(2) plt.subplots_adjust(top=0.92) plt.show()","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://zem12345678.github.io/categories/Machine-learning/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/tags/算法/"},{"name":"Machine learning","slug":"Machine-learning","permalink":"https://zem12345678.github.io/tags/Machine-learning/"},{"name":"随机森林","slug":"随机森林","permalink":"https://zem12345678.github.io/tags/随机森林/"}]},{"title":"决策树对鸾尾花进行分类","slug":"决策树对鸾尾花进行分类","date":"2019-03-15T06:43:47.716Z","updated":"2019-03-15T06:53:50.536Z","comments":true,"path":"2019/03/15/决策树对鸾尾花进行分类/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/决策树对鸾尾花进行分类/","excerpt":"","text":"决策树对鸾尾花进行分类导入所需要的包1234import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltfrom sklearn.tree import DecisionTreeClassifier ##对种类进行需处理三种花分别为0,1,212345def iris_type(s): it = &#123;b&apos;Iris-setosa&apos;: 0, b&apos;Iris-versicolor&apos;: 1, b&apos;Iris-virginica&apos;: 2&#125; return it[s] 建立决策树模型‘花萼长度’,’花萼宽度’,’花瓣长度’,’花瓣宽度’，四种特种进行两两匹配 iris_feature = ‘花萼长度’,’花萼宽度’,’花瓣长度’,’花瓣宽度’1234567891011121314151617181920212223242526272829303132333435mpl.rcParams[&apos;font.sans-serif&apos;] = [u&apos;simHei&apos;] mpl.rcParams[&apos;axes.unicode_minus&apos;] = False path = &apos;iris.csv&apos; data = np.loadtxt(path,dtype=float,delimiter=&apos;,&apos;,converters=&#123;4:iris_type&#125;) x_prime,y = np.split(data,(4,),axis=1) features_paris = [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)] plt.figure(figsize=(10,9),facecolor=&apos;#FFFFFF&apos;) for i,pair in enumerate(features_paris): #准备数据 x = x_prime[:,pair] #决策树学习 clf = DecisionTreeClassifier(criterion=&apos;entropy&apos;,min_samples_leaf=3) dt_clf = clf.fit(x,y) #画图 N,M = 500,500 x1_min,x1_max = x[:,0].min(),x[:,0].max()# 第0列的范围 x2_min, x2_max = x[:, 1].min(), x[:, 1].max() # 第1列的范围 t1 = np.linspace(x1_min, x1_max, N) t2 = np.linspace(x2_min, x2_max, M) x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点 x_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点 #训练集上的预测结果 y_hat = dt_clf.predict(x) y = y.reshape(-1) c = np.count_nonzero(y_hat==y)# 统计预测正确的个数 print(&apos;特征： &apos;, iris_feature[pair[0]], &apos; + &apos;, iris_feature[pair[1]],) print(&apos;\\t预测正确数目：&apos;, c,) print(&apos;\\t准确率: %.2f%%&apos; % (100 * float(c) / float(len(y)))) 可视化展示12345678910111213141516cm_light = mpl.colors.ListedColormap([&apos;#A0FFA0&apos;, &apos;#FFA0A0&apos;, &apos;#A0A0FF&apos;]) cm_dark = mpl.colors.ListedColormap([&apos;g&apos;, &apos;r&apos;, &apos;b&apos;]) y_hat = dt_clf.predict(x_test) # 预测值 y_hat = y_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.subplot(2, 3, i + 1) plt.pcolormesh(x1, x2, y_hat, cmap=cm_light) # 预测值 plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors=&apos;k&apos;, cmap=cm_dark) # 样本 plt.xlabel(iris_feature[pair[0]], fontsize=14) plt.ylabel(iris_feature[pair[1]], fontsize=14) plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.grid() plt.suptitle(u&apos;决策树对鸢尾花数据的两特征组合的分类结果&apos;, fontsize=18) plt.tight_layout(2) plt.subplots_adjust(top=0.92) plt.show() 特征： 花萼长度 + 花萼宽度 预测正确数目： 123 准确率: 82.00%特征： 花萼长度 + 花瓣长度 预测正确数目： 145 准确率: 96.67%特征： 花萼长度 + 花瓣宽度 预测正确数目： 144 准确率: 96.00%特征： 花萼宽度 + 花瓣长度 预测正确数目： 143 准确率: 95.33%特征： 花萼宽度 + 花瓣宽度 预测正确数目： 145 准确率: 96.67%特征： 花瓣长度 + 花瓣宽度 预测正确数目： 147 准确率: 98.00%","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://zem12345678.github.io/categories/Machine-learning/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/tags/算法/"},{"name":"Machine learning","slug":"Machine-learning","permalink":"https://zem12345678.github.io/tags/Machine-learning/"},{"name":"决策树分类","slug":"决策树分类","permalink":"https://zem12345678.github.io/tags/决策树分类/"}]},{"title":"Redis集群搭建","slug":"搭建Redis集群","date":"2019-03-15T04:14:36.433Z","updated":"2019-03-18T14:25:38.052Z","comments":true,"path":"2019/03/15/搭建Redis集群/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/搭建Redis集群/","excerpt":"","text":"搭建Redis集群为什么要有集群之前我们已经讲了主从的概念，一主可以多从，如果同时的访问量过大(1000w),主服务肯定就会挂掉，数据服务就挂掉了或者发生自然灾难大公司都会有很多的服务器(华东地区、华南地区、华中地区、华北地区、西北地区、西南地区、东北地区、台港澳地区机房) 集群的概念集群是一组相互独立的、通过高速网络互联的计算机，它们构成了一个组，并以单一系统的模式加以管理。一个客户与集群相互作用时，集群像是一个独立的服务器。集群配置是用于提高可用性和可缩放性。当请求到来首先由负载均衡服务器处理，把请求转发到另外的一台服务器上。 redis集群分类 软件层面硬件层面 软件层面：只有一台电脑，在这一台电脑上启动了多个redis服务。硬件层面：存在多台实体的电脑，每台电脑上都启动了一个redis或者多个redis服务。 ##搭建集群当前拥有两台主机172.16.179.130、172.16.179.131，这⾥的IP在使⽤时要改为实际值参考阅读redis集群搭建 http://www.cnblogs.com/wuxl360/p/5920330.html[Python]搭建redis集群 http://blog.5ibc.net/p/51020.html 配置机器1 在演示中，172.16.179.130为当前ubuntu机器的ip在172.16.179.130上进⼊Desktop⽬录，创建conf⽬录在conf⽬录下创建⽂件7000.conf，编辑内容如下 12345678port 7000bind 172.16.179.130daemonize yespidfile 7000.pidcluster-enabled yescluster-config-file 7000_node.confcluster-node-timeout 15000appendonly yes 在conf⽬录下创建⽂件7001.conf，编辑内容如下12345678port 7001bind 172.16.179.130daemonize yespidfile 7001.pidcluster-enabled yescluster-config-file 7001_node.confcluster-node-timeout 15000appendonly yes 在conf⽬录下创建⽂件7002.conf，编辑内容如下12345678port 7002bind 172.16.179.130daemonize yespidfile 7002.pidcluster-enabled yescluster-config-file 7002_node.confcluster-node-timeout 15000appendonly yes 总结：三个⽂件的配置区别在port、pidfile、cluster-config-file三项 使⽤配置⽂件启动redis服务 redis-server 7000.confredis-server 7001.confredis-server 7002.conf 查看进程如下图 配置机器2 在演示中，172.16.179.131为当前ubuntu机器的ip在172.16.179.131上进⼊Desktop⽬录，创建conf⽬录在conf⽬录下创建⽂件7003.conf，编辑内容如下12345678port 7003bind 172.16.179.131daemonize yespidfile 7003.pidcluster-enabled yescluster-config-file 7003_node.confcluster-node-timeout 15000appendonly yes 在conf⽬录下创建⽂件7004.conf，编辑内容如下12345678port 7004bind 172.16.179.131daemonize yespidfile 7004.pidcluster-enabled yescluster-config-file 7004_node.confcluster-node-timeout 15000appendonly yes 在conf⽬录下创建⽂件7005.conf，编辑内容如下12345678port 7005bind 172.16.179.131daemonize yespidfile 7005.pidcluster-enabled yescluster-config-file 7005_node.confcluster-node-timeout 15000appendonly yes 总结：三个⽂件的配置区别在port、pidfile、cluster-config-file三项 使⽤配置⽂件启动redis服务 redis-server 7003.confredis-server 7004.confredis-server 7005.conf 查看进程如下图 创建集群redis的安装包中包含了redis-trib.rb，⽤于创建集群接下来的操作在172.16.179.130机器上进⾏将命令复制，这样可以在任何⽬录下调⽤此命令 sudo cp /usr/share/doc/redis-tools/examples/redis-trib.rb /usr/local/bin/ 安装ruby环境，因为redis-trib.rb是⽤ruby开发的 sudo apt-get install ruby 在提示信息处输⼊y，然后回⻋继续安装运⾏如下命令创建集群 redis-trib.rb create –replicas 1 172.16.179.130:7000 172.16.179.130:7001 172.16.179.130:7002 172.16.179.131:7003 172.16.179.131:7004 172.16.179.131:7005 执⾏上⾯这个指令在某些机器上可能会报错,主要原因是由于安装的 ruby 不是最 新版本! 天朝的防⽕墙导致⽆法下载最新版本,所以需要设置 gem 的源 解决办法如下 – 先查看⾃⼰的 gem 源是什么地址gem source -l – 如果是https://rubygems.org/ 就需要更换– 更换指令为gem sources –add https://gems.ruby-china.org/ –remove https://rubygems.org/– 通过 gem 安装 redis 的相关依赖sudo gem install redis– 然后重新执⾏指令redis-trib.rb create –replicas 1 172.16.179.130:7000 172.16.179.130:7001 172.16.179.130:7002 172.16.179.131:7003 172.16.179.131:7004 172.16.179.131:7005 提示如下主从信息，输⼊yes后回⻋提示完成，集群搭建成功 数据验证根据上图可以看出，当前搭建的主服务器为7000、7001、7003，对应的从服务器是7004、7005、7002在172.16.179.131机器上连接7002，加参数-c表示连接到集群 redis-cli -h 172.16.179.131 -c -p 7002 写⼊数据 set name itheima⾃动跳到了7003服务器，并写⼊数据成功在7003可以获取数据，如果写入数据又重定向到7000(负载均衡) 在哪个服务器上写数据：CRC16 redis cluster在设计的时候，就考虑到了去中⼼化，去中间件，也就是说，集群中 的每个节点都是平等的关系，都是对等的，每个节点都保存各⾃的数据和整个集 群的状态。每个节点都和其他所有节点连接，⽽且这些连接保持活跃，这样就保 证了我们只需要连接集群中的任意⼀个节点，就可以获取到其他节点的数据Redis集群没有并使⽤传统的⼀致性哈希来分配数据，⽽是采⽤另外⼀种叫做哈希 槽 (hash slot)的⽅式来分配的。redis cluster 默认分配了 16384 个slot，当我们 set⼀个key 时，会⽤CRC16算法来取模得到所属的slot，然后将这个key 分到哈 希槽区间的节点上，具体算法就是：CRC16(key) % 16384。所以我们在测试的 时候看到set 和 get 的时候，直接跳转到了7000端⼝的节点Redis 集群会把数据存在⼀个 master 节点，然后在这个 master 和其对应的salve 之间进⾏数据同步。当读取数据时，也根据⼀致性哈希算法到对应的 master 节 点获取数据。只有当⼀个master 挂掉之后，才会启动⼀个对应的 salve 节点，充 当 master需要注意的是：必须要3个或以上的主节点，否则在创建集群时会失败，并且当存 活的主节点数⼩于总节点数的⼀半时，整个集群就⽆法提供服务了 Python交互安装包如下 pip install redis-py-cluster redis-py-cluster源码地址https://github.com/Grokzen/redis-py-cluster 创建⽂件redis_cluster.py，示例码如下12345678910111213141516171819from rediscluster import *if __name__ == &apos;__main__&apos;: try: # 构建所有的节点，Redis会使⽤CRC16算法，将键和值写到某个节点上 startup_nodes = [ &#123;&apos;host&apos;: &apos;192.168.26.128&apos;, &apos;port&apos;: &apos;7000&apos;&#125;, &#123;&apos;host&apos;: &apos;192.168.26.130&apos;, &apos;port&apos;: &apos;7003&apos;&#125;, &#123;&apos;host&apos;: &apos;192.168.26.128&apos;, &apos;port&apos;: &apos;7001&apos;&#125;, ] # 构建StrictRedisCluster对象 src=StrictRedisCluster(startup_nodes=startup_nodes,decode_responses=True) # 设置键为name、值为itheima的数据 result=src.set(&apos;name&apos;,&apos;itheima&apos;) print(result) # 获取键为name name = src.get(&apos;name&apos;) print(name) except Exception as e: print(e)","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/categories/数据库/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"Redis主从搭建","slug":"Redis主从搭建","date":"2019-03-15T03:57:22.091Z","updated":"2019-03-18T14:25:43.547Z","comments":true,"path":"2019/03/15/Redis主从搭建/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/Redis主从搭建/","excerpt":"","text":"Redis主从搭建主从概念⼀个master可以拥有多个slave，⼀个slave⼜可以拥有多个slave，如此下去，形成了强⼤的多级服务器集群架构master用来写数据，slave用来读数据，经统计：网站的读写比率是10:1通过主从配置可以实现读写分离master和slave都是一个redis实例(redis服务) 主从配置配置主查看当前主机的ip地址 config修改etc/redis/redis.conf文件 sudo vi redis.confbind 192.168.26.128 重启redis服务 sudo service redis stopredis-server redis.conf 配置从复制etc/redis/redis.conf文件 sudo cp redis.conf ./slave.conf 修改redis/slave.conf文件 sudo vi slave.conf 编辑内容 bind 192.168.26.128slaveof 192.168.26.128 6379port 6378 redis服务 sudo redis-server slave.conf 查看主从关系 redis-cli -h 192.168.26.128 info Replication ##数据操作在master和slave分别执⾏info命令，查看输出信息 进入主客户端 redis-cli -h 192.168.26.128 -p 6379 进入从的客户端 redis-cli -h 192.168.26.128 -p 6378 在master上写数据 get aa aa在slave上读数据 get aa","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/categories/数据库/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"Redis与Python交互","slug":"Redis与Python交互","date":"2019-03-15T03:27:29.930Z","updated":"2019-03-18T14:25:31.166Z","comments":true,"path":"2019/03/15/Redis与Python交互/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/Redis与Python交互/","excerpt":"","text":"Redis与Python交互StrictRedis对象⽅法 通过init创建对象，指定参数host、port与指定的服务器和端⼝连接，host默认为localhost，port默认为6379，db默认为01sr = StrictRedis(host=&apos;localhost&apos;, port=6379, db=0) 简写 sr=StrictRedis() 根据不同的类型，拥有不同的实例⽅法可以调⽤，与前⾯学的redis命令对应，⽅法需要的参数与命令的参数⼀致 string setsetexmsetappendgetmgetkeykeysexiststypedeleteexpiregetrangettl hash hsethmsethkeyshgethmgethvalshdel list lpushrpushlinsertlrangelsetlrem set saddsmemberssrem zset zaddzrangezrangebyscorezscorezremzremrangebyscore 使用StrictRedis对象对string类型数据进行增删改查在桌面上创建redis目录使用pycharm打开 redis目录创建redis_string.py文件12345678from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() except Exception as e: print(e) string-增加⽅法set，添加键、值，如果添加成功则返回True，如果添加失败则返回False编写代码如下1234567891011from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #添加键name，值为itheima result=sr.set(&apos;name&apos;,&apos;itheima&apos;) #输出响应结果，如果添加成功则返回True，否则返回False print(result) except Exception as e: print(e) string-获取⽅法get，添加键对应的值，如果键存在则返回对应的值，如果键不存在则返回None编写代码如下1234567891011from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #获取键name的值 result = sr.get(&apos;name&apos;) #输出键的值，如果键不存在则返回None print(result) except Exception as e: print(e) string-修改⽅法set，如果键已经存在则进⾏修改，如果键不存在则进⾏添加编写代码如下1234567891011from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #设置键name的值，如果键已经存在则进⾏修改，如果键不存在则进⾏添加 result = sr.set(&apos;name&apos;,&apos;itcast&apos;) #输出响应结果，如果操作成功则返回True，否则返回False print(result) except Exception as e: print(e) string-删除⽅法delete，删除键及对应的值，如果删除成功则返回受影响的键数，否则则返 回0编写代码如下1234567891011rom redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #设置键name的值，如果键已经存在则进⾏修改，如果键不存在则进⾏添加 result = sr.delete(&apos;name&apos;) #输出响应结果，如果删除成功则返回受影响的键数，否则则返回0 print(result) except Exception as e: print(e) 获取键⽅法keys，根据正则表达式获取键编写代码如下1234567891011from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #获取所有的键 result=sr.keys() #输出响应结果，所有的键构成⼀个列表，如果没有键则返回空列表 print(result) except Exception as e: print(e)","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/categories/数据库/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"NoSql之Redis(2)","slug":"NoSql之Redis(2)","date":"2019-03-15T03:20:45.585Z","updated":"2019-03-18T14:25:14.228Z","comments":true,"path":"2019/03/15/NoSql之Redis(2)/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/NoSql之Redis(2)/","excerpt":"","text":"NoSql之Redis(2)服务器端和客户端的命令服务器端服务器端的命令为redis-server 可以使⽤help查看帮助⽂档 redis-server –help 个人习惯 ps aux | grep redis 查看redis服务器进程sudo kill -9 pid 杀死redis服务器sudo redis-server /etc/redis/redis.conf 指定加载的配置文件 客户端客户端的命令为redis-cli可以使⽤help查看帮助⽂档 redis-cli –help 连接redis redis-cli 运⾏测试命令 ping 切换数据库 数据库没有名称，默认有16个，通过0-15来标识，连接redis默认选择第一个数据库 select 10 数据结构redis是key-value的数据结构，每条数据都是⼀个键值对键的类型是字符串注意：键不能重复 值的类型分为五种： 字符串string哈希hash列表list集合set有序集合zset 数据操作行为 保存修改获取删除 点击中⽂官⽹查看命令⽂档http://redis.cn/commands.html string类型字符串类型是 Redis 中最为基础的数据存储类型，它在 Redis 中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据或Json对象描述信息等。在Redis中字符串类型的Value最多可以容纳的数据长度是512M。 保存如果设置的键不存在则为添加，如果设置的键已经存在则修改 设置键值 set key value 例1：设置键为name值为itcast的数据 set name itcast 设置键值及过期时间，以秒为单位 setex key seconds value 例2：设置键为aa值为aa过期时间为3秒的数据 setex aa 3 aa 设置多个键值 mset key1 value1 key2 value2 … 例3：设置键为’a1’值为’python’、键为’a2’值为’java’、键为’a3’值为’c’ mset a1 python a2 java a3 c 追加值 append key value 例4：向键为a1中追加值’ haha’ append ‘a1’ ‘haha’ 获取获取：根据键获取值，如果不存在此键则返回nil get key 例5：获取键’name’的值 get ‘name’ 根据多个键获取多个值 mget key1 key2 … 例6：获取键a1、a2、a3’的值 mget a1 a2 a3 键命令查找键，参数⽀持正则表达式 keys pattern 例1：查看所有键 keys * 例2：查看名称中包含a的键 keys ‘a*’ 判断键是否存在，如果存在返回1，不存在返回0 exists key1 例3：判断键a1是否存在 exists a1 查看键对应的value的类型 type key 例4：查看键a1的值类型，为redis⽀持的五种类型中的⼀种 type a1 删除键及对应的值 del key1 key2 … 例5：删除键a2、a3 del a2 a3设置过期时间，以秒为单位 如果没有指定过期时间则⼀直存在，直到使⽤DEL移除 expire key seconds 例6：设置键’a1’的过期时间为3秒 expire ‘a1’ 3查看有效时间，以秒为单位 ttl key 例7：查看键’bb’的有效时间 ttl bb hash类型hash⽤于存储对象，对象的结构为属性、值值的类型为string 增加、修改设置单个属性 hset key field value 例1：设置键 user的属性name为itheima hset user name itheima MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error. Redis被配置为保存数据库快照，但它目前不能持久化到硬盘。用来修改集合数据的命令不能用 原因： 强制关闭Redis快照导致不能持久化。 解决方案： 运行config set stop-writes-on-bgsave-error no 命令后，关闭配置项stop-writes-on-bgsave-error解决该问题。 设置多个属性 hmset key field1 value1 field2 value2 … 例2：设置键u2的属性name为itcast、属性age为11 hmset u2 name itcast age 11 获取获取指定键所有的属性 hkeys key 例3：获取键u2的所有属性 hkeys u2 获取⼀个属性的值 hget key field 例4：获取键u2属性’name’的值 hget u2 ‘name’ 获取多个属性的值 hmget key field1 field2 … 例5：获取键u2属性’name’、’age的值 hmget u2 name age 获取所有属性的值 hvals key 例6：获取键’u2’所有属性的值 hvals u2 删除删除整个hash键及值，使⽤del命令删除属性，属性对应的值会被⼀起删除 hdel key field1 field2 … 例7：删除键’u2’的属性’age’ hdel u2 age list类型列表的元素类型为string按照插⼊顺序排序 增加在左侧插⼊数据 lpush key value1 value2 … 例1：从键为’a1’的列表左侧加⼊数据a 、 b 、c lpush a1 a b c 在右侧插⼊数据 rpush key value1 value2 … 例2：从键为’a1’的列表右侧加⼊数据0 1 rpush a1 0 1 在指定元素的前或后插⼊新元素 linsert key before或after 现有元素 新元素 例3：在键为’a1’的列表中元素’b’前加⼊’3’ linsert a1 before b 3&gt; 获取返回列表⾥指定范围内的元素 start、stop为元素的下标索引索引从左侧开始，第⼀个元素为0索引可以是负数，表示从尾部开始计数，如-1表示最后⼀个元素 lrange key start stop 例4：获取键为’a1’的列表所有元素 lrange a1 0 -1 设置指定索引位置的元素值索引从左侧开始，第⼀个元素为0索引可以是负数，表示尾部开始计数，如-1表示最后⼀个元素 lset key index value 例5：修改键为’a1’的列表中下标为1的元素值为’z’ lset a 1 z 删除删除指定元素 将列表中前count次出现的值为value的元素移除count &gt; 0: 从头往尾移除count &lt; 0: 从尾往头移除count = 0: 移除所有 lrem key count value 例6.1：向列表’a2’中加⼊元素’a’、’b’、’a’、’b’、’a’、’b’ lpush a2 a b a b a b例6.2：从’a2’列表右侧开始删除2个’b’ lrem a2 -2 b 例6.3：查看列表’py12’的所有元素 lrange a2 0 -1 set类型⽆序集合元素为string类型元素具有唯⼀性，不重复说明：对于集合没有修改操作 增加添加元素 sadd key member1 member2 … 例1：向键’a3’的集合中添加元素’zhangsan’、’lisi’、’wangwu’ sadd a3 zhangsan sili wangwu 获取返回所有的元素 smembers key 例2：获取键’a3’的集合中所有元素 smembers a3 删除删除指定元素 srem key 例3：删除键’a3’的集合中元素’wangwu’ srem a3 wangwu zset类型sorted set，有序集合元素为string类型元素具有唯⼀性，不重复每个元素都会关联⼀个double类型的score，表示权重，通过权重将元素从⼩到⼤排序说明：没有修改操作 增加添加 zadd key score1 member1 score2 member2 … 例1：向键’a4’的集合中添加元素’lisi’、’wangwu’、’zhaoliu’、’zhangsan’，权重分别为4、5、6、3 zadd a4 4 lisi 5 wangwu 6 zhaoliu 3 zhangsan 获取返回指定范围内的元素 start、stop为元素的下标索引 索引从左侧开始，第⼀个元素为0索引可以是负数，表示从尾部开始计数，如-1表示最后⼀个元素 zrange key start stop 例2：获取键’a4’的集合中所有元素 zrange a4 0 -1 返回score值在min和max之间的成员 zrangebyscore key min max 例3：获取键’a4’的集合中权限值在5和6之间的成员 zrangebyscore a4 5 6 返回成员member的score值 zscore key member 例4：获取键’a4’的集合中元素’zhangsan’的权重 zscore a4 zhangsan 删除指定元素 zrem key member1 member2 … 例5：删除集合’a4’中元素’zhangsan’ zrem a4 zhangsan删除权重在指定范围的元素 zremrangebyscore key min max 例6：删除集合’a4’中权限在5、6之间的元素 zremrangebyscore a4 5 6","categories":[{"name":"数据库","slug":"数据库","permalink":"https://zem12345678.github.io/categories/数据库/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"NoSql之Redis(1)","slug":"NoSql之Redis(1)","date":"2019-03-15T02:26:49.727Z","updated":"2019-03-15T02:29:07.208Z","comments":true,"path":"2019/03/15/NoSql之Redis(1)/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/NoSql之Redis(1)/","excerpt":"","text":"NoSql之Redis(1)Nosql介绍NoSQL：一类新出现的数据库(not only sql)泛指非关系型的数据库不支持SQL语法存储结构跟传统关系型数据库中的那种关系表完全不同，nosql中存储的数据都是KV形式NoSQL的世界中没有一种通用的语言，每种nosql数据库都有自己的api和语法，以及擅长的业务场景NoSQL中的产品种类相当多： MongodbRedisHbase hadoopCassandra hadoop NoSQL和SQL数据库的比较： 适用场景不同：sql数据库适合用于关系特别复杂的数据查询场景，nosql反之“事务”特性的支持：sql对事务的支持非常完善，而nosql基本不支持事务两者在不断地取长补短，呈现融合趋势 Redis简介 Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。从2010年3月15日起，Redis的开发工作由VMware主持。从2013年5月开始，Redis的开发由Pivotal赞助。Redis是 NoSQL技术阵营中的一员，它通过多种键值数据类型来适应不同场景下的存储需求，借助一些高层级的接口使用其可以胜任，如缓存、队列系统的不同角色 Redis特性 Redis 与其他 key - value 缓存产品有以下三个特点：Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。Redis支持数据的备份，即master-slave模式的数据备份。 Redis 优势 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 Redis应用场景 用来做缓存(ehcache/memcached)——redis的所有数据是放在内存中的（内存数据库）可以在某些特定应用场景下替代传统数据库——比如社交类的应用在一些大型系统中，巧妙地实现一些特定的功能：session共享、购物车只要你有丰富的想象力，redis可以用在可以给你无限的惊喜……. Redis 安装当前redis最新稳定版本是4.0.9当前ubuntu虚拟机中已经安装好了redis，以下步骤可以跳过 最新稳定版本下载链接： http://download.redis.io/releases/redis-4.0.9.tar.gzstep1:下载 wget http://download.redis.io/releases/redis-4.0.9.tar.gz step2:解压 tar xzf redis-4.0.9.tar.gz step3:移动，放到usr/local⽬录下 sudo mv ./redis-4.0.9 /usr/local/redis/ step4:进⼊redis⽬录 cd /usr/local/redis/ step5:生成 sudo makestep7:安装,将redis的命令安装到/usr/local/bin/⽬录 sudo make install step8:安装完成后，我们进入目录/usr/local/bin中查看 cd /usr/local/binls -all redis-server redis服务器redis-cli redis命令行客户端redis-benchmark redis性能测试工具redis-check-aof AOF文件修复工具redis-check-rdb RDB文件检索工具 step9:配置⽂件，移动到/etc/⽬录下 配置⽂件⽬录为/usr/local/redis/redis.conf sudo cp /usr/local/redis/redis.conf /etc/redis/ Mac 上安装 Redis:安装 Homebrew：https://brew.sh/ 使用 brew 安装 Redishttps://www.cnblogs.com/cloudshadow/p/mac_brew_install_redis.html 配置Redis的配置信息在/etc/redis/redis.conf下。 查看 sudo vi /etc/redis/redis.conf 核心配置选项绑定ip：如果需要远程访问，可将此⾏注释，或绑定⼀个真实ip bind 127.0.0.1 端⼝，默认为6379 port 6379 是否以守护进程运⾏ 如果以守护进程运⾏，则不会在命令⾏阻塞，类似于服务如果以⾮守护进程运⾏，则当前终端被阻塞设置为yes表示守护进程，设置为no表示⾮守护进程推荐设置为yes daemonize yes 数据⽂件 dbfilename dump.rdb 数据⽂件存储路径 dir /var/lib/redis ⽇志⽂件 logfile “/var/log/redis/redis-server.log” 数据库，默认有16个 database 16 主从复制，类似于双机备份。 slaveof 参考资料redis配置信息http://blog.csdn.net/ljphilp/article/details/52934933","categories":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"Django 之Xadmin的使用","slug":"Django 之Xadmin的使用","date":"2019-03-14T15:26:30.264Z","updated":"2019-03-14T15:27:09.638Z","comments":true,"path":"2019/03/14/Django 之Xadmin的使用/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django 之Xadmin的使用/","excerpt":"","text":"Django 之Xadmin的使用xadmin是Django的第三方扩展，可是使Django的admin站点使用更方便。 1. 安装通过如下命令安装xadmin的最新版 pip install https://github.com/sshwsfc/xadmin/tarball/master 在配置文件中注册如下应用 1234567 INSTALLED_APPS = [ ... &apos;xadmin&apos;, &apos;crispy_forms&apos;, &apos;reversion&apos;, ...] xadmin有建立自己的数据库模型类，需要进行数据库迁移 python manage.py migrate在总路由中添加xadmin的路由信息1234567import xadminurlpatterns = [ # url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;xadmin/&apos;, include(xadmin.site.urls)), ...] 2. 使用xadmin不再使用Django的admin.py，而是需要编写代码在adminx.py文件中。xadmin的站点管理类不用继承admin.ModelAdmin，而是直接继承object即可。在goods应用中创建adminx.py文件。 站点的全局配置12345678910111213141516171819import xadminfrom xadmin import viewsfrom goods import modelsclass BaseSetting(object): &quot;&quot;&quot;xadmin的基本配置&quot;&quot;&quot; enable_themes = True # 开启主题切换功能 use_bootswatch = Truexadmin.site.register(views.BaseAdminView, BaseSetting)class GlobalSettings(object): &quot;&quot;&quot;xadmin的全局配置&quot;&quot;&quot; site_title = &quot;美多商城运营管理系统&quot; # 设置站点标题 site_footer = &quot;美多商城集团有限公司&quot; # 设置站点的页脚 menu_style = &quot;accordion&quot; # 设置菜单折叠xadmin.site.register(views.CommAdminView, GlobalSettings) 站点Model管理xadmin可以使用的页面样式控制基本与Django原生的admin一直。 list_display 控制列表展示的字段search_fields 控制可以通过搜索框搜索的字段名称，xadmin使用的是模糊查询list_filter 可以进行过滤操作的列ordering 默认排序的字段readonly_fields 在编辑页面的只读字段exclude 在编辑页面隐藏的字段list_editable 在列表页可以快速直接编辑的字段show_detail_fileds 在列表页提供快速显示详情信息refresh_times 指定列表页的定时刷新list_export 控制列表页导出数据的可选格式data_charts *控制显示图标的样式model_icon 控制菜单的图标1）model_icon1234class SKUAdmin(object): model_icon = &apos;fa fa-gift&apos;xadmin.site.register(models.SKU, SKUAdmin) 可选的图标样式参考http://fontawesome.dashgame.com/2） list_display1list_display = [&apos;id&apos;, &apos;name&apos;, &apos;price&apos;, &apos;stock&apos;, &apos;sales&apos;, &apos;comments&apos;] 3）search_fields1search_fields = [&apos;id&apos;,&apos;name&apos;] 4）list_filter1list_filter = [&apos;category&apos;] 5）list_editable 1list_editable = [&apos;price&apos;, &apos;stock&apos;] 6）show_detail_fields 1show_detail_fields = [&apos;name&apos;] 7）list_export 1list_export = [&apos;xls&apos;, &apos;csv&apos;, &apos;xml&apos;] 8）refresh_times 123456lass OrderAdmin(object): list_display = [&apos;order_id&apos;, &apos;create_time&apos;, &apos;total_amount&apos;, &apos;pay_method&apos;, &apos;status&apos;] refresh_times = [3, 5] # 可选以支持按多长时间(秒)刷新页面``` ![enter image description here](https://ww1.sinaimg.cn/large/007rAy9hly1g12qk7eyyuj30lq04dt9e.jpg)9）data_charts data_charts = { &quot;order_amount&quot;: {&apos;title&apos;: &apos;订单金额&apos;, &quot;x-field&quot;: &quot;create_time&quot;, &quot;y-field&quot;: (&apos;total_amount&apos;,), &quot;order&quot;: (&apos;create_time&apos;,)}, &quot;order_count&quot;: {&apos;title&apos;: &apos;订单量&apos;, &quot;x-field&quot;: &quot;create_time&quot;, &quot;y-field&quot;: (&apos;total_count&apos;,), &quot;order&quot;: (&apos;create_time&apos;,)}, } 12345678&gt;title 控制图标名称x-field 控制x轴字段y-field 控制y轴字段，可以是多个值order 控制默认排序![enter image description here](https://ww1.sinaimg.cn/large/007rAy9hgy1g12qld0x5qj30la07vjru.jpg)10）readonly_fields class SKUAdmin(object): … readonly_fields = [‘sales’, ‘comments’]12345678910111213![enter image description here](https://ww1.sinaimg.cn/large/007rAy9hgy1g12qm5ep5sj30l704f744.jpg)站点保存对象数据方法重写在Django的原生admin站点中，如果想要在站点保存或删除数据时，补充自定义行为，可以重写如下方法：&gt;save_model(self, request, obj, form, change)delete_model(self, request, obj)而在xadmin中，需要重写如下方法：&gt;save_models(self)delete_model(self)在方法中，如果需要用到当前处理的模型类对象，需要通过self.obj来获取，如 class SKUSpecificationAdmin(object): def save_models(self): # 保存数据对象 obj = self.new_obj obj.save() # 补充自定义行为 from celery_tasks.html.tasks import generate_static_sku_detail_html generate_static_sku_detail_html.delay(obj.sku.id) def delete_model(self): # 删除数据对象 obj = self.obj sku_id = obj.sku.id obj.delete() # 补充自定义行为 from celery_tasks.html.tasks import generate_static_sku_detail_html generate_static_sku_detail_html.delay(sku_id) 123456### 自定义用户管理xadmin会自动为admin站点添加用户User的管理配置xadmin使用xadmin.plugins.auth.UserAdmin来配置如果需要自定义User配置的话，需要先unregister(User)，在添加自己的User配置并注册 import xadmin Register your models here.from users.models import Userfrom xadmin.plugins import auth class UserAdmin(auth.UserAdmin): list_display = [‘id’, ‘username’, ‘mobile’, ‘email’, ‘date_joined’] readonly_fields = [‘last_login’, ‘date_joined’] search_fields = (‘username’, ‘first_name’, ‘last_name’, ‘email’, ‘mobile’) style_fields = {‘user_permissions’: ‘m2m_transfer’, ‘groups’: ‘m2m_transfer’} def get_model_form(self, **kwargs): if self.org_obj is None: self.fields = [&apos;username&apos;, &apos;mobile&apos;, &apos;is_staff&apos;] return super().get_model_form(**kwargs) xadmin.site.unregister(User)xadmin.site.register(User, UserAdmin)`","categories":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"}]},{"title":"使用近似算法解决旅行商（TSP）问题","slug":"使用近似算法解决旅行商（TSP）问题","date":"2019-03-14T14:49:14.098Z","updated":"2019-03-14T14:50:40.372Z","comments":true,"path":"2019/03/14/使用近似算法解决旅行商（TSP）问题/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/使用近似算法解决旅行商（TSP）问题/","excerpt":"","text":"使用近似算法解决旅行商（TSP）问题什么是TSP问题？谓TSP问题（Travelling Salesman Problem）旅行商问题，即最短路径问题，就是在给定的起始点S到终止点T的通路集合中，寻求距离最小的通路，这样的通路称为S点到T点的最短路径。TSP是一种完全NP问题如果旅行商问题的权值函数满足三角不等式，即c(u,w)≤c(u,v) + c(v,w)对任意u,v,w都成立，则称它满足三角不等式。无论旅行商问题是否满足三角不等式，它均是NP-完全问题。相关定理表明，不满足三角不等式的旅行商问题不存在常数近似比的近似算法，除非NP=P。 近似算法设计近似算法是指能够在多项式时间内给出优化问题的近似优化解的算法，近似算法不仅可用于近似求解NP-完全问题，也可用于近似求解复杂度较高的P问题。 任意选择V中的一个顶点r，作为树根节点; 调用Prim算法得到图G(V,E)的最小生成树T*; 先序遍历T，访问T中的每条边两遍，得到顶点序列L; 删除L中的重复顶点形成哈密顿环C； 输出C. 算法的性能分析 时间复杂度近似算法的性能分析包括时间复杂度分析、空间复杂度分析和近似精度分析，其中时间（空间）复杂度的分析同精确复杂度相同。近似精度分析是近似算法特有的，它主要用于刻画近似算法给出的近似解相比于问题优化解的优劣程度。目前，存在三种刻画近似精度的度量，即近似比、相对误差界和1+ε近似。 该算法的时间复杂度为O(|V|^2 log|V|)。事实上，第2步开销为O(|E| log|V|)且图G是完全图，O(|E| log|V|)等于O(|V|^2 log|V|)。第3~4步的开销为O(|V|)，因为最小生成树恰有|V| - 1条边。 近似精度近似比：设A是一个优化问题的近似算法，A具有近似比（ratio bound） p(n), 如果max{C/C, C/C} ≤ p(n)。其中n是输入大小，C是A产生的解的代价，C是优化解的代价。相对误差：对于任意输入，近似算法的相对误差定义为|C - C|/C,其中C是近似解的代价，C是优化解的代价。相对误差界：一个近似算法的相对误差界为ε(n),如果|C-C|/C ≤ ε(n)。近似模式：一个优化问题的近似模式是一个以问题实例I和ε&gt;0位输入的算法。对于任意固定的ε，近似模式是一个(1+ε)-近似算法。一个近似模式A(I,ε)称为一个多项式时间近似模式，如果对于任意ε&gt;0, A(I,ε)的运行时间是|I|的多项式。一个近似模式称为完全多项式时间近似模式，如果它的运行时间是关于I/ε和输入实例大小n的多项式。对于近似解C，由于L是遍历T的每条边两次得到的回路，则有c(L) = 2 c(T), 而C又是关于L删除某些重复边后得到的结果，因此C ≤ 2 c(T)。 对于优化解C，由于C是一个简单环，则删除任一条边便可生成树，而且该树的代价一定不低于最小生成树的代价，因此有c(C) ≥ c(T)。综上，有C ≤ 2 c(T) ≤ 2 C。所以C / C ≤ 2，该算法的近似比为2。 近似算法解决TSP问题过程TSP问题的实质可以抽象为在一个带权重的完全无向图中，找到一个权值总和最小的哈密顿回路TSP问题翻译为数学语言为，在N个城市的完全无向图G中 其中每个城市之间的距离矩阵为 目标函数为 需要求解的变量为w，w是使得目标函数达到最小值的一个排列 且w的最后一项满足回到出发城市 满足三角不等式的TSP模型和算法步骤我们从费用函数出发，费用函数也叫代价函数，指的是两个城市之间的费用指数或者代价程度的量化。在大多数的实际情况中，从一个地方u直接到另一个地方w，这个走法花费的代价总是最小的，而如果从u到w需要经过某个中转站v，则这种走法花费的代价却不可能比直接到达的走法花费的代价更小将上述的理论转化为数学语言为其中c是费用函数，这个方程说明了，直接从u-&gt;w花费的代价，要比从u-&gt;v-&gt;w花费的代价要小，我们称这个费用函数满足三角不等式三角不等式的定义为：任意一个欧拉平面的三角形两边之和始终大于第三边，这是一个非常自然的不等式，其中欧拉平面上任意两点之间的欧式距离就满足三角不等式，为此，我们只要设TSP中的费用函数为欧式距离，即可将TSP问题转化为满足三角不等式的TSP模型 近似算法的解题步骤求解上述TSP模型的步骤（1）选择G的任意一个顶点r作为根节点(出发/结束点)（2）用Prim算法找出G的一棵以r为根的最小生成树T（3）前序遍历访问树T，得到遍历顺序组成的顶点表L（4）将r加到顶点表L的末尾，按L中顶点的次序组成哈密顿回路H数学上已经证明，当费用函数满足三角不等式时，上述近似算法找出的哈密顿回路产生的总费用，不会超过最优回路的2倍 图的存储结构首先我们需要将图表示为我们熟悉的数据结构，图可以使用两种存储结构，分别是邻接链表和邻接矩阵邻接链表：是一个由链表组成的一维数组，数组中每个元素都存储以每个顶点为表头的链表邻接矩阵：以矩阵的形式存储图中所有顶点之间的关系用链表表示图的关系，会显得数据结构较为复杂，但节省空间的开销，而用矩阵来表示图的关系就显得非常清晰，但空间开销较大，这里我们选择邻接矩阵来表示TSP案例中的无向图G我们设欧式距离为费用函数，矩阵中的每一行代表G中每一个的顶点到其余各个顶点的费用(欧式距离)，如果出现到达不了或者自身到达自身的情况，我们用无穷大inf来填充表示不可达123456789101112def price_cn(vec1, vec2): return np.linalg.norm(np.array(vec1) - np.array(vec2))# 从去过的点中，找到连接到未去过的点的边里，最小的代价边(贪心算法)def find_min_edge(visited_ids, no_visited_ids): min_weight, min_from, min_to = np.inf, np.inf, np.inf for from_index in visited_ids: for to_index, weight in enumerate(G[from_index]): if from_index != to_index and weight &lt; min_weight and to_index in no_visited_ids: min_to = to_index min_from = from_index min_weight = G[min_from][min_to] return (min_from, min_to), min_weight Prim最小生成树算法两点可以确定一条直线，则最小生成树的定义为：用n-1条边连接具有n个顶点的无向图G，并且使得边长的总和最小接下来我们需要找到G中的这颗最小生成树T，从T的定义可知，T满足（1）T有且只有n-1条边（2）T的总长度达到最小值这里我们使用Prim算法来生成T，Prim算法的策略步骤为（1）设集合V是G中所有的顶点，集合U是G中已经走过的顶点，集合U-V是G中没有走过的顶点（2）从G中的起点a开始遍历，将a加入到集合U中，并将a从集合U-V替出（3）在集合U-V中剩余的n-1个顶点中寻找与集合U中的a关联，且权重最小的那条边的终点b，将b加入到集合U中，并将b从集合U-V替出（4）同理，在集合U-V中剩余的n-2个顶点中寻找与集合U中的a或b关联，且权重最小的那条边的终点c，将c加入到集合U中，并将c从集合U-V替出（5）重复步骤(4)，直到G中所有的顶点都加入到集合U，且集合U-V为空，则集合U中的顶点就构成了T显然，Prim算法的策略属于贪心算法，因为每一步所加入的边，都必须是使得当前数T的总权重增加量最小的边12345678910111213def prim(G, root_index=0): visited_ids = [root_index] # 初始化去过的点的集合 T_path = [] while len(visited_ids) != G.shape[0]: no_visited_ids = contain_no_visited_ids(G, visited_ids) # 维护未去过的点的集合 (min_from, min_to), min_weight = find_min_edge(visited_ids, no_visited_ids) visited_ids.append(min_to) # 维护去过的点的集合 T_path.append((min_from, min_to)) T = np.full_like(G, np.inf) # 最小生成树的矩阵形式，n-1条边组成 for (from_, to_) in T_path: T[from_][to_] = G[from_][to_] T[to_][from_] = G[to_][from_] return T, T_path 遍历序遍历：访问根节点—&gt;前序遍历左子树—&gt;前序遍历右子树123456789101112def preorder_tree_walk(T, root_index=0): is_visited = [False] * T.shape[0] stack = [root_index] T_walk = [] while len(stack) != 0: node = stack.pop() T_walk.append(node) is_visited[node] = True nodes = np.where(T[node] != np.inf)[0] if len(nodes) &gt; 0: [stack.append(node) for node in reversed(nodes) if is_visited[node] is False] return T_walk 哈密顿回路哈密顿回路的定义为：由指定的起点前往指定的终点，途中经过的城市有且只经过一次，所以一个无向图中含有若干个哈密顿回路按照近似算法的最后一步，我们将根节点加入到顶点表的末尾，将顶点表的顶点顺序依次连接，就得到哈密顿回路123456789def create_H(G, L): H = np.full_like(G, np.inf) H_path = [] for i, from_node in enumerate(L[0:-1]): to_node = L[i + 1] H[from_node][to_node] = G[from_node][to_node] H[to_node][from_node] = G[to_node][from_node] H_path.append((from_node, to_node)) return H, H_path 完成了近似算法的计算，找到了一个在G中从a出发，最后回到a，中间每个城市只经过一次的最小费用的行程走法，即计算出了目标函数的顶点排列w为 结果分析从结果上可以看出，近似算法是解决TSP问题的一种有效方法，它可以在多项式时间内计算出一个近似解，来逼近真实的最优解，这个近似解尽量的逼近满足TSP的条件（1）从开始点回到开始点，每个点都要经过且只经过一次（2）行程的总费用达到最小值近似算法求解TSP问题（3）近似算法是求解TSP问题的一个渐进式算法（4）近似解法求出的近似解和实际最优解的近似比不超过2，即w的总代价，在最优总代价的2倍之内 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&apos;&apos;&apos;TSP—近似算法1、选择G的任意一个顶点2、Prim算法找出找出最小生成树T3、前序遍历树T得到的顶点表L4、将根节点添加到L的末尾，按表L中顶点的次序组成哈密顿回路H&apos;&apos;&apos;import numpy as npimport matplotlib.pyplot as plt# 代价函数（具有三角不等式性质）def price_cn(vec1, vec2): return np.linalg.norm(np.array(vec1) - np.array(vec2))# 从去过的点中，找到连接到未去过的点的边里，最小的代价边(贪心算法)def find_min_edge(visited_ids, no_visited_ids): min_weight, min_from, min_to = np.inf, np.inf, np.inf for from_index in visited_ids: for to_index, weight in enumerate(G[from_index]): if from_index != to_index and weight &lt; min_weight and to_index in no_visited_ids: min_to = to_index min_from = from_index min_weight = G[min_from][min_to] return (min_from, min_to), min_weight# 维护未走过的点的集合def contain_no_visited_ids(G, visited_ids): no_visited_ids = [] # 还没有走过的点的索引集合 [no_visited_ids.append(idx) for idx, _ in enumerate(G) if idx not in visited_ids] return no_visited_ids# 生成最小生成树Tdef prim(G, root_index=0): visited_ids = [root_index] # 初始化去过的点的集合 T_path = [] while len(visited_ids) != G.shape[0]: no_visited_ids = contain_no_visited_ids(G, visited_ids) # 维护未去过的点的集合 (min_from, min_to), min_weight = find_min_edge(visited_ids, no_visited_ids) visited_ids.append(min_to) # 维护去过的点的集合 T_path.append((min_from, min_to)) T = np.full_like(G, np.inf) # 最小生成树的矩阵形式，n-1条边组成 for (from_, to_) in T_path: T[from_][to_] = G[from_][to_] T[to_][from_] = G[to_][from_] return T, T_path# 先序遍历图(最小生成树)的路径，得到顶点列表Ldef preorder_tree_walk(T, root_index=0): is_visited = [False] * T.shape[0] stack = [root_index] T_walk = [] while len(stack) != 0: node = stack.pop() T_walk.append(node) is_visited[node] = True nodes = np.where(T[node] != np.inf)[0] if len(nodes) &gt; 0: [stack.append(node) for node in reversed(nodes) if is_visited[node] is False] return T_walk# 生成哈密尔顿回路Hdef create_H(G, L): H = np.full_like(G, np.inf) H_path = [] for i, from_node in enumerate(L[0:-1]): to_node = L[i + 1] H[from_node][to_node] = G[from_node][to_node] H[to_node][from_node] = G[to_node][from_node] H_path.append((from_node, to_node)) return H, H_path# 可视化画出哈密顿回路def draw_H(citys, H_path): fig = plt.figure() ax = fig.add_subplot(111) plt.xlim(0, 7) plt.ylim(0, 7) for (from_, to_) in H_path: p1 = plt.Circle(citys[from_], 0.2, color=&apos;red&apos;) p2 = plt.Circle(citys[to_], 0.2, color=&apos;red&apos;) ax.add_patch(p1) ax.add_patch(p2) ax.plot((citys[from_][0], citys[to_][0]), (citys[from_][1], citys[to_][1]), color=&apos;red&apos;) ax.annotate(s=chr(97 + to_), xy=citys[to_], xytext=(-8, -4), textcoords=&apos;offset points&apos;, fontsize=20) ax.axis(&apos;equal&apos;) ax.grid() plt.show()if __name__ == &apos;__main__&apos;: citys = [(2, 6), (2, 4), (1, 3), (4, 6), (5, 5), (4, 4), (6, 4), (3, 2)] # 城市坐标 G = [] # 完全无向图 for i, curr_point in enumerate(citys): line = [] for j, other_point in enumerate(citys): line.append(price_cn(curr_point, other_point)) if i != j else line.append(np.inf) G.append(line) G = np.array(G) # 1、选择G的任意一个顶点 root_index = 0 # 2、Prim算法找出找出最小生成树T T, T_path = prim(G, root_index=root_index) # 3、前序遍历树T得到的顶点表L L = preorder_tree_walk(T, root_index=root_index) # 4、将根节点添加到L的末尾，按表L中顶点的次序组成哈密顿回路H L.append(root_index) H, H_path = create_H(G, L) print(&apos;最小生成树的路径为：&#123;&#125;&apos;.format(T_path)) [print(chr(97 + v), end=&apos;,&apos; if i &lt; len(L) - 1 else &apos;\\n&apos;) for i, v in enumerate(L)] print(&apos;哈密顿回路的路径为：&#123;&#125;&apos;.format(H_path)) print(&apos;哈密顿回路产生的代价为：&#123;&#125;&apos;.format(sum(G[from_][to_] for (from_, to_) in H_path))) # draw_H(citys, H_path)","categories":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/categories/算法/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"数据结构","slug":"数据结构","permalink":"https://zem12345678.github.io/tags/数据结构/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://zem12345678.github.io/tags/LeetCode/"}]},{"title":"Logistic回归对鸾尾花进行分类","slug":"Logistic回归对鸾尾花进行分类","date":"2019-03-14T14:22:10.666Z","updated":"2019-03-15T01:11:54.870Z","comments":true,"path":"2019/03/14/Logistic回归对鸾尾花进行分类/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Logistic回归对鸾尾花进行分类/","excerpt":"","text":"逻辑斯蒂回归对鸾尾花进行分类Sigmoid函数逻辑回归也被称为广义线性回归模型，它与线性回归模型的形式基本上相同，都具有 ax+b，其中a和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将ax+b作为因变量，即y = ax+b，而logistic回归则通过函数S将ax+b对应到一个隐状态p，p = S(ax+b)，然后根据p与1-p的大小决定因变量的值。这里的函数S就是Sigmoid函数 IRIS数据集介绍Iris也称鸢尾花卉数据集,是常用的分类实验数据集，由R.A. Fisher于1936年收集整理的。其中包含3种植物种类，分别是山鸢尾（setosa）变色鸢尾（versicolor）和维吉尼亚鸢尾（virginica），每类50个样本，共150个样本。 该数据集包含4个特征变量，1个类别变量。iris每个样本都包含了4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，以及1个类别变量（label）。我们需要建立一个分类器，分类器可以通过这4个特征来预测鸢尾花卉种类是属于山鸢尾，变色鸢尾还是维吉尼亚鸢尾。其中有一个类别是线性可分的，其余两个类别线性不可分，这在最后的分类结果绘制图中可观察到。 导入所需的包12345678import numpy as npfrom sklearn.linear_model import LogisticRegressionimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn import preprocessingimport pandas as pdfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipeline pandas进行数据预处理12345678data = pd.read_csv(path, header=None) iris_types = data[4].unique() for i, type in enumerate(iris_types): data.set_value(data[4] == type, 4, i) x, y = np.split(data.values, (4,), axis=1) x = x.astype(np.float) y = y.astype(np.int) # 仅使用前两列特征 导入模型，调用逻辑回归LogisticRegression()函数。训练LogisticRegression分类器123456789101112131415161718x = x[:, :2] lr = Pipeline([(&apos;sc&apos;, StandardScaler()), (&apos;clf&apos;, LogisticRegression()) ]) lr.fit(x, y.ravel()) y_hat = lr.predict(x) y_hat_prob = lr.predict_proba(x) np.set_printoptions(suppress=True) print (&apos;y_hat = \\n&apos;, y_hat) print (&apos;y_hat_prob = \\n&apos;, y_hat_prob) print (u&apos;准确度：%.2f%%&apos; % (100*np.mean(y_hat == y.ravel()))) # 画图 N, M = 500, 500 # 横纵各采样多少个值 x1_min, x1_max = x[:, 0].min(), x[:, 0].max() # 第0列的范围 x2_min, x2_max = x[:, 1].min(), x[:, 1].max() # 第1列的范围 t1 = np.linspace(x1_min, x1_max, N) t2 = np.linspace(x2_min, x2_max, M) x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点 x_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点 训练结果可视化12345678910111213141516mpl.rcParams[&apos;font.sans-serif&apos;] = [u&apos;simHei&apos;] mpl.rcParams[&apos;axes.unicode_minus&apos;] = False cm_light = mpl.colors.ListedColormap([&apos;#77E0A0&apos;, &apos;#FF8080&apos;, &apos;#A0A0FF&apos;]) cm_dark = mpl.colors.ListedColormap([&apos;g&apos;, &apos;r&apos;, &apos;b&apos;]) y_hat = lr.predict(x_test) # 预测值 y_hat = y_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.figure(facecolor=&apos;w&apos;) plt.pcolormesh(x1, x2, y_hat, cmap=cm_light) # 预测值的显示 plt.scatter(x[:, 0], x[:, 1], c=np.squeeze(y), edgecolors=&apos;k&apos;, s=50, cmap=cm_dark) # 样本的显示 plt.xlabel(u&apos;花萼长度&apos;, fontsize=14) plt.ylabel(u&apos;花萼宽度&apos;, fontsize=14) plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.grid() plt.title(u&apos;鸢尾花Logistic回归分类效果 - 标准化&apos;, fontsize=17) plt.show()","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://zem12345678.github.io/categories/Machine-learning/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/tags/算法/"},{"name":"回归分类","slug":"回归分类","permalink":"https://zem12345678.github.io/tags/回归分类/"},{"name":"Machine learning","slug":"Machine-learning","permalink":"https://zem12345678.github.io/tags/Machine-learning/"}]},{"title":"Django Restful Framework(DRF)的开发思考（3）","slug":"Django Restful Framework(DRF)的开发思考（3）","date":"2019-03-14T13:24:04.046Z","updated":"2019-03-14T13:24:46.627Z","comments":true,"path":"2019/03/14/Django Restful Framework(DRF)的开发思考（3）/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django Restful Framework(DRF)的开发思考（3）/","excerpt":"","text":"Django Restful Framework(DRF)的开发思考（3）认证&amp;权限,限流,过滤&amp;排序 , 分页 ，异常 1）认证&amp;权限2）限流控制用户访问API接口的频率。 针对匿名用户和认证用户分别进行限流。 1234567891011# 限流(针对匿名用户和认证用户分别进行限流控制)'DEFAULT_THROTTLE_CLASSES': ( 'rest_framework.throttling.AnonRateThrottle', # 针对匿名用户 'rest_framework.throttling.UserRateThrottle' # 针对认证用户),# 限流频次设置'DEFAULT_THROTTLE_RATES': &#123; 'user': '5/minute', # 认证用户5次每分钟 'anon': '3/minute', # 匿名用户3次每分钟&#125;, 针对匿名用户和认证用户统一进行限流。 123456789# 限流(针对匿名用户和认证用户进行统一限流控制)'DEFAULT_THROTTLE_CLASSES': ( 'rest_framework.throttling.ScopedRateThrottle',),'DEFAULT_THROTTLE_RATES': &#123; 'contacts': '5/minute', 'upload': '3/minute',&#125;, 3）过滤&amp;排序4）分页两种分页方式PageNumberPagination和LimitOffsetPagination。 使用PageNumberPagination分页时，获取分页数据时可以通过page传递页码参数。如果想要分页时指定页容量，需要自定义分页类。 1234567class StandardResultPagination(PageNumberPagination): # 默认页容量 page_size = 3 # 指定页容量参数名称 page_size_query_param = 'page_size' # 最大页容量 max_page_size = 5 使用LimitOffsetPagination分页时，获取分页数据时可以传递参数offset(偏移量)和limit(限制条数)。 注：如果使用的全局分页设置，某个列表视图如果不需要分页，直接在视图类中设置pagination_class = None。 5）异常DRF自带异常处理功能，可以对某些特定的异常进行处理并返回给客户端组织好的错误信息。能够处理的异常如下: 12345678910APIException 所有异常的父类ParseError 解析错误AuthenticationFailed 认证失败NotAuthenticated 尚未认证PermissionDenied 权限决绝NotFound 未找到MethodNotAllowed 请求方式不支持NotAcceptable 要获取的数据格式不支持Throttled 超过限流次数ValidationError 校验失败 可以自定义DRF框架的异常处理函数(补充一些异常处理)并指定EXCEPTION_HANDLER配置项。","categories":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/categories/前后端分离/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Restful","slug":"Restful","permalink":"https://zem12345678.github.io/tags/Restful/"},{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/tags/前后端分离/"}]},{"title":"Django REST framework JWT","slug":"Django REST framework JWT","date":"2019-03-14T13:05:55.099Z","updated":"2019-03-14T13:06:46.855Z","comments":true,"path":"2019/03/14/Django REST framework JWT/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django REST framework JWT/","excerpt":"","text":"Django REST framework JWT我们在验证完用户的身份后（检验用户名和密码），需要向用户签发JWT，在需要用到用户身份信息的时候，还需核验用户的JWT。 关于签发和核验JWT，我们可以使用Django REST framework JWT扩展来完成。 文档网站http://getblimp.github.io/django-rest-framework-jwt/ 安装配置安装 pip install djangorestframework-jwt 配置1234567891011REST_FRAMEWORK = &#123; &apos;DEFAULT_AUTHENTICATION_CLASSES&apos;: ( &apos;rest_framework_jwt.authentication.JSONWebTokenAuthentication&apos;, &apos;rest_framework.authentication.SessionAuthentication&apos;, &apos;rest_framework.authentication.BasicAuthentication&apos;, ),&#125;JWT_AUTH = &#123; &apos;JWT_EXPIRATION_DELTA&apos;: datetime.timedelta(days=1),&#125; JWT_EXPIRATION_DELTA 指明token的有效期 使用Django REST framework JWT 扩展的说明文档中提供了手动签发JWT的方法1234567from rest_framework_jwt.settings import api_settingsjwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLERjwt_encode_handler = api_settings.JWT_ENCODE_HANDLERpayload = jwt_payload_handler(user)token = jwt_encode_handler(payload) 在注册成功后，连同返回token，需要在注册视图中创建token。 修改CreateUserSerializer序列化器，在create方法中增加手动创建token的方法123456789101112131415161718192021222324252627282930313233343536from rest_framework_jwt.settings import api_settingsclass CreateUserSerializer(serializers.ModelSerializer): &quot;&quot;&quot; 创建用户序列化器 &quot;&quot;&quot; ... token = serializers.CharField(label=&apos;登录状态token&apos;, read_only=True) # 增加token字段 class Meta： ... fields = (&apos;id&apos;, &apos;username&apos;, &apos;password&apos;, &apos;password2&apos;, &apos;sms_code&apos;, &apos;mobile&apos;, &apos;allow&apos;, &apos;token&apos;) # 增加token ... def create(self, validated_data): &quot;&quot;&quot; 创建用户 &quot;&quot;&quot; # 移除数据库模型类中不存在的属性 del validated_data[&apos;password2&apos;] del validated_data[&apos;sms_code&apos;] del validated_data[&apos;allow&apos;] user = super().create(validated_data) # 调用django的认证系统加密密码 user.set_password(validated_data[&apos;password&apos;]) user.save() # 补充生成记录登录状态的token jwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLER jwt_encode_handler = api_settings.JWT_ENCODE_HANDLER payload = jwt_payload_handler(user) token = jwt_encode_handler(payload) user.token = token return user 前端保存token我们可以将JWT保存在cookie中，也可以保存在浏览器的本地存储里，我们保存在浏览器本地存储中 浏览器的本地存储提供了sessionStorage 和 localStorage 两种： sessionStorage 浏览器关闭即失效localStorage 长期有效使用方法1234567sessionStorage.变量名 = 变量值 // 保存数据sessionStorage.变量名 // 读取数据sessionStorage.clear() // 清除所有sessionStorage保存的数据localStorage.变量名 = 变量值 // 保存数据localStorage.变量名 // 读取数据localStorage.clear() // 清除所有localStorage保存的数据 在前端js/register.js文件中增加保存token12345678910111213141516171819var vm = new Vue(&#123; ... methods: &#123; ... on_submit: function()&#123; axios.post(...) .then(response =&gt; &#123; // 记录用户的登录状态 sessionStorage.clear(); localStorage.clear(); localStorage.token = response.data.token; localStorage.username = response.data.username; localStorage.user_id = response.data.id; location.href = &apos;/index.html&apos;; &#125;) .catch(...) &#125; &#125;&#125;)","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Token","slug":"Token","permalink":"https://zem12345678.github.io/tags/Token/"},{"name":"Jwt","slug":"Jwt","permalink":"https://zem12345678.github.io/tags/Jwt/"}]},{"title":"FastDFS客户端与自定义文件存储系统","slug":"FastDFS客户端与自定义文件存储系统","date":"2019-03-14T12:56:29.267Z","updated":"2019-03-14T12:57:39.281Z","comments":true,"path":"2019/03/14/FastDFS客户端与自定义文件存储系统/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/FastDFS客户端与自定义文件存储系统/","excerpt":"","text":"FastDFS客户端与自定义文件存储系统1. FastDFS的Python客户端python版本的FastDFS客户端使用说明参考https://github.com/jefforeilly/fdfs_client-py 安装安装fdfs_client-py-master.zip到虚拟环境中 pip install fdfs_client-py-master.zippip install mutagenpip install requests 使用使用FastDFS客户端，需要有配置文件。我们在meiduo_mall/utils目录下新建fastdfs目录，将提供给大家的client.conf配置文件放到这个目录中。需要修改一下client.conf配置文件12base_path=FastDFS客户端存放日志文件的目录tracker_server=运行tracker服务的机器ip:22122 上传文件需要先创建fdfs_client.client.Fdfs_client的对象，并指明配置文件，如12from fdfs_client.client import Fdfs_clientclient = Fdfs_client(&apos;meiduo_mall/utils/fastdfs/client.conf&apos;) 通过创建的客户端对象执行上传文件的方法123client.upload_by_filename(文件名)或client.upload_by_buffer(文件bytes数据) 如：1234567&gt;&gt;&gt; ret = client.upload_by_filename(&apos;/Users/delron/Desktop/1.png&apos;)getting connection&lt;fdfs_client.connection.Connection object at 0x1098d4cc0&gt;&lt;fdfs_client.fdfs_protol.Tracker_header object at 0x1098d4908&gt;&gt;&gt;&gt; ret&#123;&apos;Group name&apos;: &apos;group1&apos;, &apos;Remote file_id&apos;: &apos;group1/M00/00/02/CtM3BVr-k6SACjAIAAJctR1ennA809.png&apos;, &apos;Status&apos;: &apos;Upload successed.&apos;, &apos;Local file name&apos;: &apos;/Users/delron/Desktop/1.png&apos;, &apos;Uploaded size&apos;: &apos;151.00KB&apos;, &apos;Storage IP&apos;: &apos;10.211.55.5&apos;&#125;&gt;&gt;&gt; Remote file_id 即为FastDFS保存的文件的路径 2. 自定义Django文件存储系统在学习Django框架的时候，我们已经讲过，Django自带文件存储系统，但是默认文件存储在本地，在本项目中，我们需要将文件保存到FastDFS服务器上，所以需要自定义文件存储系统。 自定义文件存储系统的方法如下： 1）需要继承自django.core.files.storage.Storage，如1234from django.core.files.storage import Storageclass FastDFSStorage(Storage): ... 2）支持Django不带任何参数来实例化存储类，也就是说任何设置都应该从django.conf.settings中获取 1234567891011from django.conf import settingsfrom django.core.files.storage import Storageclass FastDFSStorage(Storage): def __init__(self, base_url=None, client_conf=None): if base_url is None: base_url = settings.FDFS_URL self.base_url = base_url if client_conf is None: client_conf = settings.FDFS_CLIENT_CONF self.client_conf = client_conf 3）存储类中必须实现_open()和_save()方法，以及任何后续使用中可能用到的其他方法。 _open(name, mode=’rb’) 被Storage.open()调用，在打开文件时被使用。 _save(name, content) 被Storage.save()调用，name是传入的文件名，content是Django接收到的文件内容，该方法需要将content文件内容保存。 Django会将该方法的返回值保存到数据库中对应的文件字段，也就是说该方法应该返回要保存在数据库中的文件名称信息。 exists(name) 如果名为name的文件在文件系统中存在，则返回True，否则返回False。 url(name) 返回文件的完整访问URL delete(name) 删除name的文件 listdir(path) 列出指定路径的内容 size(name) 返回name文件的总大小 注意，并不是这些方法全部都要实现，可以省略用不到的方法。 4）需要为存储类添加django.utils.deconstruct.deconstructible装饰器我们在meiduo_mall/utils/fastdfs目录中创建fdfs_storage.py文件，实现可以使用FastDFS存储文件的存储类如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from django.conf import settingsfrom django.core.files.storage import Storagefrom django.utils.deconstruct import deconstructiblefrom fdfs_client.client import Fdfs_client@deconstructibleclass FastDFSStorage(Storage): def __init__(self, base_url=None, client_conf=None): &quot;&quot;&quot; 初始化 :param base_url: 用于构造图片完整路径使用，图片服务器的域名 :param client_conf: FastDFS客户端配置文件的路径 &quot;&quot;&quot; if base_url is None: base_url = settings.FDFS_URL self.base_url = base_url if client_conf is None: client_conf = settings.FDFS_CLIENT_CONF self.client_conf = client_conf def _save(self, name, content): &quot;&quot;&quot; 在FastDFS中保存文件 :param name: 传入的文件名 :param content: 文件内容 :return: 保存到数据库中的FastDFS的文件名 &quot;&quot;&quot; client = Fdfs_client(self.client_conf) ret = client.upload_by_buffer(content.read()) if ret.get(&quot;Status&quot;) != &quot;Upload successed.&quot;: raise Exception(&quot;upload file failed&quot;) file_name = ret.get(&quot;Remote file_id&quot;) return file_name def url(self, name): &quot;&quot;&quot; 返回文件的完整URL路径 :param name: 数据库中保存的文件名 :return: 完整的URL &quot;&quot;&quot; return self.base_url + name def exists(self, name): &quot;&quot;&quot; 判断文件是否存在，FastDFS可以自行解决文件的重名问题 所以此处返回False，告诉Django上传的都是新文件 :param name: 文件名 :return: False &quot;&quot;&quot; return False 3. 在Django配置中设置自定义文件存储类在settings/dev.py文件中添加设置123456# django文件存储DEFAULT_FILE_STORAGE = &apos;meiduo_mall.utils.fastdfs.fdfs_storage.FastDFSStorage&apos;# FastDFSFDFS_URL = &apos;http://image.meiduo.site:8888/&apos; FDFS_CLIENT_CONF = os.path.join(BASE_DIR, &apos;utils/fastdfs/client.conf&apos;) 4. 添加image域名在/etc/hosts中添加访问FastDFS storage服务器的域名 127.0.0.1 image.meiduo.site","categories":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/categories/FastDFS/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/tags/FastDFS/"}]},{"title":"Django使用CKEditor富文本编辑器","slug":"Django使用CKEditor富文本编辑器","date":"2019-03-14T09:52:49.928Z","updated":"2019-03-14T09:56:27.558Z","comments":true,"path":"2019/03/14/Django使用CKEditor富文本编辑器/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django使用CKEditor富文本编辑器/","excerpt":"","text":"Django使用CKEditor富文本编辑器在运营后台，运营人员需要录入商品并编辑商品的详情信息，而商品的详情信息不是普通的文本，可以是包含了HTML语法格式的字符串。为了快速简单的让用户能够在页面中编辑带格式的文本，我们引入富文本编辑器。富文本即具备丰富样式格式的文本。 我们使用功能强大的CKEditor富文本编辑器。 1. 安装 pip install django-ckeditor 2. 添加应用在INSTALLED_APPS中添加123456INSTALLED_APPS = [ ... &apos;ckeditor&apos;, # 富文本编辑器 &apos;ckeditor_uploader&apos;, # 富文本编辑器上传图片模块 ...] 3. 添加CKEditor设置在settings/dev.py中添加123456789# 富文本编辑器ckeditor配置CKEDITOR_CONFIGS = &#123; &apos;default&apos;: &#123; &apos;toolbar&apos;: &apos;full&apos;, # 工具条功能 &apos;height&apos;: 300, # 编辑器高度 # &apos;width&apos;: 300, # 编辑器宽 &#125;,&#125;CKEDITOR_UPLOAD_PATH = &apos;&apos; # 上传图片保存路径，使用了FastDFS，所以此处设为&apos;&apos; 4. 添加ckeditor路由在总路由中添加 url(r’^ckeditor/‘, include(‘ckeditor_uploader.urls’)), 5. 为模型类添加字段ckeditor提供了两种类型的Django模型类字段 ckeditor.fields.RichTextField 不支持上传文件的富文本字段ckeditor_uploader.fields.RichTextUploadingField 支持上传文件的富文本字段在商品模型类（SPU）中，要保存商品的详细介绍、包装信息、售后服务，这三个字段需要作为富文本字段 1234567891011from ckeditor.fields import RichTextFieldfrom ckeditor_uploader.fields import RichTextUploadingFieldclass Goods(BaseModel): &quot;&quot;&quot; 商品SPU &quot;&quot;&quot; ... desc_detail = RichTextUploadingField(default=&apos;&apos;, verbose_name=&apos;详细介绍&apos;) desc_pack = RichTextField(default=&apos;&apos;, verbose_name=&apos;包装信息&apos;) desc_service = RichTextUploadingField(default=&apos;&apos;, verbose_name=&apos;售后服务&apos;) 6. 修改Bug我们将通过Django上传的图片保存到了FastDFS中，而保存在FastDFS中的文件名没有后缀名，ckeditor在处理上传后的文件名按照有后缀名来处理，所以会出现bug错误， 修正方法找到虚拟环境目录中的ckeditor_uploader/views.py文件，如 ~/.virtualenvs/meiduo/lib/python3.5/site-packages/ckeditor_uploader/views.py 将第95行代码修改如下","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"},{"name":"富文本","slug":"富文本","permalink":"https://zem12345678.github.io/tags/富文本/"}]},{"title":"使用Docker安装FastDFS","slug":"使用Docker安装FastDFS","date":"2019-03-14T09:45:54.351Z","updated":"2019-03-14T09:54:26.402Z","comments":true,"path":"2019/03/14/使用Docker安装FastDFS/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/使用Docker安装FastDFS/","excerpt":"","text":"使用Docker安装FastDFS1. 获取镜像可以利用已有的FastDFS Docker镜像来运行FastDFS。 获取镜像可以通过下载 docker image pull delron/fastdfs 加载好镜像后，就可以开启运行FastDFS的tracker和storage了。 2. 运行tracker执行如下命令开启tracker 服务 docker run -dti –network=host –name tracker -v /var/fdfs/tracker:/var/fdfs delron/fastdfs tracker 我们将fastDFS tracker运行目录映射到本机的 /var/fdfs/tracker目录中。执行如下命令查看tracker是否运行起来 docker container ls 如果想停止tracker服务，可以执行如下命令 docker container stop tracker 停止后，重新运行tracker，可以执行如下命令 docker container start tracker 3. 运行storage执行如下命令开启storage服务 docker run -dti –network=host –name storage -e TRACKER_SERVER=10.211.55.5:22122 -v /var/fdfs/storage:/var/fdfs delron/fastdfs storage TRACKER_SERVER=本机的ip地址:22122 本机ip地址不要使用127.0.0.1我们将fastDFS storage运行目录映射到本机的/var/fdfs/storage目录中 执行如下命令查看storage是否运行起来 docker container ls 如果想停止storage服务，可以执行如下命令 docker container stop storage 停止后，重新运行storage，可以执行如下命令 docker container start storage 注意：如果无法重新运行，可以删除/var/fdfs/storage/data目录下的fdfs_storaged.pid 文件，然后重新运行storage。","categories":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/categories/FastDFS/"}],"tags":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/tags/FastDFS/"},{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/tags/Docker/"}]},{"title":"Django项目部署","slug":"Django项目部署","date":"2019-03-14T09:37:31.966Z","updated":"2019-03-14T09:39:11.170Z","comments":true,"path":"2019/03/14/Django项目部署/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django项目部署/","excerpt":"","text":"Django项目部署 1.静态文件开发阶段: live-server 部署阶段: nginx当Django运行在生产模式时，将不再提供静态文件的支持，需要将静态文件交给静态文件服务器。 我们先收集所有静态文件。项目中的静态文件除了我们使用的前端项目front_end_pc中之外，django本身还有自己的静态文件，如果rest_framework、xadmin、admin、ckeditor等。我们需要收集这些静态文件，集中一起放到静态文件服务器中。 我们要将收集的静态文件放到front_end_pc目录下的static目录中，所以先创建目录static。Django提供了收集静态文件的方法。先在配置文件中配置收集之后存放的目录 STATIC_ROOT = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), ‘front_end_pc/static’) 然后执行收集命令 python manage.py collectstatic 我们使用Nginx服务器作为静态文件服务器打开Nginx的配置文件 sudo vim /usr/local/nginx/conf/nginx.conf 在server部分中配置1234567891011server &#123; listen 80; server_name www.meiduo.site; location / &#123; root /home/python/Desktop/front_end_pc; index index.html index.htm; &#125; # 余下省略&#125; 重启Nginx服务器 sudo /usr/local/nginx/sbin/nginx -s reload 首次启动nginx服务器 sudo /usr/local/nginx/sbin/nginx 停止nginx服务器 sudo /usr/local/nginx/sbin/nginx -s stop 2. 动态接口开发阶段：Django提供的开发web服务器 python manage.py runserver 部署阶段：uwsgi(遵循wsgi协议的web服务器)。 域名: api.meiduo.site 使用uwsgi: 安装 pip install uwsgi 配置 使用 启动 uwsgi –ini uwsgi.ini 停止 uwsgi –stop uwsgi.pid 在项目中复制开发配置文件dev.py 到生产配置prod.py 修改配置文件prod.py中 DEBUG = True ALLOWED_HOSTS = […, ‘www.meiduo.site&#39;] # 添加www.meiduo.site CORS_ORIGIN_WHITELIST = ( ‘127.0.0.1:8080’, ‘localhost:8080’, ‘www.meiduo.site:8080&#39;, ‘www.meiduo.site&#39;, # 添加) 修改wsgi.py文件 os.environ.setdefault(“DJANGO_SETTINGS_MODULE”, “meiduo_mall.settings.prod”) django的程序通常使用uwsgi服务器来运行安装uwsgi pip install uwsgi 在项目目录/meiduo_mall 下创建uwsgi配置文件 uwsgi.ini123456789101112131415161718192021[uwsgi]#使用nginx连接时使用，Django程序所在服务器地址socket=127.0.0.1:8001#直接做web服务器使用，Django程序所在服务器地址#http=127.0.0.1:8001#项目目录chdir=/Users/delron/Desktop/meiduo/meiduo_mall#项目中wsgi.py文件的目录，相对于项目目录wsgi-file=meiduo_mall/wsgi.py# 进程数processes=4# 线程数threads=2# uwsgi服务器的角色master=True# 存放进程编号的文件pidfile=uwsgi.pid# 日志文件，因为uwsgi可以脱离终端在后台运行，日志看不见。我们以前的runserver是依赖终端的daemonize=uwsgi.log# 指定依赖的虚拟环境virtualenv=/Users/smart/.virtualenvs/meiduo 启动uwsgi服务器 uwsgi –ini uwsgi.ini 注意如果想要停止服务器，除了可以使用kill命令之外，还可以通过 uwsgi –stop uwsgi.pid 修改Nginx配置文件，让Nginx接收到请求后转发给uwsgi服务器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647upstream meiduo &#123; server 10.211.55.2:8001; # 此处为uwsgi运行的ip地址和端口号 # 如果有多台服务器，可以在此处继续添加服务器地址 &#125; #gzip on; server &#123; listen 8000; server_name api.meiduo.site; location / &#123; include uwsgi_params; uwsgi_pass meiduo; &#125; &#125; server &#123; listen 80; server_name www.meiduo.site; #charset koi8-r; #access_log logs/host.access.log main; location /xadmin &#123; include uwsgi_params; uwsgi_pass meiduo; &#125; location /ckeditor &#123; include uwsgi_params; uwsgi_pass meiduo; &#125; location / &#123; root /home/python/Desktop/front_end_pc; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; 重启nginx sudo /usr/local/nginx/sbin/nginx -s reload","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"Nginx","slug":"Nginx","permalink":"https://zem12345678.github.io/tags/Nginx/"}]},{"title":"Docker的简单操作","slug":"Docker的简单操作","date":"2019-03-14T08:41:39.252Z","updated":"2019-03-14T08:43:04.952Z","comments":true,"path":"2019/03/14/Docker的简单操作/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Docker的简单操作/","excerpt":"","text":"Docker的简单操作1. 在Ubuntu中安装Docker更新ubuntu的apt源索引 sudo apt-get update 安装包允许apt通过HTTPS使用仓库 sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common 添加Docker官方GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 设置Docker稳定版仓库 sudo add-apt-repository \\ “deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable” 添加仓库后，更新apt源索引 sudo apt-get update 安装最新版Docker CE（社区版） sudo apt-get install docker-ce 检查Docker CE是否安装正确 sudo docker run hello-world 出现如下信息，表示安装成功为了避免每次命令都输入sudo，可以设置用户权限，注意执行后须注销重新登录 sudo usermod -a -G docker $USER 2. 启动与停止安装完成Docker后，默认已经启动了docker服务，如需手动控制docker服务的启停，可执行如下命令 启动docker sudo service docker start 停止docker sudo service docker stop 重启docker sudo service docker restart 3. Docker镜像操作什么是Docker镜像Docker 镜像是由文件系统叠加而成(是一种文件的存储形式)。最底端是一个文件引 导系统，即 bootfs，这很像典型的 Linux/Unix 的引导文件系统。Docker 用户几乎永远不会和 引导系统有什么交互。实际上，当一个容器启动后，它将会被移动到内存中，而引导文件系 统则会被卸载，以留出更多的内存供磁盘镜像使用。Docker 容器启动是需要一些文件的， 而这些文件就可以称为 Docker 镜像。Docker 把应用程序及其依赖，打包在 image 文件里面。只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 image 是二进制文件。实际开发中，一个 image 文件往往通过继承另一个 image 文件，加上一些个性化设置而生成。举例来说，你可以在 Ubuntu 的 image 基础上，往里面加入 Apache 服务器，形成你的 image。 image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。 为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 Docker Hub 是最重要、最常用的 image 仓库。此外，出售自己制作的 image 文件也是可以的。 列出镜像 docker image ls REPOSITORY：镜像所在的仓库名称TAG：镜像标签IMAGEID：镜像IDCREATED：镜像的创建日期(不是获取该镜像的日期)SIZE：镜像大小 为了区分同一个仓库下的不同镜像，Docker 提供了一种称为标签(Tag)的功能。每个 镜像在列出来时都带有一个标签，例如latest、 12.10、12.04 等等。每个标签对组成特定镜像的一 些镜像层进行标记(比如，标签 12.04 就是对所有 Ubuntu12.04 镜像层的标记)。这种机制 使得同一个仓库中可以存储多个镜像。— 版本号 我们在运行同一个仓库中的不同镜像时，可以通过在仓库名后面加上一个冒号和标签名 来指定该仓库中的某一具体的镜像，例如 docker run –name custom_container_name –i –t docker.io/ubunto:12.04 /bin/bash，表明从镜像 Ubuntu:12.04 启动一个容器，而这个镜像的操 作系统就是 Ubuntu:12.04。在构建容器时指定仓库的标签也是一个好习惯。 拉取镜像Docker维护了镜像仓库，分为共有和私有两种，共有的官方仓库Docker Hub(https://hub.docker.com/)是最重要最常用的镜像仓库。私有仓库（Private Registry）是开发者或者企业自建的镜像存储库，通常用来保存企业 内部的 Docker 镜像，用于内部开发流程和产品的发布、版本控制。 要想获取某个镜像，我们可以使用pull命令，从仓库中拉取镜像到本地，如 docker image pull library/hello-world 上面代码中，docker image pull是抓取 image 文件的命令。library/hello-world是 image 文件在仓库里面的位置，其中library是 image 文件所在的组，hello-world是 image 文件的名字。 由于 Docker 官方提供的 image 文件，都放在library组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。 docker image pull hello-world 删除镜像docker image rm 镜像名或镜像id如 docker image rm hello-world 4. Docker 容器操作创建容器docker run [option] 镜像名 [向启动容器中传入的命令]常用可选参数说明： -i 表示以“交互模式”运行容器-t 表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即 分配一个伪终端。–name 为创建的容器命名-v 表示目录映射关系(前者是宿主机目录，后者是映射到宿主机上的目录，即 宿主机目录:容器中目录)，可以使 用多个-v 做多个目录或文件映射。注意:最好做目录映射，在宿主机上做修改，然后 共享到容器上。-d 在run后面加上-d参数,则会创建一个守护式容器在后台运行(这样创建容器后不 会自动登录容器，如果只加-i -t 两个参数，创建后就会自动进去容器)。-p 表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p 做多个端口映射-e 为容器设置环境变量–network=host 表示将主机的网络环境映射到容器中，容器的网络与主机相同 交互式容器例如，创建一个交互式容器，并命名为myubuntu docker run -it –name=myubuntu ubuntu /bin/bash 在容器中可以随意执行linux命令，就是一个ubuntu的环境，当执行exit命令退出时，该容器也随之停止。 守护式容器创建一个守护式容器:如果对于一个需要长期运行的容器来说，我们可以创建一个守护式容器。在容器内部exit退出时，容器也不会停止。 docker run -dit –name=myubuntu2 ubuntu 进入已运行的容器 docker exec -it 容器名或容器id 进入后执行的第一个命令 如 docker exec -it myubuntu2 /bin/bash 查看容器列出本机正在运行的容器 docker container ls 列出本机所有容器，包括已经终止运行的 docker container ls –all 停止与启动容器停止一个已经在运行的容器 docker container stop 容器名或容器id 启动一个已经停止的容器 docker container start 容器名或容器id kill掉一个已经在运行的容器 docker container kill 容器名或容器id 删除容器 docker container rm 容器名或容器id 5. 将容器保存为镜像我们可以通过如下命令将容器保存为镜像 docker commit 容器名 镜像名 6. 镜像备份与迁移我们可以通过save命令将镜像打包成文件，拷贝给别人使用 docker save -o 保存的文件名 镜像名 如 docker save -o ./ubuntu.tar ubuntu 在拿到镜像文件后，可以通过load方法，将镜像加载到本地 docker load -i ./ubuntu.tar","categories":[{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/categories/Docker/"}],"tags":[{"name":"容器","slug":"容器","permalink":"https://zem12345678.github.io/tags/容器/"},{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/tags/Docker/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://zem12345678.github.io/tags/虚拟化/"}]},{"title":"Django 导出Excel文件","slug":"Django 导出Excel文件","date":"2019-03-14T08:18:02.834Z","updated":"2019-03-14T08:22:28.344Z","comments":true,"path":"2019/03/14/Django 导出Excel文件/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django 导出Excel文件/","excerpt":"","text":"Django 导出Excel文件 12设置HTTPResponse的类型reposnse = HttpResponse(content_type=&apos;application/vnd.ms-excel&apos;) 代码123456789101112131415161718192021222324252627282930313233343536373839404142def get(self,request): filename = &apos;导入模板&apos; + &apos;.xls&apos; # 设置HTTPResponse的类型 reposnse = HttpResponse(content_type=&apos;application/vnd.ms-excel&apos;) # 创建一个文件对象 reposnse[&apos;Content-Disposition&apos;] = &apos;attachment;filename=&apos;+filename # 创建一个sheet对象 wb = xlwt.Workbook(encoding=&apos;utf-8&apos;) sheet = wb.add_sheet(&apos;order-sheet&apos;) # 设置文件头的样式 style_heading = xlwt.easyxf(&quot;&quot;&quot; font: name Arial, colour_index white, bold on, height 0xA0; align: wrap off, vert center, horiz center; pattern: pattern solid, fore-colour 0x19; borders: left THIN, right THIN, top THIN, bottom THIN; &quot;&quot;&quot;) # 写入文件标题 sheet.write(0, 0, &apos;姓名&apos;, style_heading) sheet.write(0, 1, &apos;手机号&apos;, style_heading) sheet.write(0, 2, &apos;身份证号&apos;, style_heading) sheet.write(0, 3, &apos;区域号&apos;, style_heading) # 写出到IO output = BytesIO() wb.save(output) output.seek(0) reposnse.write(output.getvalue()) logger.info(&apos;用户:%s 导出了导入模板&apos; % (request.user.username)) return reposnse","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"},{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"}]},{"title":"关于Docker","slug":"关于Docker","date":"2019-03-14T08:06:22.488Z","updated":"2019-03-14T08:08:08.051Z","comments":true,"path":"2019/03/14/关于Docker/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/关于Docker/","excerpt":"","text":"关于Docker1.什么是Docker 容器技术在计算机的世界中，容器拥有一段漫长且传奇的历史。容器与管理程序虚拟化 (hypervisor virtualization，HV)有所不同，管理程序虚拟化通过中间层将一台或者多台独立 的机器虚拟运行与物理硬件之上，而容器则是直接运行在操作系统内核之上的用户空间。因 此，容器虚拟化也被称为“操作系统级虚拟化”，容器技术可以让多个独立的用户空间运行 在同一台宿主机上。 由于“客居”于操作系统，容器只能运行与底层宿主机相同或者相似的操作系统，这看 起来并不是非常灵活。例如:可以在 Ubuntu 服务中运行 Redhat Enterprise Linux，但无法再 Ubuntu 服务器上运行 Microsoft Windows。 相对于彻底隔离的管理程序虚拟化，容器被认为是不安全的。而反对这一观点的人则认 为，由于虚拟容器所虚拟的是一个完整的操作系统，这无疑增大了攻击范围，而且还要考虑 管理程序层潜在的暴露风险。 尽管有诸多局限性，容器还是被广泛部署于各种各样的应用场合。在超大规模的多租户 服务部署、轻量级沙盒以及对安全要求不太高的隔离环境中，容器技术非常流行。最常见的 一个例子就是“权限隔离监牢”(chroot jail)，它创建一个隔离的目录环境来运行进程。 如果权限隔离监牢正在运行的进程被入侵者攻破，入侵者便会发现自己“身陷囹圄”，因为 权限不足被困在容器所创建的目录中，无法对宿主机进一步破坏。 最新的容器技术引入了 OpenVZ、Solaris Zones 以及 Linux 容器(LXC)。使用这些新技 术，容器不在仅仅是一个单纯的运行环境。在自己的权限类内，容器更像是一个完整的宿主 机。对 Docker 来说，它得益于现代 Linux 特性，如控件组(control group)、命名空间 (namespace)技术，容器和宿主机之间的隔离更加彻底，容器有独立的网络和存储栈，还 拥有自己的资源管理能力，使得同一台宿主机中的多个容器可以友好的共存。 容器被认为是精益技术，因为容器需要的开销有限。和传统虚拟化以及半虚拟化相比， 容器不需要模拟层(emulation layer)和管理层(hypervisor layer)，而是使用操作系统的系 统调用接口。这降低了运行单个容器所需的开销，也使得宿主机中可以运行更多的容器。 尽管有着光辉的历史，容器仍未得到广泛的认可。一个很重要的原因就是容器技术的复 杂性:容器本身就比较复杂，不易安装，管理和自动化也很困难。而 Docker 就是为了改变 这一切而生的。 Docker特点1）上手快用户只需要几分钟，就可以把自己的程序“Docker 化”。Docker 依赖于“写时复制” (copy-on-write)模型，使修改应用程序也非常迅速，可以说达到“随心所致，代码即改” 的境界。 随后，就可以创建容器来运行应用程序了。大多数 Docker 容器只需要不到 1 秒中即可 启动。由于去除了管理程序的开销，Docker 容器拥有很高的性能，同时同一台宿主机中也 可以运行更多的容器，使用户尽可能的充分利用系统资源。 2）职责的逻辑分类使用 Docker，开发人员只需要关心容器中运行的应用程序，而运维人员只需要关心如 何管理容器。Docker 设计的目的就是要加强开发人员写代码的开发环境与应用程序要部署 的生产环境一致性。从而降低那种“开发时一切正常，肯定是运维的问题(测试环境都是正 常的，上线后出了问题就归结为肯定是运维的问题)” 3）快速高效的开发生命周期Docker 的目标之一就是缩短代码从开发、测试到部署、上线运行的周期，让你的应用 程序具备可移植性，易于构建，并易于协作。(通俗一点说，Docker 就像一个盒子，里面 可以装很多物件，如果需要这些物件的可以直接将该大盒子拿走，而不需要从该盒子中一件 件的取。) 4）鼓励使用面向服务的架构Docker 还鼓励面向服务的体系结构和微服务架构。Docker 推荐单个容器只运行一个应 用程序或进程，这样就形成了一个分布式的应用程序模型，在这种模型下，应用程序或者服 务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序 都变得非常简单，同时也提高了程序的内省性。(当然，可以在一个容器中运行多个应用程 序) 2. Docker组件1）Docker 客户端和服务器Docker 是一个客户端-服务器(C/S)架构程序。Docker 客户端只需要向 Docker 服务器 或者守护进程发出请求，服务器或者守护进程将完成所有工作并返回结果。Docker 提供了 一个命令行工具 Docker 以及一整套 RESTful API。你可以在同一台宿主机上运行 Docker 守护 进程和客户端，也可以从本地的 Docker 客户端连接到运行在另一台宿主机上的远程 Docker 守护进程。 2）Docker镜像镜像是构建 Docker 的基石。用户基于镜像来运行自己的容器。镜像也是 Docker 生命周 期中的“构建”部分。镜像是基于联合文件系统的一种层式结构，由一系列指令一步一步构 建出来。例如: 添加一个文件; 执行一个命令; 打开一个窗口。 也可以将镜像当作容器的“源代码”。镜像体积很小，非常“便携”，易于分享、存储和更 新。 3）Registry（注册中心）Docker 用 Registry 来保存用户构建的镜像。Registry 分为公共和私有两种。Docker 公司 运营公共的 Registry 叫做 Docker Hub。用户可以在 Docker Hub 注册账号，分享并保存自己的 镜像(说明:在 Docker Hub 下载镜像巨慢，可以自己构建私有的 Registry)。 4）Docker容器Docker 可以帮助你构建和部署容器，你只需要把自己的应用程序或者服务打包放进容 器即可。容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，镜 像是Docker生命周期中的构建或者打包阶段，而容器则是启动或者执行阶段。 容器基于 镜像启动，一旦容器启动完成后，我们就可以登录到容器中安装自己需要的软件或者服务。所以 Docker 容器就是: 一个镜像格式; 一些列标准操作; 一个执行环境。 Docker 借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Docker 将这个模 型运用到自己的设计中，唯一不同的是:集装箱运输货物，而 Docker 运输软件。 和集装箱一样，Docker 在执行上述操作时，并不关心容器中到底装了什么，它不管是 web 服务器，还是数据库，或者是应用程序服务器什么的。所有的容器都按照相同的方式将 内容“装载”进去。 Docker 也不关心你要把容器运到何方:我们可以在自己的笔记本中构建容器，上传到 Registry，然后下载到一个物理的或者虚拟的服务器来测试，在把容器部署到具体的主机中。 像标准集装箱一样，Docker 容器方便替换，可以叠加，易于分发，并且尽量通用。 使用 Docker，我们可以快速的构建一个应用程序服务器、一个消息总线、一套实用工 具、一个持续集成(CI)测试环境或者任意一种应用程序、服务或工具。我们可以在本地构 建一个完整的测试环境，也可以为生产或开发快速复制一套复杂的应用程序栈。 3. 使用Docker做什么容器提供了隔离性，结论是，容器可以为各种测试提供很好的沙盒环境。并且，容器本 身就具有“标准性”的特征，非常适合为服务创建构建块。Docker 的一些应用场景如下: 加速本地开发和构建流程，使其更加高效、更加轻量化。本地开发人员可以构建、 运行并分享 Docker 容器。容器可以在开发环境中构建，然后轻松的提交到测试环境中，并 最终进入生产环境。能够让独立的服务或应用程序在不同的环境中，得到相同的运行结果。这一点在 面向服务的架构和重度依赖微型服务的部署由其实用。用 Docker 创建隔离的环境来进行测试。例如，用 Jenkins CI 这样的持续集成工具 启动一个用于测试的容器。Docker 可以让开发者先在本机上构建一个复杂的程序或架构来进行测试，而不是 一开始就在生产环境部署、测试。","categories":[{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/categories/Docker/"}],"tags":[{"name":"容器","slug":"容器","permalink":"https://zem12345678.github.io/tags/容器/"},{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/tags/Docker/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://zem12345678.github.io/tags/虚拟化/"}]},{"title":"FastDFS分布式文件系统","slug":"FastDFS分布式文件系统","date":"2019-03-14T07:43:06.160Z","updated":"2019-03-14T08:11:36.272Z","comments":true,"path":"2019/03/14/FastDFS分布式文件系统/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/FastDFS分布式文件系统/","excerpt":"","text":"FastDFS分布式文件系统1. 什么是FastDFSFastDFS 是用 c 语言编写的一款开源的分布式文件系统。FastDFS 为互联网量身定制， 充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用 FastDFS 很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。 FastDFS 架构包括 Tracker server 和 Storage server。客户端请求 Tracker server 进行文 件上传、下载，通过 Tracker server 调度最终由 Storage server 完成文件上传和下载。 Tracker server 作用是负载均衡和调度，通过 Tracker server 在文件上传时可以根据一些 策略找到 Storage server 提供文件上传服务。可以将 tracker 称为追踪服务器或调度服务器。 Storage server 作用是文件存储，客户端上传的文件最终存储在 Storage 服务器上， Storageserver 没有实现自己的文件系统而是利用操作系统 的文件系统来管理文件。可以将 storage 称为存储服务器。服务端两个角色: Tracker: 管理集群，tracker 也可以实现集群。每个 tracker 节点地位平等。收集 Storage 集群的状态。Storage: 实际保存文件， Storage 分为多个组，每个组之间保存的文件是不同的。每 个组内部可以有多个成员，组成员内部保存的内容是一样的，组成员的地位是一致的，没有 主从的概念。 2. 文件上传流程客户端上传文件后存储服务器将文件 ID 返回给客户端，此文件 ID 用于以后访问该文 件的索引信息。文件索引信息包括:组名，虚拟磁盘路径，数据两级目录，文件名。 组名：文件上传后所在的 storage 组名称，在文件上传成功后有 storage 服务器返回， 需要客户端自行保存。虚拟磁盘路径：storage 配置的虚拟路径，与磁盘选项 store_path*对应。如果配置了 store_path0 则是 M00，如果配置了 store_path1 则是 M01，以此类推。数据两级目录：storage 服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据 文件。文件名：与文件上传时不同。是由存储服务器根据特定信息生成，文件名包含:源存储 服务器 IP 地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息。 3. 简易FastDFS构建","categories":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/categories/FastDFS/"}],"tags":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/tags/FastDFS/"},{"name":"分布式","slug":"分布式","permalink":"https://zem12345678.github.io/tags/分布式/"}]},{"title":"关于JWT","slug":"关于JWT","date":"2019-03-14T06:18:00.631Z","updated":"2019-03-14T06:25:48.447Z","comments":true,"path":"2019/03/14/关于JWT/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/关于JWT/","excerpt":"","text":"关于JWT在用户注册或登录后，我们想记录用户的登录状态，或者为用户创建身份认证的凭证。我们不再使用Session认证机制，而使用Json Web Token认证机制。 [TOC] 什么是JWT Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。 起源 说起JWT，我们应该来谈一谈基于token的认证和传统的session认证的区别。 ##传统的session认证我们知道，http协议本身是一种无状态的协议，而这就意味着如果用户向我们的应用提供了用户名和密码来进行用户认证，那么下一次请求时，用户还要再一次进行用户认证才行，因为根据http协议，我们并不能知道是哪个用户发出的请求，所以为了让我们的应用能识别是哪个用户发出的请求，我们只能在服务器存储一份用户登录的信息，这份登录信息会在响应时传递给浏览器，告诉其保存为cookie,以便下次请求时发送给我们的应用，这样我们的应用就能识别请求来自哪个用户了,这就是传统的基于session认证。但是这种基于session的认证使应用本身很难得到扩展，随着不同客户端用户的增加，独立的服务器已无法承载更多的用户，而这时候基于session认证应用的问题就会暴露出来. ###基于session认证所显露的问题Session: 每个用户经过我们的应用认证之后，我们的应用都要在服务端做一次记录，以方便用户下次请求的鉴别，通常而言session都是保存在内存中，而随着认证用户的增多，服务端的开销会明显增大。 扩展性: 用户认证之后，服务端做认证记录，如果认证的记录被保存在内存中的话，这意味着用户下次请求还必须要请求在这台服务器上,这样才能拿到授权的资源，这样在分布式的应用上，相应的限制了负载均衡器的能力。这也意味着限制了应用的扩展能力。 CSRF: 因为是基于cookie来进行用户识别的, cookie如果被截获，用户就会很容易受到跨站请求伪造的攻击。 基于token的鉴权机制基于token的鉴权机制类似于http协议也是无状态的，它不需要在服务端去保留用户的认证信息或者会话信息。这就意味着基于token认证机制的应用不需要去考虑用户在哪一台服务器登录了，这就为应用的扩展提供了便利。 流程上是这样的： 用户使用用户名密码来请求服务器服务器进行验证用户的信息服务器通过验证发送给用户一个token客户端存储token，并在每次请求时附送上这个token值服务端验证token值，并返回数据这个token必须要在每次请求时传递给服务端，它应该保存在请求头里， 另外，服务端要支持CORS(跨来源资源共享)策略，一般我们在服务端这么做就可以了Access-Control-Allow-Origin: *。 那么我们现在回到JWT的主题上。 JWT长什么样？JWT是由三段信息构成的，将这三段信息文本用.链接一起就构成了Jwt字符串。就像这样: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ JWT的构成第一部分我们称它为头部（header),第二部分我们称其为载荷（payload, 类似于飞机上承载的物品)，第三部分是签证（signature). headerwt的头部承载两部分信息： 声明类型，这里是jwt声明加密的算法 通常直接使用 HMAC SHA256完整的头部就像下面这样的JSON： { ‘typ’: ‘JWT’, ‘alg’: ‘HS256’} 然后将头部进行base64加密（该加密是可以对称解密的),构成了第一部分. eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 payload载荷就是存放有效信息的地方。这个名字像是特指飞机上承载的货品，这些有效信息包含三个部分 标准中注册的声明公共的声明私有的声明标准中注册的声明 (建议但不强制使用) ： iss: jwt签发者sub: jwt所面向的用户aud: 接收jwt的一方exp: jwt的过期时间，这个过期时间必须要大于签发时间nbf: 定义在什么时间之前，该jwt都是不可用的.iat: jwt的签发时间jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。公共的声明 ： 公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密.私有的声明 ： 私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。 定义一个payload: { “sub”: “1234567890”, “name”: “John Doe”, “admin”: true} 然后将其进行base64加密，得到JWT的第二部分。 eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9 signatureJWT的第三部分是一个签证信息，这个签证信息由三部分组成： header (base64后的)payload (base64后的)secret 这个部分需要base64加密后的header和base64加密后的payload使用.连接组成的字符串，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第三部分。 // javascriptvar encodedString = base64UrlEncode(header) + ‘.’ + base64UrlEncode(payload); var signature = HMACSHA256(encodedString, ‘secret’); // TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 将这三部分用.连接成一个完整的字符串,构成了最终的jwt: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 注意：secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用来进行jwt的签发和jwt的验证，所以，它就是你服务端的私钥，在任何场景都不应该流露出去。一旦客户端得知这个secret, 那就意味着客户端是可以自我签发jwt了。 如何应用一般是在请求头里加入Authorization，并加上Bearer标注： fetch(‘api/user/1’, { headers: { ‘Authorization’: ‘Bearer ‘ + token }})服务端会验证token，如果验证通过就会返回相应的资源。整个流程就是这样的:总结优点因为json的通用性，所以JWT是可以进行跨语言支持的，像JAVA,JavaScript,NodeJS,PHP等很多语言都可以使用。因为有了payload部分，所以JWT可以在自身存储一些其他业务逻辑所必要的非敏感信息。便于传输，jwt的构成非常简单，字节占用很小，所以它是非常便于传输的。它不需要在服务端保存会话信息, 所以它易于应用的扩展 安全相关 不应该在jwt的payload部分存放敏感信息，因为该部分是客户端可解密的部分。保护好secret私钥，该私钥非常重要。如果可以，请使用https协议","categories":[{"name":"JWT","slug":"JWT","permalink":"https://zem12345678.github.io/categories/JWT/"}],"tags":[{"name":"Token","slug":"Token","permalink":"https://zem12345678.github.io/tags/Token/"},{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"},{"name":"权限认证","slug":"权限认证","permalink":"https://zem12345678.github.io/tags/权限认证/"}]},{"title":"flask项目使用 Gunicorn + Nginx 进行布署","slug":"flask项目使用 Gunicorn + Nginx 进行布署","date":"2019-03-14T05:50:18.325Z","updated":"2019-03-14T06:26:08.221Z","comments":true,"path":"2019/03/14/flask项目使用 Gunicorn + Nginx 进行布署/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/flask项目使用 Gunicorn + Nginx 进行布署/","excerpt":"","text":"flask项目使用 Gunicorn + Nginx 进行布署基于ubuntu 16.04系统，使用 Gunicorn + Nginx 进行布署，云服务器为阿里云 [TOC] Markdown简介 选择云服务器:阿里云服务器。(https://zh.wikipedia.org/wiki/Markdown)个人免费获取 [https://free.aliyun.com/] 创建服务器选择ubuntu16.04 64位的操作系统 进入控制台,查看实例创建情况给安全组配置规则，添加5000端口(一并加上5001端口)利用命令行进行远程服务器登录 ssh 用户名@ip地址 ##相关环境安装以下操作都在远程服务器上进行操作 (ubuntu 16.04)先更新 apt 相关源 sudo apt-get update mysql安装 apt-get install mysql-serverapt-get install libmysqlclient-dev redis安装 sudo apt-get install redis-server 安装虚拟环境 pip install virtualenvpip install virtualenvwrapper 使得安装的virtualenvwrapper生效，编辑~/.bashrc文件，内容如下: export WORKON_HOME=$HOME/.virtualenvsexport PROJECT_HOME=$HOME/workspacesource /usr/local/bin/virtualenvwrapper.sh 使编辑后的文件生效 source ~/.bashrc12345678910@requires_authorizationdef somefunc(param1='', param2=0): '''A docstring''' if param1 &gt; param2: # interesting print 'Greater' return (param2 - param1 + 1) or Noneclass SomeClass: pass&gt;&gt;&gt; message = '''interpreter... prompt''' requirements文件Python 项目中可以包含一个 requirements.txt 文件，用于记录所有依赖包及其精确的版本号，以便在新环境中进行部署操作。 在虚拟环境使用以下命令将当前虚拟环境中的依赖包以版本号生成至文件中： pip freeze &gt; requirements.txt 当需要创建这个虚拟环境的完全副本，可以创建一个新的虚拟环境，并在其上运行以下命令： pip install -r requirements.txt 在安装 Flask-MySQLdb 的时候可能会报错，可能是依赖包没有安装，执行以下命令安装依赖包： sudo apt-get build-dep python-mysqldb Nginx采用 C 语言编写实现分流、转发、负载均衡相关操作安装 $ sudo apt-get install nginx运行及停止 /etc/init.d/nginx start #启动/etc/init.d/nginx stop #停止 配置文件 编辑文件:/etc/nginx/sites-available/default123456789101112131415161718192021222324252627# 如果是多台服务器的话，则在此配置，并修改 location 节点下面的 proxy_pass upstream flask &#123; server 127.0.0.1:5000; server 127.0.0.1:5001;&#125;server &#123; # 监听80端口 listen 80 default_server; listen [::]:80 default_server; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name _; location / &#123; # 请求转发到gunicorn服务器 proxy_pass http://127.0.0.1:5000; # 请求转发到多个gunicorn服务器 # proxy_pass http://flask; # 设置请求头，并将头信息传递给服务器端 proxy_set_header Host $host; # 设置请求头，传递原始请求ip给 gunicorn 服务器 proxy_set_header X-Real-IP $remote_addr; &#125;&#125; Gunicorn Gunicorn（绿色独角兽）是一个Python WSGI的HTTP服务器从Ruby的独角兽（Unicorn ）项目移植该Gunicorn服务器与各种Web框架兼容，实现非常简单，轻量级的资源消耗Gunicorn直接用命令启动，不需要编写配置文件 相关操作安装 pip install gunicorn 查看选项 gunicorn -h 运行 -w: 表示进程（worker） -b：表示绑定ip地址和端口号（bind）gunicorn -w 2 -b 127.0.0.1:5000 运行文件名称:Flask程序实例名 参考阅读： Gunicorn相关配置：https://blog.csdn.net/y472360651/article/details/78538188 其他操作拷贝本地代码到远程 scp -r 本地文件路径 root@39.106.21.198:远程保存路径","categories":[{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/categories/Flask/"}],"tags":[{"name":"Nginx","slug":"Nginx","permalink":"https://zem12345678.github.io/tags/Nginx/"},{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/tags/Flask/"},{"name":"Gunicorn","slug":"Gunicorn","permalink":"https://zem12345678.github.io/tags/Gunicorn/"}]},{"title":"flask-jwt-extended使用详解","slug":"flask-jwt-extended使用详解","date":"2019-03-14T02:59:34.702Z","updated":"2019-03-14T06:30:57.770Z","comments":true,"path":"2019/03/14/flask-jwt-extended使用详解/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/flask-jwt-extended使用详解/","excerpt":"","text":"flask-jwt-extended使用详解 相关配置注册jwt123app.config[&apos;JWT_SECRET_KEY&apos;] = &apos;jwt-secret-attendance&apos;jwt = JWTManager(app)app.config[&apos;JWT_BLACKLIST_ENABLED&apos;] = False 产生token12access_token = create_access_token(identity=username)return jsonify(msg=&quot;登录成功&quot;,access_token=access_token) 获取当前用户1username = get_jwt_identity() 高级用法自定义密钥和加秘方式12345678910access_token = encode_access_token(identity=jwt_manager._user_identity_callback(username), secret='revoked-secret', algorithm=config.algorithm, expires_delta=None, fresh=False, user_claims=jwt_manager._user_claims_callback(username), csrf=config.csrf_protect, identity_claim_key=config.identity_claim_key, user_claims_key=config.user_claims_key, json_encoder=config.json_encoder) 令牌撤销 app.config[‘JWT_BLACKLIST_TOKEN_CHECKS’] = [‘access’, ‘refresh’] 创建我们的函数以检查令牌是否已被列入黑名单。在这简单情况下，我们将只存储在Redis的令JTI（唯一标识符）每当我们创建一个新令牌时（撤销状态为’false’）。这个 function将返回令牌的撤销状态。如果令牌没有 存在于这个商店，我们不知道它来自哪里（因为我们正在新添加 创建令牌到我们的商店，撤销状态为’false’）。在这种情况下 出于安全考虑，我们会考虑撤销令牌。12revoked_store = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True) 1234567@jwt.token_in_blacklist_loaderdef check_if_token_is_revoked(decrypted_token): jti = decrypted_token['jti'] entry = revoked_store.get(jti) if entry is None: return True return entry == 'true' 产生token1234567access_token = create_access_token(identity=username)将令牌存储在redis中，状态目前未被撤销。我们可以使用`get_jti（）`方法获取唯一标识符字符串每个令牌。我们还可以在redis中设置这些令牌的到期时间，所以它们会在到期后自动删除。我们将设定令牌过期后不久将自动删除的所有内容access_jti = get_jti(encoded_token=access_token)revoked_store.set(access_jti, 'false', ACCESS_EXPIRES * 1.2) 撤销视图123456@app.route('/auth/access_revoke', methods=['DELETE'])@jwt_requireddef logout(): jti = get_raw_jwt()['jti'] revoked_store.set(jti, 'true', ACCESS_EXPIRES * 1.2) return jsonify(&#123;\"msg\": \"Access token revoked\"&#125;), 200 在令牌中存储和获取数据自定义一个基类存储角色和用户id12345class UserObject: def __init__(self,id,username, role): self.id = id self.username = username self.role = role 1234567891011创建一个将在create_access_token时调用的函数用来。它将采取任何传递到的对象create_access_token方法，让我们定义什么是自定义声明应添加到访问令牌中 @jwt.user_claims_loader def add_claims_to_access_token(user): return &#123;'role': user.role,'id':user.id&#125;创建一个将在create_access_token时调用的函数用来。它将采取任何传递到的对象create_access_token方法，让我们定义什么标识应该是访问令牌的 @jwt.user_identity_loader def user_identity_lookup(user): return user.username 产生令牌123data = UserObject(id=user.id,username=username,role=user.role)expires = datetime.timedelta(days=2)access_token = create_access_token(identity=data, expires_delta=expires) 如何获取数据123username = get_jwt_claims()user_id = get_jwt_claims()['id']user_role = get_jwt_claims()['role']","categories":[{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/categories/Flask/"}],"tags":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/tags/前后端分离/"},{"name":"Jwt","slug":"Jwt","permalink":"https://zem12345678.github.io/tags/Jwt/"},{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/tags/Flask/"}]},{"title":"神兽保佑,代码无bug　","slug":"神兽保佑,代码无bug","date":"2019-03-14T01:59:47.216Z","updated":"2019-03-14T06:26:22.700Z","comments":true,"path":"2019/03/14/神兽保佑,代码无bug/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/神兽保佑,代码无bug/","excerpt":"","text":"神兽保佑,代码无bug ┏┓ ┏┓+ + ┏┛┻━━━┛┻┓ + + ┃ ┃ ┃ ━ ┃ ++ + + + ████━████ ┃+ ┃ ┃ + ┃ ┻ ┃ ┃ ┃ + + ┗━┓ ┏━┛ ┃ ┃ ┃ ┃ + + + + ┃ ┃ Code is far away from bug with the animal protecting ┃ ┃ + 神兽保佑,代码无bug ┃ ┃ ┃ ┃ + ┃ ┗━━━┓ + + ┃ ┣┓ ┃ ┏┛ ┗┓┓┏━┳┓┏┛ + + + + ┃┫┫ ┃┫┫ ┗┻┛ ┗┻┛+ + + +","categories":[{"name":"杂谈","slug":"杂谈","permalink":"https://zem12345678.github.io/categories/杂谈/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://zem12345678.github.io/tags/随笔/"}]},{"title":"Flask导出excel文件","slug":"Flask导出excel文件","date":"2019-03-14T01:50:50.660Z","updated":"2019-03-14T06:27:01.352Z","comments":true,"path":"2019/03/14/Flask导出excel文件/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Flask导出excel文件/","excerpt":"","text":"Flask导出excel文件 1response = make_response(output.getvalue()) 提示：flask中没有Httpreponse类型这一点与django不同我们需哟啊使用flask中make_responese来导入数据流]。 代码1234567891011121314151617181920212223242526272829303132333435now = datetime.now() time = datetime.strftime(now, &apos;%Y%m%d%H%M%S&apos;) 把时间格式化 filename = time + &apos;.xls&apos; # 创建一个sheet对象 wb = xlwt.Workbook(encoding=&apos;utf-8&apos;) sheet = wb.add_sheet(&apos;order-sheet&apos;) # 写入文件标题 sheet.write(0, 0, &apos;职工号&apos;) sheet.write(0, 1, &apos;姓名&apos;) sheet.write(0, 2, &apos;性别&apos;) sheet.write(0, 3, &apos;身份证号&apos;) sheet.write(0, 4, &apos;手机&apos;) sheet.write(0, 5, &apos;办公室&apos;) sheet.write(0, 6, &apos;邮箱&apos;) sheet.write(0, 7, &apos;QQ&apos;) data_row = 1 for teacher in Teacher.query.all(): sheet.write(data_row, 0, teacher.to_full_dict()[&apos;sno&apos;]) sheet.write(data_row, 1, teacher.to_full_dict()[&apos;name&apos;]) sheet.write(data_row, 2, teacher.to_full_dict()[&apos;sex&apos;]) sheet.write(data_row, 3, teacher.to_full_dict()[&apos;idcard_num&apos;]) sheet.write(data_row, 4, teacher.to_full_dict()[&apos;phone_num&apos;]) sheet.write(data_row, 5, teacher.to_full_dict()[&apos;office&apos;]) sheet.write(data_row, 6, teacher.to_full_dict()[&apos;email&apos;]) sheet.write(data_row, 7, teacher.to_full_dict()[&apos;qq&apos;]) data_row = data_row + 1 # 写出到IO output = BytesIO() wb.save(output) output.seek(0) response = make_response(output.getvalue()) response.headers[&apos;content_type=&apos;] = &apos; application/vnd.ms-excel&apos; # 创建一个文件对象 response.headers[&apos;Content-Disposition&apos;] = &apos;attachment;filename=&#123;&#125;&apos;.format(filename.encode().decode(&apos;latin-1&apos;)) return response","categories":[{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/categories/Flask/"}],"tags":[{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"},{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/tags/Flask/"}]},{"title":"django导入excel文件使用pandas处理并批量插入","slug":"django导入excel文件使用pandas处理并批量插入","date":"2019-03-14T01:26:04.463Z","updated":"2019-03-14T06:31:19.557Z","comments":true,"path":"2019/03/14/django导入excel文件使用pandas处理并批量插入/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/django导入excel文件使用pandas处理并批量插入/","excerpt":"","text":"django导入excel文件使用pandas处理并批量插入导入库：123import pandas as pdimport xlwtfrom io import BytesIO #io数据流 django视图类1234567891011121314151617181920212223242526272829class ImportFarmerData(View): def post(self,request): excel_raw_data = pd.read_excel(request.FILES.get(&apos;file&apos;,&apos;&apos;),header=None) 删除第一行的标题 获取每列 excel_raw_data.drop([0,0],inplace=True) name_col = excel_raw_data.iloc[:,[0]] card_id_col = excel_raw_data.iloc[:,[1]] phone_col = excel_raw_data.iloc[:,[2]] area_num_col = excel_raw_data.iloc[:,[3]]对每一列数据进行处理，从DataFrame类型转换为list类型 name_list = name_col.values.tolist() card_id_list = card_id_col.values.tolist() phone_list = phone_col.values.tolist()对每一列的每一行的数据进行转换，转换为str类型 for i in range(len(name_list)): name_list_index = name_list[i] card_id_list_index = card_id_list[i] phone_list_index = phone_list[i] area_num_index = area_num_list[i] farmer_profile = FarmersProfile() farmer_profile.name = name_list_index[0] farmer_profile.card_id = card_id_list_index[0] farmer_profile.phone = phone_list_index[0] farmer_profile.area_num = area_num_index[0] farmer_profile.address_id = address.id farmer_profile.save() return HttpResponse(json.dumps(&#123;&apos;code&apos;:&apos;200&apos;,&apos;msg&apos;:&apos;导入成功&apos;&#125;) 由于前端使用leiui返回格式必须为json格式 HTML：1&lt;button type=&quot;button&quot; class=&quot;layui-btn&quot; id=&quot;test4&quot; name=&quot;excel_data&quot;&gt;&lt;i class=&quot;layui-icon&quot;&gt;&lt;/i&gt;导入excel&lt;/button&gt; ##ajax:123456789101112131415161718192021layui.use(&apos;upload&apos;, function()&#123; var $ = layui.jquery, upload = layui.upload; //指定允许上传的文件类型 upload.render(&#123; //允许上传的文件后缀 elem: &apos;#test4&apos;, type: &apos;post&apos;, url: &apos;&#123;% url &apos;users:import_famer&apos; %&#125;&apos;, accept: &apos;file&apos;, //普通文件, exts: &apos;xls&apos;, //只允许上传压缩文件, data: &#123;&apos;csrfmiddlewaretoken&apos;: &apos;&#123;&#123; csrf_token &#125;&#125;&apos;&#125;, done: function(res) &#123; if (res.code == 200 ) &#123; layer.msg(res.msg); &#125; &#125; ,error:function (res) &#123; &#125; &#125;);","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"},{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"}]},{"title":"最长公共子序列（LCS）问题","slug":"最长公共子序列（LCS）问题","date":"2019-03-13T15:41:20.313Z","updated":"2019-03-14T14:50:35.724Z","comments":true,"path":"2019/03/13/最长公共子序列（LCS）问题/","link":"","permalink":"https://zem12345678.github.io/2019/03/13/最长公共子序列（LCS）问题/","excerpt":"","text":"最长公共子序列（LCS）问题给定两个 1 到 n 的排列 A,B （即长度为 n 的序列，其中 [1,n] 之间的所有数都出现了恰好一次）。 求 它们的最长公共子序列长度。 子序列： 一个序列A ＝ a1,a2,……an,中任意删除若干项，剩余的序列叫做A的一个子序列。也可以认为是从序列A按原顺序保留任意若干项得到的序列。求解算法对于母串X=&lt;x1,x2,⋯,xm&gt;, Y=&lt;y1,y2,⋯,yn&gt;，求LCS与最长公共子串。 暴力解法假设 m&lt;n， 对于母串X，我们可以暴力找出2的m次方个子序列，然后依次在母串Y中匹配，算法的时间复杂度会达到指数级O(n∗2的m次)。显然，暴力求解不太适用于此类问题。 动态规划假设Z=&lt;z1,z2,⋯,zk&gt;是X与Y的LCS， 我们观察到如果Xm=Yn，则Zk=Xm=Yn，有Zk−1是Xm−1与Yn−1的LCS；如果Xm≠Yn，则Zk是Xm与Yn−1的LCS，或者是Xm−1与Yn的LCS。因此，求解LCS的问题则变成递归求解的两个子问题。但是，上述的递归求解的办法中，重复的子问题多，效率低下。改进的办法——用空间换时间，用数组保存中间状态，方便后面的计算。先假设有 C[i,j] = | LCS(x[1…i] , y(1…j)) |，则有 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// n：表示两序列长度// a：描述序列 a（这里需要注意的是，由于 a 的下标从 1 开始，因此 a[0] 的值为-1，你可以忽略它的值，只需知道我们从下标 1 开始存放有效信息即可） // b：描述序列b（同样地，b[0] 的值为 -1）// 返回值：最长公共子序列的长度#include &lt;cstdio&gt; #include &lt;cstring&gt; #include &lt;cmath&gt; #include &lt;cstdlib&gt; #include &lt;algorithm&gt; #include &lt;queue&gt; #include &lt;stack&gt; #include &lt;map&gt; #include &lt;set&gt; #include &lt;vector&gt; #include &lt;iostream&gt;#include &lt;utility&gt;using namespace std;#define PII pair&lt;int,int&gt; #define PIII pair&lt;pair&lt;int,int&gt;,int&gt; #define mp make_pair#define pb push_back#define sz size() #define fi first#define se secondtypedef unsigned int ui;typedef long long ll;typedef unsigned long long ull;vector&lt;int&gt; pos,a,b,f;int getAns(int n,vector&lt;int&gt;a,vector&lt;int&gt;b)&#123; f.resize(n+1);pos.resize(n+1); for(int i=0;i&lt;=n;++i) f[i]=n+2; for(int i=1;i&lt;=n;++i)&#123; pos[b[i]]=i; &#125; for(int i=1;i&lt;=n;++i)&#123; a[i]=pos[a[i]]; &#125; f[0]=0; for(int i=1;i&lt;=n;++i) *lower_bound(f.begin(),f.end(),a[i])=a[i]; return int(lower_bound(f.begin(),f.end(),n+1)-f.begin())-1;&#125;int main()&#123; int n;scanf(&quot;%d&quot;,&amp;n); a.resize(n+1);b.resize(n+1); for(int i=1;i&lt;=n;++i)&#123; scanf(&quot;%d&quot;,&amp;a[i]); &#125; for(int i=1;i&lt;=n;++i)&#123; scanf(&quot;%d&quot;,&amp;b[i]); &#125; printf(&quot;%d\\n&quot;,getAns(n,a,b)); return 0;&#125; #输出结果123451 2 4 3 55 2 3 4 12","categories":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/tags/算法/"},{"name":"LeetCood","slug":"LeetCood","permalink":"https://zem12345678.github.io/tags/LeetCood/"},{"name":"数据结构","slug":"数据结构","permalink":"https://zem12345678.github.io/tags/数据结构/"},{"name":"C/C++","slug":"C-C","permalink":"https://zem12345678.github.io/tags/C-C/"}]},{"title":"Django Restful Framework(DRF)的开发思考（2）","slug":"Django Restful Framework(DRF)的开发思考（2）","date":"2019-03-13T14:59:11.338Z","updated":"2019-03-14T06:28:27.431Z","comments":true,"path":"2019/03/13/Django Restful Framework(DRF)的开发思考（2）/","link":"","permalink":"https://zem12345678.github.io/2019/03/13/Django Restful Framework(DRF)的开发思考（2）/","excerpt":"","text":"Restful一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制： CORS CORS是现代浏览亲支持快于请求的一种方式，全称是“跨域资源共享“(Cross-origin resource sharing),当使用XMLHttpRequest发送请求时，浏览器发现该请求不符合同源测率，会给该请求头：Origin，后台进行以系列处理，如果确定请求则在返回结果加入一个响应头：Access-Control-Allow-Origin；浏览器判断该响应头中是否包含Origin的值，如果有浏览器会处理响应，我们就可以拿到响应数据，如果不包含浏览器之间驳回，这时我们无法拿到响应数据 JWT ：Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密；FastDFS ：FastDFS 是用 c 语言编写的一款开源的分布式文件系统。FastDFS 为互联网量身定制， 充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用 FastDFS 很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。FastDFS 架构包括 Tracker server 和 Storage server。客户端请求 Tracker server 进行文 件上传、下载，通过 Tracker server 调度最终由 Storage server 完成文件上传和下载。 [TOC] Django认证系统|跨域请求|Celery提供了用户模型类User和User的相关操作方法。 自定义User模型类之后，需要设置配置项AUTH_USER_MODEL。 1. 前后端域名设置域名和IP是对应的关系。 DNS解析：获取域名对应的IP 通过域名访问网站时，先到本地的hosts中查找IP和域名的对应关系，如果查到直接根据IP访问网站，如果查不到再进行DNS域名解析。 前端服务器域名: www.meiduo.site 后端API服务器域名：api.meiduo.site 2. 短信验证码123456URL: GET /sms_codes/(?P&lt;mobile&gt;1[3-9]\\d&#123;9&#125;)/参数: url地址中传递mobile响应: &#123; \"message\": \"OK\" &#125; 3. 跨域请求浏览器的同源策略: 协议、主机IP和端口PORT相同的地址是同源，否则是非同源。 当发起请求的页面地址和被请求的地址不是同源，那么这个请求就是跨域请求。 在发起请求时，如果浏览器发现请求是跨域请求，那么在请求的报文头中，会添加如下信息: Origin: 源请求IP地址 例如：Origin: http://www.meiduo.site:8080 在被请求的服务器返回的响应中，如果响应头中包含如下信息： Access-Control-Allow-Origin: 源请求IP地址 例如：Access-Control-Allow-Origin: http://www.meiduo.site:8080 那么浏览器认为被请求服务器支持来源地址对其进行跨域请求，否则认为不支持，浏览器会将请求直接驳回。 Django跨域请求扩展使用。 4. celery异步任务队列本质: ​ 使用进程或协程调用函数实现异步。 基本概念： ​ 发出者：发出所有执行的任务(任务就是函数)。 ​ (中间人)任务队列：存放所要执行的任务信息。 ​ 处理者：也就是工作的进程或协程，负责监听任务队列，发现任务便执行对应的任务函数。 特点： ​ 1）任务发送者和处理者可以分布在不同的电脑上，通过中间人进行信息交换。 ​ 2）任务队列中的任务会进行排序，先添加的任务会被先执行。 使用： ​ 1）安装 pip install celery ​ 2）创建Celery对象并配置中间人地址 ​ from celery import Celery ​ celery_app = Celery(‘demo’) ​ 配置文件：broker_url=’中间人地址’ ​ celery_app.config_from_object(‘配置文件路径’) ​ 3）定义任务函数 ​ @celery_app.task(name=’my_first_task’) ​ def my_task(a, b): ​ print(‘任务函数被执行’) ​ … ​ 4）启动worker ​ celery -A ‘celery_app文件路径’ worker -l info ​ 5）发出任务 ​ my_task.delay(2, 3) 5. 用户注册12345678910111213141516URL: POST /users/参数: &#123; \"username\": \"用户名\", \"password\": \"密码\", \"password2\": \"重复密码\", \"mobile\": \"手机号\", \"sms_code\": \"短信验证码\", \"allow\": \"是否同意协议\" &#125;响应: &#123; \"id\": \"用户id\", \"username\": \"用户名\", \"mobile\": \"手机号\" &#125; 补充(axios请求发送)1234567891011121314151617axios.get('url地址', [config]) .then(response =&gt; &#123; // 请求成功，可通过response.data获取响应数据 &#125;) .catch(error =&gt; &#123; // 请求失败，可通过error.response获取响应对象 // error.response.data获取响应数据 &#125;)axios.post('url地址', [data], [config]) .then(response =&gt; &#123; // 请求成功，可通过response.data获取响应数据 &#125;) .catch(error =&gt; &#123; // 请求失败，可通过error.response获取响应对象 // error.response.data获取响应数据 &#125;) 注册|JWT Token|普通登录1. 用户注册12345678910111213141516URL: POST /users/参数: &#123; \"username\": \"用户名\", \"password\": \"密码\", \"password2\": \"重复密码\", \"mobile\": \"手机号\", \"sms_code\": \"短信验证码\", \"allow\": \"是否同意协议\" &#125;响应: &#123; \"id\": \"用户id\", \"username\": \"用户名\", \"mobile\": \"手机号\" &#125; 创建新用户：User.objects.create_user(username, email=None, password=None, *\\extra_fields*) 2. JWT token1）session认证 12345671. 接收用户名和密码2. 判断用户名和密码是否正确3. 保存用户的登录状态(session) session['user_id'] = 1 session['username'] = 'smart' session['mobile'] = '13155667788'4. 返回应答，登录成功 session认证的一些问题: session存储在服务器端，如果登录的用户过多，服务器开销比较大。 session依赖于cookie，session的标识存在cookie中，可能会有CSRF(跨站请求伪造)。 2）jwt 认证机制(替代session认证) 12341. 接收用户名和密码2. 判断用户名和密码是否正确3. 生成(签发)一个jwt token(token中保存用户的身份信息) 公安局(服务器)=&gt;签发身份证(jwt token)4. 返回应答，将jwt token信息返回给客户端。 如果之后需要进行身份认证，客户端需要将jwt token发送给服务器，由服务器验证jwt token的有效性。 3）jwt 的数据格式 12eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFO a）headers头部 1234&#123; &quot;token类型声明&quot;, &quot;加密算法&quot;&#125; base64加密(很容易被解密) b）payload(载荷)：用来保存有效信息 123456&#123; &quot;user_id&quot;: 1, &quot;username&quot;: &quot;smart&quot;, &quot;mobile&quot;: &quot;13155667788&quot;, &quot;exp&quot;: &quot;有效时间&quot;&#125; base64加密 c）signature(签名)：防止jwt token被伪造 将headers和payload进行拼接，用.隔开，使用一个密钥(secret key)进行加密，加密之后的内容就是签名。 jwt token是由服务器生成，密钥保存在服务器端。 3. jwt 扩展签发jwt token1234567from rest_framework_jwt.settings import api_settingsjwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLERjwt_encode_handler = api_settings.JWT_ENCODE_HANDLERpayload = jwt_payload_handler(user)token = jwt_encode_handler(payload) 4. 用户登录1）jwt 扩展的登录视图obtain_jwt_token： ​ 接收username和password，进行用户名和密码验证，正确情况下签发jwt token并返回给客户端。 2）更改obtain_jwt_token组织响应数据的函数： 123456789101112131415def jwt_response_payload_handler(token, user=None, request=None): \"\"\" 自定义jwt认证成功返回数据 \"\"\" return &#123; 'token': token, 'user_id': user.id, 'username': user.username &#125;# JWT扩展配置JWT_AUTH = &#123; # ... 'JWT_RESPONSE_PAYLOAD_HANDLER': 'users.utils.jwt_response_payload_handler',&#125; 3）登录支持用户名和手机号 123obtain_jwt_token-&gt; from django.contrib.auth import authenticate-&gt; from django.contrib.auth.backends import ModelBackend(authenticate: 根据用户名和密码进行校验) 自定义Django的认证系统后端，同时设置配置项AUTHENTICATION_BACKENDS。 ##QQ登录 1. QQ登录-预备工作1）成为QQ开发者 2）创建开发者应用 3）查询QQ登录开发文档 2. QQ登录-开发关键点获取QQ登录用户的唯一身份标识(openid)，然后根据openid进行处理。 判断该QQ用户是否绑定过本网站的用户，如果绑定过，直接登录，如果未绑定过，先进行绑定。 QQ用户绑定: 将openid 和 用户user_id 对应关系存下来。 id user_id openid 1 2 Akdk19389kDkdkk99939kdk 2 2 AKdkk838e8jdiafkdkkFKKKFf 注：一个用户可以绑定多个qq账户。 3. QQ登录API1）获取QQ登录网址API 12345API: GET /oauth/qq/authorization/?next=&lt;url&gt;响应:&#123; \"login_url\": \"QQ登录网址\"&#125; 2）获取QQ登录用户openid并进行处理API 12345678910111213API: GET /oauth/qq/user/?code=&lt;code&gt;参数: code响应: 1）如果openid已经绑定过本网站的用户，直接签发jwt token，返回 &#123; 'user_id': &lt;用户id&gt;, 'username': &lt;用户名&gt;, 'token': &lt;token&gt; &#125; 2）如果openid没有绑定过本网站的用户，先对openid进行加密生成token，把加密的内容返回 &#123; 'access_token': &lt;token&gt; &#125; 3）绑定QQ登录用户的信息API 1234567891011121314API: POST /oauth/qq/user/参数: &#123; \"mobile\": &lt;手机号&gt;, \"password\": &lt;密码&gt;, \"sms_code\": &lt;短信验证码&gt;, \"access_token\": &lt;access_token&gt; &#125;响应: &#123; 'id': &lt;用户id&gt;, 'username': &lt;用户名&gt;, 'token': &lt;token&gt; &#125; ###4. 相关模块的使用 1234567891011# 将python字典转化为查询字符串from urllib.parse import urlencode # 将查询字符串转换成python字典from urllib.parse import parse_qs# 发起网络请求from urllib.request import urlopen# itsdangerous: 加密和解密from itsdangerous import TimedJSONWebSignatureSerializer 邮件发送|省市三级联动个人信息获取1）给User添加email_active字段，用于记录邮箱email是否被激活。 2）API接口：获取用户个人信息。 123456789101112请求方式和URL地址: GET /user/前端传递参数: 在请求头中携带jwt token。返回值: &#123; \"id\": \"&lt;用户id&gt;\", \"username\": \"&lt;用户名&gt;\", \"mobile\": \"&lt;手机号&gt;\", \"email\": \"&lt;邮箱&gt;\", \"email_active\": \"&lt;激活标记&gt;\" &#125;注: DRF中`JSONWebTokenAuthentication`认证机制会根据jwt token对用户身份进行认证，如果认证失败返回401错误，如果是权限禁止返回403错误。 3）使用RetrieveAPIView时，其获取单个对象时是根据pk获取，我们这里所有获取的是当前登录的用户，所以需要把get_object方法进行重写。 4）request对象的user属性。 对于request对象，有一个user属性。这个user属性： ​ a）如果用户认证成功，request.user是User模型类的实例对象，存放的是当前登录用户的信息。 ​ b）如果用户未认证，request.user是AnonymousUser类的实例对象。 ###用户邮箱设置 1）API接口：设置用户个人邮箱。 123456789请求方式和URL地址: PUT /email/前端传递参数: 1）在请求头中携带jwt token。 2）邮箱email返回值: &#123; \"id\": \"&lt;用户id&gt;\", \"email\": \"&lt;邮箱&gt;\", &#125; 2）处理流程: ​ a）接收参数并进行校验(email是否传递，格式是否正确) ​ b）保存用户的邮箱信息并发送激活邮件 ​ c）返回应答，设置邮箱成功 3）发送激活邮件 ​ a）生成激活链接：在激活链接中需要保存待激活用户的id和email，但是为了安全，需要对信息进行加密。 ​ b）发送邮件：配置文件中先进行邮件发送配置，在使用django的send_mail方法发送邮件，为了不影响邮件设置过程，邮件采用celery发送。 12from django.core.mail import send_mailsend_mail(subject='邮件主题', message='正文', from_email='发件人', recipient_list='收件人列表', html_message='html邮件正文') 个人邮箱激活1）API接口：激活用户个人邮箱。 1234567请求方式和URL地址: GET /emails/verification/?token=xxx前端传递参数: 1）token返回值: &#123; \"message\": \"处理结果\" &#125; 2）处理流程: ​ a）接收参数token并进行校验(token是否传递，token是否有效) ​ b）将用户邮箱激活标记设置为已激活。 ​ c）返回应答，邮箱激活成功。 省市县三级信息1）信息存储(自关联) 地区的自关联其实是一个特殊一对多的关系。 一个省包含很多个市，一个市包含很多县。 id(地区id) name(地区名) parent_id(父级地区ID) 320000 江苏省 NULL 320200 无锡市 320000 320282 宜兴市 320200 2）模型类自关联(地区的自关联其实是一个特殊的一对多) 123456class Area(models.Model): \"\"\" 行政区域 \"\"\" name = models.CharField(max_length=20, verbose_name='名称') parent = models.ForeignKey('self', on_delete=models.SET_NULL, related_name='subs', null=True, blank=True, verbose_name='上级行政区域') 注：模型类自关联，ForeignKey第一个参数传self。 另外之前的关联查询中，有了一个area对象之后，查询关联的下级地区信息和父级地区，例如: 123area = Area.objects.get(id='320200')area.parent # 父级地区，由多查一，对象名.关联属性area.area_set.all() # 子级地区，由一查多，对象名.多类名_set.all() 当ForeignKey创建关联属性时，指定了related_name=&#39;subs&#39;之后，再查询和area对象关联的子级地区，不在使用area.area_set.all()，而是使用area.subs.all()。 3）定义导入地区信息shell脚本 12#! /bin/bashmysql -u'&lt;用户名&gt;' -p'&lt;密码&gt;' -h'&lt;数据库主机IP&gt;' '&lt;数据库名&gt;' &lt; '&lt;sql文件&gt;' 4）地区视图集。 补充(客户端发送请求时携带JWT token数据)12345678910111213axios.get(this.host + '/user/', &#123; headers: &#123; // 通过请求头向后端传递JWT token的方法 'Authorization': 'JWT ' + &lt;JWT token数据&gt; &#125;, responseType: 'json',&#125;).then(response =&gt; &#123; ...&#125;).catch(error =&gt; &#123; ...&#125;);","categories":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/categories/前后端分离/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Restful","slug":"Restful","permalink":"https://zem12345678.github.io/tags/Restful/"},{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/tags/前后端分离/"}]},{"title":"Django Restful Framework(DRF)的开发思考（1）","slug":"Django Restful Framework(DRF)的开发思考（1）","date":"2019-03-13T14:19:12.518Z","updated":"2019-03-14T06:29:15.596Z","comments":true,"path":"2019/03/13/Django Restful Framework(DRF)的开发思考（1）/","link":"","permalink":"https://zem12345678.github.io/2019/03/13/Django Restful Framework(DRF)的开发思考（1）/","excerpt":"","text":"Restful一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制： CORS CORS是现代浏览亲支持快于请求的一种方式，全称是“跨域资源共享“(Cross-origin resource sharing),当使用XMLHttpRequest发送请求时，浏览器发现该请求不符合同源测率，会给该请求头：Origin，后台进行以系列处理，如果确定请求则在返回结果加入一个响应头：Access-Control-Allow-Origin；浏览器判断该响应头中是否包含Origin的值，如果有浏览器会处理响应，我们就可以拿到响应数据，如果不包含浏览器之间驳回，这时我们无法拿到响应数据 JWT ：Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密； FastDFS ：FastDFS 是用 c 语言编写的一款开源的分布式文件系统。FastDFS 为互联网量身定制， 充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用 FastDFS 很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。FastDFS 架构包括 Tracker server 和 Storage server。客户端请求 Tracker server 进行文 件上传、下载，通过 Tracker server 调度最终由 Storage server 完成文件上传和下载。 Web网站中开发模式（前后端分离&amp;前后端不分离）web开发模式 开发模式 说明 前后端不分离 前端展示的数据效果最终是由后端进行控制的，由后端使用模板进行模板的渲染，将渲染后的内容返回给客户端进行显示 前后端分离 后端服务器只返回前端所需要数据，前端获取数据之后自己控制数据怎么进行展示 注意：前端发起ajax-&gt;后端服务器返回分类新闻数据-&gt;前端进行页面拼接和展示。 RestAPI接口风格前后端分离开发中广泛采用的一种API设计风格。​关键点：url地址尽量使用名词，不要出现动词。采用不同请求方式，执行不同操作。 GET 获取​POST 新增PUT 修改​DELETE 删除GET /books/: 获取所有图书，返回所有图书信息POST /books/: 新建一本图书，返回新建的图书信息GET /books/id/: 获取指定图书，返回指定图书信息PUT /books/id/: 修改指定图书，返回修改图书信息DELETE /books/id/: 删除指定图书，返回空文档 过滤参数放在查询字符串中。响应状态码选择。 200：获取或修改​201：新建​204：删除​400：客户端请求有误​404：客户端请求的资源找不到​500：服务器出错 响应数据返回json。 使用Django知识自定义RestAPI接口准备工作-&gt;创建应用-&gt;定义模板类-&gt;生成数据表-&gt;添加测试数据。 使用Django知识自定义RestAPI接口: 获取所有图书的信息 GET /books/ 新增一本图书信息 POST /books/ 获取指定的图书信息 GET /books/(?P\\d+)/ 修改指定的图书信息 PUT /books/(?P\\d+)/ 删除指定的图书信息 DELETE /books/(?P\\d+)/ RestAPI接口开发时的工作(序列化和反序列化) 开发模式 说明 序列化 将程序中一种数据结构类型转化为其他的数据格式，这个过程叫做序列化。在我们前面例子中，将模型对象转化为python字典或json数据，这个过程可以认为是序列化过程。 反序列化: 将其他格式数据转换为程序中数据，这个过程叫做反序列化。在我们前面例子中，将客户端发送的数据保存在模型对象中，这个过程可以认为是反序列化过程。 主要工作 ​ 1）将请求数据保存在模型对象中(反序列化)。​ 2）操作数据库。​ 3）将模型对象转换为前端所需的格式(序列化)。 ## 序列化器|视图类|拓展类序列化类的功能：进行序列化和反序列化 序列化：将对象转化为字典数据反序列化：1）数据校验 2）数据保存(新增&amp;更新)。定义序列化器类：继承Serializer或ModelSerializer 1234567891011121314151617181920212223242526272829from rest_framework import serializers# serializers.Serializer：定义任何序列化器类时，都可以直接继承于Serializer# serializers.ModelSerializer：如果定义的序列化器类针对的是一个模型类，可以直接继承ModelSerializerclass User(object): def __init__(self, username, age): self.username = username self.age = ageclass UserSerializer(serializers.Serializer): &quot;&quot;&quot;序列化器类&quot;&quot;&quot; # 序列化器字段 = serializers.字段类型(选项参数) username = serializers.CharField() age = serializers.IntegerField()if __name__ == &apos;__main__&apos;: user = User(&apos;smart&apos;, 18) # &#123; # &quot;username&quot;: &quot;smart&quot;, # &quot;age&quot;: 18 # &#125; serializer = UserSerializer(user) # 获取序列化之后的字典数据 serializer.data &quot;hello&quot; 1. 序列化器-序列化1）序列化单个对象 123book = BookInfo.objects.get(id=1)serializer = BookInfoSerializer(book)serializer.data 2）序列化多个对象 123books = BookInfo.objects.all()serializer = BookInfoSerializer(books, many=True)serializer.data 3）关联对象的序列化 123451. 将关联对象序列化为关联对象主键 PrimaryKeyRelatedFieldå2. 将关联对象使用指定的序列化器进行序列化3. 将关联对象序列化为关联对象模型类__str__方法的返回值 StringRelatedField注意：如果关联对象有多个，定义字段时，需要添加many=True 2. 序列化器-反序列化1）反序列化之数据校验 1234567891011data = &#123;'btitle': 'python'&#125;serializer = BookInfoSerializer(data=data)serializer.is_valid() # 调用此方法会对传递的data数据内容进行校验，校验成功返回True, 否则返回Falseserializer.errors # 获取校验失败的错误信息serializer.validated_data # 获取校验之后的数据# 补充验证行为1. 对对应的字段指定validators2. 序列化器类中定义对应的方法validate_&lt;field_name&gt;对指定的字段进行校验3. 如果校验需要结合多个字段内容，定义validate方法 2）反序列化之数据保存(新增或更新) 1234567891011121314151617# 数据校验之后，调用此方法可以进行数据保存，可能会调用序列化器类中的create或updateserializer.save() # 调用create，创建序列化器对象时只传递了datadata = &#123;'btitle': 'python'&#125;serializer = BookInfoSerializer(data=data)serializer.is_valid()serializer.save()# 调用update，创建序列化器对象时传递了data和对象book = BookInfo.objects.get(id=1)data = &#123;'btitle': 'python'&#125;serializer = BookInfoSerializer(book, data=data)serializer.is_valid()serializer.save()# 注意: Serializer类中create和update没有进行实现，需要自己实现代码。 3. 使用Serializer改写Django自定义RestAPI接口将序列化和反序列化部分代码使用序列化器完成。 4. APIView视图类&amp;Request对象&amp;Response对象1）APIView视图类 ​ APIView是DRF框架中所有视图类的父类。 ​ 继承自APIView之后，处理函数中的request参数不再是Django原始的HttpRequest对象，而是由DRF框架封装的Request类的对象。 ​ 进行异常处理。 ​ 认证&amp;权限&amp;限流。 2）Request类| 属性 | 说明 || :——– | ——–:|| equest.data: | 包含传递的请求体数据(比如之前的request.body和request.POST)，并且已经转换成了字典或类字典类型。 || 前后端分离 | 包含查询字符串参数(相当于之前的request.GET) |​3）Response类 ​ 通过Response返回响应数据时，会根据前端请求头中的Accept转化成对应的响应数据类型，仅支持json和html，默认返回json。 4）补充 类视图对象`self.kwargs`保存这从url地址中提取的所有命名参数。 5. 使用APIView改写Django自定义RestAPI接口将获取参数以及响应部分代码进行改写。 6. GenericAPIView视图类APIView类的子类，封装了操作序列化器和操作数据库的方法，经常和Mixin扩展类配合使用。 过滤&amp;分页。 属性： ​serializer_class: 指定视图使用的序列化器类​queryset: 指定视图使用的查询集 方法: get_serializer_class: 返回当前视图使用的序列化器类​get_serializer: 创建一个序列化器类的对象​get_queryset: 获取当前视图使用的查询集​get_object: 获取单个对象，默认根据主键进行查询 ##子类视图类|视图集|路由Router| 类名 | 说明 || ————– | ———————————————————— || APIView | 1）继承自View，封装了Django 本身的HttpRequest对象为Request对象。2）统一的异常处理。 3）认证&amp;权限&amp;限流。 || GenericAPIView | 1）继承自APIView，提供了操作序列化器和数据库数据的方法，通常和Mixin扩展类配合使用。2）过滤&amp;分页。 | 1. Mixin扩展类DRF提供了5个扩展类，封装了5个通用的操作流程。 类名 说明 ListModelMixin 提供了一个list方法，封装了返回模型数据列表信息的通用流程。 CreateModelMixin 提供了一个create方法，封装了创建一个模型对象数据信息的通用流程。 RetrieveModelMixin 提供了一个retrieve方法，封装了获取一个模型对象数据信息的通用流程。 UpdateModelMixin 提供了一个update方法，封装了更新一个模型对象数据信息的通用流程。 DestroyModelMixin 提供了一个destroy方法，封装了删除一个模型对象数据信息的通用流程。 2. 子类视图为了方便我们开发RestAPI，DRF框架除了提供APIView和GenericAPIView视图类之外，还提供了一些子类视图类，这些子类视图类同时继承了GenericAPIView和对应的Mixin扩展类，并且提供了对应的请求方法。 类名 说明 ListAPIView 1）继承自ListModelMixin和GenericAPIView。2）如果想定义一个视图只提供列出模型列表信息的接口，继承此视图类是最快的方式。 CreateAPIView 1）继承自CreateModelMixin和GenericAPIView。2）如果想定义一个视图只提供创建一个模型信息的接口，继承此视图类是最快的方式。 RetrieveAPIView 1）继承自RetrieveModelMixin和GenericAPIView。2）如果想定义一个视图只提供获取一个模型信息的接口，继承此视图类是最快的方式。 UpdateAPIView 1）继承自UpdateModelMixin和GenericAPIView。2）如果只想定义一个视图只提供更新一个模型信息的接口，继承此视图类是最快的方式。 DestroyAPIView 1）继承自DestroyModelMixin和GenericAPIView。2）如果只想定义一个视图只提供删除一个模型信息的接口，继承此视图类是最快的方式。 ListCreateAPIView 1）继承自ListModelMixin，CreateModelMixin和GenericAPIView。2）如果只想定义一个视图提供列出模型列表和创建一个模型信息的接口，继承此视图类是最快的方式。 RetrieveUpdateAPIView 1）继承自RetrieveModelMixin，UpdateModelMixin和GenericAPIView。2）如果只想定义一个视图提供获取一个模型信息和更新一个模型信息的接口，继承此视图类是最快的方式。 RetrieveDestroyAPIView 1）继承自RetrieveModelMixin，DestroyModelMixin和GenericAPIView。2）如果只想定义一个视图提供获取一个模型信息和删除一个模型信息的接口，继承此视图类是最快的方式。 RetrieveUpdateDestoryAPIView 1）继承自RetrieveModelMixin，UpdateModelMixin，DestroyModelMixin和GenericAPIView。2）如果只想定义一个视图提供获取一个模型信息和更新一个模型信息和删除一个模型信息的接口，继承此视图类是最快的方式。 示例1： 123456789101112131415161718# 需求1：写一个视图，提供一个接口 1. 获取一组图书数据 GET /books/ class BookListView(ListAPIView): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer # 需求2：写一个视图，提供一个接口 1. 获取指定的图书数据 GET /books/(?P&lt;pk&gt;\\d+)/ class BookDetailView(RetrieveAPIView): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer # 需求3：写一个视图，提供两个接口 1. 获取指定的图书数据 GET /books/(?P&lt;pk&gt;\\d+)/ 2. 更新指定的图书数据 PUT /books/(?P&lt;pk&gt;\\d+)/ class BookDetailView(RetrieveUpdateAPIView): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer 3. 视图集类将操作同一资源的处理方法放在同一个类中(视图集)，处理方法不要以请求方式命名，而是以对应的action命名， ​ list: 提供一组数据 ​ create: 创建一条新数据 ​ retrieve: 获取指定的数据 ​ update: 更新指定的数据 ​ destroy: 删除指定的数据 进行url配置时需要指明请求方法和处理函数之间的对应关系。 类名 说明 ViewSet 1）继承自ViewSetMixin和APIView。2）如果使用视图集时不涉及数据库和序列化器的操作，可以直接继承此类。 GenericViewSet 1）继承自ViewSetMixin和GenericAPIView。2）如果使用视图集涉及数据库和序列化器的操作，可以直接继承此类。 ModelViewSet 1）继承自5个Mixin扩展类和GenericViewSet。2）如果使用视图集想一次提供通用的5种操作，继承这个类是最快的。 ReadOnlyModelViewSet 1）继承自ListModelMixin，RetrieveModelMixin和GenericViewSet。2）如果使用视图集想一次提供list操作和retrieve操作，继承这个类是最快的。 示例1： 123456789101112131415161718192021# 需求1：写一个视图集，提供以下两种操作 1. 获取一组图书信息(list) GET /books/ 2. 新建一本图书信息(create) POST /books/ class BookInfoViewSet(ListModelMixin, CreateModelMixin, GenericViewSet): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer# 需求2：写一个视图集，提供以下两种操作 1. 获取一组图书信息(list) GET /books/ 2. 获取指定图书信息(retrieve) GET /books/(?P&lt;pk&gt;\\d+)/ class BookInfoViewSet(ReadOnlyModelViewSet): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer # 需求3：写一个视图集，提供以下三种操作 1. 获取一组图书信息(list) GET /books/ 2. 获取指定图书信息(retrieve) GET /books/(?P&lt;pk&gt;\\d+)/ 3. 更新指定图书信息(update) PUT /books/(?P&lt;pk&gt;\\d+)/ class BookInfoViewSet(UpdateModelMixin, ReadOnlyModelViewSet): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer 注: 除了常见的5种基本操作之外，如果想给一个视图集中添加其他处理方法，直接在视图集中定义即可。 4. 路由Router1）路由Router是专门配合视图集来使用的，可以使用Router自动生成视图集中相应处理函数对应的URL配置项。 2）使用Router自动生成视图集中相应处理函数对应的URL配置项时，除了常见的5种基本操作之外，如果视图集中有添加的其他处理方法，则需要给这些方法加上action装饰器之后，才会动态生成其对应的URL配置项。 其他功能1）认证&amp;权限 2）限流 控制用户访问API接口的频率。 针对匿名用户和认证用户分别进行限流。 1234567891011# 限流(针对匿名用户和认证用户分别进行限流控制)'DEFAULT_THROTTLE_CLASSES': ( 'rest_framework.throttling.AnonRateThrottle', # 针对匿名用户 'rest_framework.throttling.UserRateThrottle' # 针对认证用户),# 限流频次设置'DEFAULT_THROTTLE_RATES': &#123; 'user': '5/minute', # 认证用户5次每分钟 'anon': '3/minute', # 匿名用户3次每分钟&#125;, 针对匿名用户和认证用户统一进行限流。 123456789# 限流(针对匿名用户和认证用户进行统一限流控制)'DEFAULT_THROTTLE_CLASSES': ( 'rest_framework.throttling.ScopedRateThrottle',),'DEFAULT_THROTTLE_RATES': &#123; 'contacts': '5/minute', 'upload': '3/minute',&#125;, 3）过滤&amp;排序 4）分页 两种分页方式PageNumberPagination和LimitOffsetPagination。 使用PageNumberPagination分页时，获取分页数据时可以通过page传递页码参数。如果想要分页时指定页容量，需要自定义分页类。 1234567class StandardResultPagination(PageNumberPagination): # 默认页容量 page_size = 3 # 指定页容量参数名称 page_size_query_param = 'page_size' # 最大页容量 max_page_size = 5 使用LimitOffsetPagination分页时，获取分页数据时可以传递参数offset(偏移量)和limit(限制条数)。 注：如果使用的全局分页设置，某个列表视图如果不需要分页，直接在视图类中设置pagination_class = None。 5）异常 DRF自带异常处理功能，可以对某些特定的异常进行处理并返回给客户端组织好的错误信息。能够处理的异常如下: 12345678910APIException 所有异常的父类ParseError 解析错误AuthenticationFailed 认证失败NotAuthenticated 尚未认证PermissionDenied 权限决绝NotFound 未找到MethodNotAllowed 请求方式不支持NotAcceptable 要获取的数据格式不支持Throttled 超过限流次数ValidationError 校验失败 可以自定义DRF框架的异常处理函数(补充一些异常处理)并指定EXCEPTION_HANDLER配置项。 6）接口文档","categories":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/categories/前后端分离/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Restful","slug":"Restful","permalink":"https://zem12345678.github.io/tags/Restful/"},{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/tags/前后端分离/"}]},{"title":"SpringcloudVue项目学习笔记","slug":"SpringcloudVue项目学习笔记","date":"2018-11-14T12:21:21.836Z","updated":"2019-03-13T15:15:09.301Z","comments":true,"path":"2018/11/14/SpringcloudVue项目学习笔记/","link":"","permalink":"https://zem12345678.github.io/2018/11/14/SpringcloudVue项目学习笔记/","excerpt":"","text":"springcloud-vue-project架构图","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zem12345678.github.io/categories/学习笔记/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://zem12345678.github.io/tags/SpringCloud/"},{"name":"Vue","slug":"Vue","permalink":"https://zem12345678.github.io/tags/Vue/"},{"name":"微服务","slug":"微服务","permalink":"https://zem12345678.github.io/tags/微服务/"},{"name":"Java","slug":"Java","permalink":"https://zem12345678.github.io/tags/Java/"}]},{"title":"day01-springboot","slug":"day01-springboot","date":"2018-11-14T12:13:33.696Z","updated":"2019-03-13T15:22:41.883Z","comments":true,"path":"2018/11/14/day01-springboot/","link":"","permalink":"https://zem12345678.github.io/2018/11/14/day01-springboot/","excerpt":"","text":"0.学习目标 了解SpringBoot的作用 掌握java配置的方式 了解SpringBoot自动配置原理 掌握SpringBoot的基本使用 了解Thymeleaf的基本使用 1. 了解SpringBoot在这一部分，我们主要了解以下3个问题： 什么是SpringBoot 为什么要学习SpringBoot SpringBoot的特点 1.1.什么是SpringBootSpringBoot是Spring项目中的一个子工程，与我们所熟知的Spring-framework 同属于spring的产品: 我们可以看到下面的一段介绍： Takes an opinionated view of building production-ready Spring applications. Spring Boot favors convention over configuration and is designed to get you up and running as quickly as possible. 翻译一下： 用一些固定的方式来构建生产级别的spring应用。Spring Boot 推崇约定大于配置的方式以便于你能够尽可能快速的启动并运行程序。 其实人们把Spring Boot 称为搭建程序的脚手架。其最主要作用就是帮我们快速的构建庞大的spring项目，并且尽可能的减少一切xml配置，做到开箱即用，迅速上手，让我们关注与业务而非配置。 1.2.为什么要学习SpringBootjava一直被人诟病的一点就是臃肿、麻烦。当我们还在辛苦的搭建项目时，可能Python程序员已经把功能写好了，究其原因注意是两点： 复杂的配置， 项目各种配置其实是开发时的损耗， 因为在思考 Spring 特性配置和解决业务问题之间需要进行思维切换，所以写配置挤占了写应用程序逻辑的时间。 一个是混乱的依赖管理。 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了，你还要知道这些库的哪个版本和其他库不会有冲突，这难题实在太棘手。并且，依赖管理也是一种损耗，添加依赖不是写应用程序代码。一旦选错了依赖的版本，随之而来的不兼容问题毫无疑问会是生产力杀手。 而SpringBoot让这一切成为过去！ Spring Boot 简化了基于Spring的应用开发，只需要“run”就能创建一个独立的、生产级别的Spring应用。Spring Boot为Spring平台及第三方库提供开箱即用的设置（提供默认设置，存放默认配置的包就是启动器），这样我们就可以简单的开始。多数Spring Boot应用只需要很少的Spring配置。 我们可以使用SpringBoot创建java应用，并使用java –jar 启动它，就能得到一个生产级别的web工程。 1.3.SpringBoot的特点Spring Boot 主要目标是： 为所有 Spring 的开发者提供一个非常快速的、广泛接受的入门体验 开箱即用（启动器starter-其实就是SpringBoot提供的一个jar包），但通过自己设置参数（.properties），即可快速摆脱这种方式。 提供了一些大型项目中常见的非功能性特性，如内嵌服务器、安全、指标，健康检测、外部化配置等 绝对没有代码生成，也无需 XML 配置。 更多细节，大家可以到官网查看。 2.快速入门接下来，我们就来利用SpringBoot搭建一个web工程，体会一下SpringBoot的魅力所在！ 2.1.创建工程我们先新建一个空的工程： 工程名为demo： 新建一个model： 使用maven来构建： 然后填写项目坐标： 目录结构： 项目结构： 2.2.添加依赖看到这里很多同学会有疑惑，前面说传统开发的问题之一就是依赖管理混乱，怎么这里我们还需要管理依赖呢？难道SpringBoot不帮我们管理吗？ 别着急，现在我们的项目与SpringBoot还没有什么关联。SpringBoot提供了一个名为spring-boot-starter-parent的工程，里面已经对各种常用依赖（并非全部）的版本进行了管理，我们的项目需要以这个项目为父工程，这样我们就不用操心依赖的版本问题了，需要什么依赖，直接引入坐标即可！ 2.2.1.添加父工程坐标12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;&lt;/parent&gt; 2.2.2.添加web启动器为了让SpringBoot帮我们完成各种自动配置，我们必须引入SpringBoot提供的自动配置依赖，我们称为启动器。因为我们是web项目，这里我们引入web启动器： 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 需要注意的是，我们并没有在这里指定版本信息。因为SpringBoot的父工程已经对版本进行了管理了。 这个时候，我们会发现项目中多出了大量的依赖： 这些都是SpringBoot根据spring-boot-starter-web这个依赖自动引入的，而且所有的版本都已经管理好，不会出现冲突。 2.2.3.管理jdk版本默认情况下，maven工程的jdk版本是1.5，而我们开发使用的是1.8，因此这里我们需要修改jdk版本，只需要简单的添加以下属性即可： 123&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt; 2.2.4.完整pom123456789101112131415161718192021222324252627&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.leyou.demo&lt;/groupId&gt; &lt;artifactId&gt;springboot-demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.3.启动类Spring Boot项目通过main函数即可启动，我们需要创建一个启动类： 然后编写main函数： 123456@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 2.4.编写controller接下来，我们就可以像以前那样开发SpringMVC的项目了！ 我们编写一个controller： 代码： 12345678@RestControllerpublic class HelloController &#123; @GetMapping(\"hello\") public String hello()&#123; return \"hello, spring boot!\"; &#125;&#125; 2.5.启动测试接下来，我们运行main函数，查看控制台： 并且可以看到监听的端口信息： 1）监听的端口是8080 2）SpringMVC的映射路径是：/ 3）/hello路径已经映射到了HelloController中的hello()方法 打开页面访问：http://localhost:8080/hello 测试成功了！ 3.Java配置在入门案例中，我们没有任何的配置，就可以实现一个SpringMVC的项目了，快速、高效！ 但是有同学会有疑问，如果没有任何的xml，那么我们如果要配置一个Bean该怎么办？比如我们要配置一个数据库连接池，以前会这么玩： 1234567&lt;!-- 配置连接池 --&gt;&lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" init-method=\"init\" destroy-method=\"close\"&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\" /&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\" /&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\" /&gt;&lt;/bean&gt; 现在该怎么做呢？ 3.1.回顾历史事实上，在Spring3.0开始，Spring官方就已经开始推荐使用java配置来代替传统的xml配置了，我们不妨来回顾一下Spring的历史： Spring1.0时代 在此时因为jdk1.5刚刚出来，注解开发并未盛行，因此一切Spring配置都是xml格式，想象一下所有的bean都用xml配置，细思极恐啊，心疼那个时候的程序员2秒 Spring2.0时代 Spring引入了注解开发，但是因为并不完善，因此并未完全替代xml，此时的程序员往往是把xml与注解进行结合，貌似我们之前都是这种方式。 Spring3.0及以后 3.0以后Spring的注解已经非常完善了，因此Spring推荐大家使用完全的java配置来代替以前的xml，不过似乎在国内并未推广盛行。然后当SpringBoot来临，人们才慢慢认识到java配置的优雅。 有句古话说的好：拥抱变化，拥抱未来。所以我们也应该顺应时代潮流，做时尚的弄潮儿，一起来学习下java配置的玩法。 3.2.尝试java配置java配置主要靠java类和一些注解，比较常用的注解有： @Configuration：声明一个类作为配置类，代替xml文件 @Bean：声明在方法上，将方法的返回值加入Bean容器，代替&lt;bean&gt;标签 @value：属性注入 @PropertySource：指定外部属性文件， 我们接下来用java配置来尝试实现连接池配置： 首先引入Druid连接池依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 创建一个jdbc.properties文件，编写jdbc属性： 1234jdbc.driverClassName=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/leyoujdbc.username=rootjdbc.password=123 然后编写代码： 1234567891011121314151617181920212223@Configuration@PropertySource(\"classpath:jdbc.properties\")public class JdbcConfig &#123; @Value(\"$&#123;jdbc.url&#125;\") String url; @Value(\"$&#123;jdbc.driverClassName&#125;\") String driverClassName; @Value(\"$&#123;jdbc.username&#125;\") String username; @Value(\"$&#123;jdbc.password&#125;\") String password; @Bean public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setDriverClassName(driverClassName); dataSource.setUsername(username); dataSource.setPassword(password); return dataSource; &#125;&#125; 解读： @Configuration：声明我们JdbcConfig是一个配置类 @PropertySource：指定属性文件的路径是:classpath:jdbc.properties 通过@Value为属性注入值 通过@Bean将 dataSource()方法声明为一个注册Bean的方法，Spring会自动调用该方法，将方法的返回值加入Spring容器中。 然后我们就可以在任意位置通过@Autowired注入DataSource了！ 我们在HelloController中测试： 1234567891011@RestControllerpublic class HelloController &#123; @Autowired private DataSource dataSource; @GetMapping(\"hello\") public String hello() &#123; return \"hello, spring boot!\" + dataSource; &#125;&#125; 然后Debug运行并查看： 属性注入成功了！ 3.3.SpringBoot的属性注入在上面的案例中，我们实验了java配置方式。不过属性注入使用的是@Value注解。这种方式虽然可行，但是不够强大，因为它只能注入基本类型值。 在SpringBoot中，提供了一种新的属性注入方式，支持各种java基本数据类型及复杂类型的注入。 1）我们新建一个类，用来进行属性注入： 123456789@ConfigurationProperties(prefix = \"jdbc\")public class JdbcProperties &#123; private String url; private String driverClassName; private String username; private String password; // ... 略 // getters 和 setters&#125; 在类上通过@ConfigurationProperties注解声明当前类为属性读取类 prefix=&quot;jdbc&quot;读取属性文件中，前缀为jdbc的值。 在类上定义各个属性，名称必须与属性文件中jdbc.后面部分一致 需要注意的是，这里我们并没有指定属性文件的地址，所以我们需要把jdbc.properties名称改为application.properties，这是SpringBoot默认读取的属性文件名： 2）在JdbcConfig中使用这个属性： 1234567891011121314@Configuration@EnableConfigurationProperties(JdbcProperties.class)public class JdbcConfig &#123; @Bean public DataSource dataSource(JdbcProperties jdbc) &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(jdbc.getUrl()); dataSource.setDriverClassName(jdbc.getDriverClassName()); dataSource.setUsername(jdbc.getUsername()); dataSource.setPassword(jdbc.getPassword()); return dataSource; &#125;&#125; 通过@EnableConfigurationProperties(JdbcProperties.class)来声明要使用JdbcProperties这个类的对象 然后你可以通过以下方式注入JdbcProperties： @Autowired注入 12@Autowiredprivate JdbcProperties prop; 构造函数注入 1234private JdbcProperties prop;public JdbcConfig(Jdbcproperties prop)&#123; this.prop = prop;&#125; 声明有@Bean的方法参数注入 1234@Beanpublic Datasource dataSource(JdbcProperties prop)&#123; // ...&#125; 本例中，我们采用第三种方式。 3）测试结果： 大家会觉得这种方式似乎更麻烦了，事实上这种方式有更强大的功能，也是SpringBoot推荐的注入方式。两者对比关系： 优势： Relaxed binding：松散绑定 不严格要求属性文件中的属性名与成员变量名一致。支持驼峰，中划线，下划线等等转换，甚至支持对象引导。比如：user.friend.name：代表的是user对象中的friend属性中的name属性，显然friend也是对象。@value注解就难以完成这样的注入方式。 meta-data support：元数据支持，帮助IDE生成属性提示（写开源框架会用到）。 ​ 3.4.更优雅的注入事实上，如果一段属性只有一个Bean需要使用，我们无需将其注入到一个类（JdbcProperties）中。而是直接在需要的地方声明即可： 1234567891011@Configurationpublic class JdbcConfig &#123; @Bean // 声明要注入的属性前缀，SpringBoot会自动把相关属性通过set方法注入到DataSource中 @ConfigurationProperties(prefix = \"jdbc\") public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); return dataSource; &#125;&#125; 我们直接把@ConfigurationProperties(prefix = &quot;jdbc&quot;)声明在需要使用的@Bean的方法上，然后SpringBoot就会自动调用这个Bean（此处是DataSource）的set方法，然后完成注入。使用的前提是：该类必须有对应属性的set方法！ 我们将jdbc的url改成：/heima，再次测试： 4.自动配置原理使用SpringBoot之后，一个整合了SpringMVC的WEB工程开发，变的无比简单，那些繁杂的配置都消失不见了，这是如何做到的？ 一切魔力的开始，都是从我们的main函数来的，所以我们再次来看下启动类： 我们发现特别的地方有两个： 注解：@SpringBootApplication run方法：SpringApplication.run() 我们分别来研究这两个部分。 4.1.了解@SpringBootApplication点击进入，查看源码： 这里重点的注解有3个： @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 4.1.1.@SpringBootConfiguration我们继续点击查看源码： 通过这段我们可以看出，在这个注解上面，又有一个@Configuration注解。通过上面的注释阅读我们知道：这个注解的作用就是声明当前类是一个配置类，然后Spring会自动扫描到添加了@Configuration的类，并且读取其中的配置信息。而@SpringBootConfiguration是来声明当前类是SpringBoot应用的配置类，项目中只能有一个。所以一般我们无需自己添加。 4.1.2.@EnableAutoConfiguration关于这个注解，官网上有一段说明： The second class-level annotation is @EnableAutoConfiguration. This annotationtells Spring Boot to “guess” how you want to configure Spring, based on the jardependencies that you have added. Since spring-boot-starter-web added Tomcatand Spring MVC, the auto-configuration assumes that you are developing a webapplication and sets up Spring accordingly. 简单翻译以下： 第二级的注解@EnableAutoConfiguration，告诉SpringBoot基于你所添加的依赖，去“猜测”你想要如何配置Spring。比如我们引入了spring-boot-starter-web，而这个启动器中帮我们添加了tomcat、SpringMVC的依赖。此时自动配置就知道你是要开发一个web应用，所以就帮你完成了web及SpringMVC的默认配置了！ 总结，SpringBoot内部对大量的第三方库或Spring内部库进行了默认配置，这些配置是否生效，取决于我们是否引入了对应库所需的依赖，如果有那么默认配置就会生效。 所以，我们使用SpringBoot构建一个项目，只需要引入所需框架的依赖，配置就可以交给SpringBoot处理了。除非你不希望使用SpringBoot的默认配置，它也提供了自定义配置的入口。 4.1.3.@ComponentScan我们跟进源码： 并没有看到什么特殊的地方。我们查看注释： 大概的意思： 配置组件扫描的指令。提供了类似与&lt;context:component-scan&gt;标签的作用 通过basePackageClasses或者basePackages属性来指定要扫描的包。如果没有指定这些属性，那么将从声明这个注解的类所在的包开始，扫描包及子包 而我们的@SpringBootApplication注解声明的类就是main函数所在的启动类，因此扫描的包是该类所在包及其子包。因此，一般启动类会放在一个比较前的包目录中。 4.2.默认配置原理4.2.1默认配置类通过刚才的学习，我们知道@EnableAutoConfiguration会开启SpringBoot的自动配置，并且根据你引入的依赖来生效对应的默认配置。那么问题来了： 这些默认配置是在哪里定义的呢？ 为何依赖引入就会触发配置呢？ 其实在我们的项目中，已经引入了一个依赖：spring-boot-autoconfigure，其中定义了大量自动配置类： 还有： 非常多，几乎涵盖了现在主流的开源框架，例如： redis jms amqp jdbc jackson mongodb jpa solr elasticsearch … 等等 我们来看一个我们熟悉的，例如SpringMVC，查看mvc 的自动配置类： 打开WebMvcAutoConfiguration： 我们看到这个类上的4个注解： @Configuration：声明这个类是一个配置类 @ConditionalOnWebApplication(type = Type.SERVLET) ConditionalOn，翻译就是在某个条件下，此处就是满足项目的类是是Type.SERVLET类型，也就是一个普通web工程，显然我们就是 @ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class }) 这里的条件是OnClass，也就是满足以下类存在：Servlet、DispatcherServlet、WebMvcConfigurer，其中Servlet只要引入了tomcat依赖自然会有，后两个需要引入SpringMVC才会有。这里就是判断你是否引入了相关依赖，引入依赖后该条件成立，当前类的配置才会生效！ @ConditionalOnMissingBean(WebMvcConfigurationSupport.class) 这个条件与上面不同，OnMissingBean，是说环境中没有指定的Bean这个才生效。其实这就是自定义配置的入口，也就是说，如果我们自己配置了一个WebMVCConfigurationSupport的类，那么这个默认配置就会失效！ 接着，我们查看该类中定义了什么： 视图解析器： 处理器适配器（HandlerAdapter）： 还有很多，这里就不一一截图了。 4.2.2.默认配置属性另外，这些默认配置的属性来自哪里呢？ 我们看到，这里通过@EnableAutoConfiguration注解引入了两个属性：WebMvcProperties和ResourceProperties。这不正是SpringBoot的属性注入玩法嘛。 我们查看这两个属性类： 找到了内部资源视图解析器的prefix和suffix属性。 ResourceProperties中主要定义了静态资源（.js,.html,.css等)的路径： 如果我们要覆盖这些默认属性，只需要在application.properties中定义与其前缀prefix和字段名一致的属性即可。 4.3.总结SpringBoot为我们提供了默认配置，而默认配置生效的条件一般有两个： 你引入了相关依赖 你自己没有配置Bean 1）启动器 所以，我们如果不想配置，只需要引入依赖即可，而依赖版本我们也不用操心，因为只要引入了SpringBoot提供的stater（启动器），就会自动管理依赖及版本了。 因此，玩SpringBoot的第一件事情，就是找启动器，SpringBoot提供了大量的默认启动器，参考课前资料中提供的《SpringBoot启动器.txt》 2）全局配置 另外，SpringBoot的默认配置，都会读取默认属性，而这些属性可以通过自定义application.properties文件来进行覆盖。这样虽然使用的还是默认配置，但是配置中的值改成了我们自定义的。 因此，玩SpringBoot的第二件事情，就是通过application.properties来覆盖默认属性值，形成自定义配置。我们需要知道SpringBoot的默认属性key，非常多，参考课前资料提供的：《SpringBoot全局属性.md》 属性文件支持两种格式，application.properties和application.yml yml的语法实例： 12345678jdbc: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/leyou username: root password: 123server: port: 80 5.SpringBoot实践接下来，我们来看看如何用SpringBoot来玩转以前的SSM,我们沿用之前讲解SSM用到的数据库tb_user和实体类User 5.1.整合SpringMVC虽然默认配置已经可以使用SpringMVC了，不过我们有时候需要进行自定义配置。 5.1.1.修改端口查看SpringBoot的全局属性可知，端口通过以下方式配置： 12# 映射端口server.port=80 重启服务后测试： 5.1.2.访问静态资源现在，我们的项目是一个jar工程，那么就没有webapp，我们的静态资源该放哪里呢？ 回顾我们上面看的源码，有一个叫做ResourceProperties的类，里面就定义了静态资源的默认查找路径： 默认的静态资源路径为： classpath:/META-INF/resources/ classpath:/resources/ classpath:/static/ classpath:/public 只要静态资源放在这些目录中任何一个，SpringMVC都会帮我们处理。 我们习惯会把静态资源放在classpath:/static/目录下。我们创建目录，并且添加一些静态资源： 重启项目后测试 5.1.3.添加拦截器拦截器也是我们经常需要使用的，在SpringBoot中该如何配置呢？ 拦截器不是一个普通属性，而是一个类，所以就要用到java配置方式了。在SpringBoot官方文档中有这么一段说明： If you want to keep Spring Boot MVC features and you want to add additional MVC configuration (interceptors, formatters, view controllers, and other features), you can add your own @Configuration class of type WebMvcConfigurer but without @EnableWebMvc. If you wish to provide custom instances of RequestMappingHandlerMapping, RequestMappingHandlerAdapter, or ExceptionHandlerExceptionResolver, you can declare a WebMvcRegistrationsAdapter instance to provide such components. If you want to take complete control of Spring MVC, you can add your own @Configuration annotated with @EnableWebMvc. 翻译： 如果你想要保持Spring Boot 的一些默认MVC特征，同时又想自定义一些MVC配置（包括：拦截器，格式化器, 视图控制器、消息转换器 等等），你应该让一个类实现WebMvcConfigurer，并且添加@Configuration注解，但是千万不要加@EnableWebMvc注解。如果你想要自定义HandlerMapping、HandlerAdapter、ExceptionResolver等组件，你可以创建一个WebMvcRegistrationsAdapter实例 来提供以上组件。 如果你想要完全自定义SpringMVC，不保留SpringBoot提供的一切特征，你可以自己定义类并且添加@Configuration注解和@EnableWebMvc注解 总结：通过实现WebMvcConfigurer并添加@Configuration注解来实现自定义部分SpringMvc配置。 首先我们定义一个拦截器： 12345678910111213141516171819public class LoginInterceptor implements HandlerInterceptor &#123; private Logger logger = LoggerFactory.getLogger(LoginInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; logger.debug(\"preHandle method is now running!\"); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) &#123; logger.debug(\"postHandle method is now running!\"); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; logger.debug(\"afterCompletion method is now running!\"); &#125;&#125; 然后，我们定义配置类，注册拦截器： 123456789101112131415161718192021@Configurationpublic class MvcConfig implements WebMvcConfigurer&#123; /** * 通过@Bean注解，将我们定义的拦截器注册到Spring容器 * @return */ @Bean public LoginInterceptor loginInterceptor()&#123; return new LoginInterceptor(); &#125; /** * 重写接口中的addInterceptors方法，添加自定义拦截器 * @param registry */ @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 通过registry来注册拦截器，通过addPathPatterns来添加拦截路径 registry.addInterceptor(this.loginInterceptor()).addPathPatterns(\"/**\"); &#125;&#125; 结构如下： 接下来运行并查看日志： 你会发现日志中什么都没有，因为我们记录的log级别是debug，默认是显示info以上，我们需要进行配置。 SpringBoot通过logging.level.*=debug来配置日志级别，*填写包名 12# 设置com.leyou包的日志级别为debuglogging.level.com.leyou=debug 再次运行查看： 1232018-05-05 17:50:01.811 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : preHandle method is now running!2018-05-05 17:50:01.854 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : postHandle method is now running!2018-05-05 17:50:01.854 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : afterCompletion method is now running! 5.2.整合jdbc和事务spring中的jdbc连接和事务是配置中的重要一环，在SpringBoot中该如何处理呢？ 答案是不需要处理，我们只要找到SpringBoot提供的启动器即可： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt; 当然，不要忘了数据库驱动，SpringBoot并不知道我们用的什么数据库，这里我们选择MySQL： 1234&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 至于事务，SpringBoot中通过注解来控制。就是我们熟知的@Transactional 123456789101112131415@Servicepublic class UserService &#123; @Autowired private UserMapper userMapper; public User queryById(Long id)&#123; return this.userMapper.selectByPrimaryKey(id); &#125; @Transactional public void deleteById(Long id)&#123; this.userMapper.deleteByPrimaryKey(id); &#125;&#125; 5.3.整合连接池其实，在刚才引入jdbc启动器的时候，SpringBoot已经自动帮我们引入了一个连接池：HikariCP应该是目前速度最快的连接池了，我们看看它与c3p0的对比： 因此，我们只需要指定连接池参数即可： 123456spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/heima30 username: root password: 123 当然，如果你更喜欢Druid连接池，也可以使用Druid官方提供的启动器： 123456&lt;!-- Druid连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 而连接信息的配置与上面是类似的，只不过在连接池特有属性上，方式略有不同： 12345678910#初始化连接数spring.datasource.druid.initial-size=1#最小空闲连接spring.datasource.druid.min-idle=1#最大活动连接spring.datasource.druid.max-active=20#获取连接时测试是否可用spring.datasource.druid.test-on-borrow=true#监控页面启动spring.datasource.druid.stat-view-servlet.allow=true 5.4.整合mybatis5.4.1.mybatisSpringBoot官方并没有提供Mybatis的启动器，不过Mybatis官网自己实现了： 123456&lt;!--mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 配置，基本没有需要配置的： 1234# mybatis 别名扫描mybatis.type-aliases-package=com.heima.pojo# mapper.xml文件位置,如果没有映射文件，请注释掉mybatis.mapper-locations=classpath:mappers/*.xml 需要注意，这里没有配置mapper接口扫描包，因此我们需要给每一个Mapper接口添加@Mapper注解，才能被识别。 123@Mapperpublic interface UserMapper &#123;&#125; 或者，我们也可以不加注解，而是在启动类上添加扫描包注解： 12345678@SpringBootApplication@MapperScan(\"cn.itcast.demo.mapper\")public class Application &#123; public static void main(String[] args) &#123; // 启动代码 SpringApplication.run(Application.class, args); &#125;&#125; 5.4.2.通用mapper通用Mapper的作者也为自己的插件编写了启动器，我们直接引入即可： 123456&lt;!-- 通用mapper --&gt;&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt;&lt;/dependency&gt; 不需要做任何配置就可以使用了。 123@Mapperpublic interface UserMapper extends tk.mybatis.mapper.common.Mapper&lt;User&gt;&#123;&#125; 5.5.启动测试将controller进行简单改造： 123456789101112@RestControllerpublic class HelloController &#123; @Autowired private UserService userService; @GetMapping(\"/hello\") public User hello() &#123; User user = this.userService.queryById(8L); return user; &#125;&#125; 我们启动项目，查看： 6.JDK1.8参考课前资料：JDK1.8新特性.md","categories":[{"name":"随堂笔记","slug":"随堂笔记","permalink":"https://zem12345678.github.io/categories/随堂笔记/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://zem12345678.github.io/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zem12345678.github.io/tags/SpringBoot/"},{"name":"Spring","slug":"Spring","permalink":"https://zem12345678.github.io/tags/Spring/"}]},{"title":"RabbitMQ简介","slug":"RabbitMQ简介","date":"2018-10-19T03:29:16.775Z","updated":"2018-10-19T04:28:44.379Z","comments":true,"path":"2018/10/19/RabbitMQ简介/","link":"","permalink":"https://zem12345678.github.io/2018/10/19/RabbitMQ简介/","excerpt":"","text":"RabbitMQ是一个由erlang开发的AMQP（Advanced Message Queue ）的开源实现。AMQP 的出现其实也是应了广大人民群众的需求，虽然在同步消息通讯的世界里有很多公开标准（如 COBAR的 IIOP ，或者是 SOAP 等），但是在异步消息处理中却不是这样，只有大企业有一些商业实现（如微软的 MSMQ ，IBM 的 Websphere MQ 等），因此，在 2006 年的 6 月，Cisco 、Redhat、iMatix 等联合制定了 AMQP 的公开标准。 RabbitMQ是由RabbitMQ Technologies Ltd开发并且提供商业支持的。该公司在2010年4月被SpringSource（VMWare的一个部门）收购。在2013年5月被并入Pivotal。其实VMWare，Pivotal和EMC本质上是一家的。不同的是VMWare是独立上市子公司，而Pivotal是整合了EMC的某些资源，现在并没有上市。 RabbitMQ的官网是http://www.rabbitmq.comRabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括： 什么叫消息队列(MQ)消息（Message）是指在应用间传送的数据。消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。 消息队列（Message Queue）是一种应用程序对应用程序的通信方法，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。通过消息队列通信，让A，B两个服务指间保持低耦合，实现业务的灵活拓展。 为何用消息队列(MQ)从上面的描述中可以看出消息队列是一种应用间的异步协作机制，那什么时候需要使用 MQ 呢？ 以常见的订单系统为例，用户点击【下单】按钮之后的业务逻辑可能包括：扣减库存、生成相应单据、发红包、发短信通知。在业务发展初期这些逻辑可能放在一起同步执行，随着业务的发展订单量增长，需要提升系统服务的性能，这时可以将一些不需要立即生效的操作拆分出来异步执行，比如发放红包、发短信通知等。这种场景下就可以用 MQ ，在下单的主流程（比如扣减库存、生成相应单据）完成之后发送一条消息到 MQ 让主流程快速完结，而由另外的单独线程拉取MQ的消息（或者由 MQ 推送消息），当发现 MQ 中有发红包或发短信之类的消息时，执行相应的业务逻辑。 以上是用于业务解耦的情况，其它常见场景包括最终一致性、广播、错峰流控等等。 RabbitMQ 特点 可靠性（Reliability）RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。 灵活的路由（Flexible Routing）在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。 消息集群（Clustering）多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。 高可用（Highly Available Queues）队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。 多种协议（Multi-protocol）RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。 多语言客户端（Many Clients）RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby ,python,等等。 管理界面（Management UI）RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。 跟踪机制（Tracing）如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。 插件机制（Plugin System）RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。 RabbitMQ 中的概念模型消息模型所有 MQ 产品从模型抽象上来说都是一样的过程：消费者（consumer）订阅某个队列。生产者（producer）创建消息，然后发布到队列（queue）中，最后将消息发送到监听的消费者。 RabbitMQ 基本概念上面只是最简单抽象的描述，具体到 RabbitMQ 则有更详细的概念需要解释。上面介绍过 RabbitMQ 是 AMQP 协议的一个开源实现，所以其内部实际上也是 AMQP 中的基本概念： Message消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher消息的生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。 Binding绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Queue消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Connection网络连接，比如一个TCP连接。 Channel信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker表示消息队列服务器实体。AMQP 中的消息路由AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。 Exchange 类型Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： direct 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routing key 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配、单播的模式。 fanout 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 topic topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“”。#匹配0个或多个单词，匹配不多不少一个单词。","categories":[{"name":"异步处理","slug":"异步处理","permalink":"https://zem12345678.github.io/categories/异步处理/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://zem12345678.github.io/tags/分布式/"},{"name":"消息队列","slug":"消息队列","permalink":"https://zem12345678.github.io/tags/消息队列/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://zem12345678.github.io/tags/RabbitMQ/"}]},{"title":"Hadoop、Storm、Spark","slug":"Hadoop、Storm、Spark","date":"2018-10-18T07:28:29.350Z","updated":"2018-10-18T07:32:03.219Z","comments":true,"path":"2018/10/18/Hadoop、Storm、Spark/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/Hadoop、Storm、Spark/","excerpt":"","text":"Storm与Spark、Hadoop这三种框架，各有各的优点，每个框架都有自己的最佳应用场景。所以，在不同的应用场景下，应该选择不同的框架。 StormStorm是最佳的流式计算框架，Storm由Java和Clojure写成，Storm的优点是全内存计算，所以它的定位是分布式实时计算系统，按照Storm作者的说法，Storm对于实时计算的意义类似于Hadoop对于批处理的意义。 Storm的适用场景：1）流数据处理Storm可以用来处理源源不断流进来的消息，处理之后将结果写入到某个存储中去。2）分布式RPC。由于Storm的处理组件是分布式的，而且处理延迟极低，所以可以作为一个通用的分布式RPC框架来使用。 sparkSparkSpark是一个基于内存计算的开源集群计算系统，目的是更快速的进行数据分析。Spark由加州伯克利大学AMP实验室Matei为主的小团队使用Scala开发开发，类似于Hadoop MapReduce的通用并行计算框架，Spark基于Map Reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的Map Reduce的算法。 Spark的适用场景：1）多次操作特定数据集的应用场合Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。2）粗粒度更新状态的应用由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如Web服务的存储或者是增量的Web爬虫和索引。就是对于那种增量修改的应用模型不适合。总的来说Spark的适用面比较广泛且比较通用。 hadoopHadoop是实现了MapReduce的思想，将数据切片计算来处理大量的离线数据数据。Hadoop处理的数据必须是已经存放在HDFS上或者类似HBase的数据库中，所以Hadoop实现的时候是通过移动计算到这些存放数据的机器上来提高效率。 Hadoop的适用场景：1）海量数据的离线分析处理2）大规模Web信息搜索3）数据密集型并行计算 简单来说：Hadoop适合于离线的批量数据处理适用于对实时性要求极低的场景Storm适合于实时流数据处理，实时性方面做得极好Spark是内存分布式计算框架，试图吞并Hadoop的Map-Reduce批处理框架和Storm的流处理框架，但是Spark已经做得很不错了，批处理方面性能优于Map-Reduce，但是流处理目前还是弱于Storm，产品仍在改进之中","categories":[{"name":"大数据","slug":"大数据","permalink":"https://zem12345678.github.io/categories/大数据/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://zem12345678.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://zem12345678.github.io/tags/Hadoop/"}]},{"title":"python基础小谈","slug":"python基础小谈","date":"2018-10-18T06:55:13.351Z","updated":"2018-10-18T07:07:25.385Z","comments":true,"path":"2018/10/18/python基础小谈/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/python基础小谈/","excerpt":"","text":"python语音是动态解释类型的，被称为胶水语言，再python的底层函数我们会经常看到两个形参*args,**kwargs，那么它们的本质是什么，什么使用它们呢？ 一 .*args 和 **kwargs 是什么？*args本质是一个tuple（元组），**kwargs本质是一个dict（字典）。 二.怎么用 *args 和 **kwargs?def my_fun(*args, **kwargs ): print (‘args = ‘, args) print (‘kwargs = ‘, kwargs) 调用就比较有意思了，传统的比如，c, c++, Java, C#，基本都是一对一传参，但是python靠这两个参数，可以实现多参的灵活传入。如下所示，我完全可以这么调用： my_fun(1,3,5,9, a=2, b=4) 这样打印的结果： args = (1,3,5,9) # 是一个元组 kwargs = { ‘a’: 2 , ‘b’:4 } #是一个字典 注意事项：上述函数 my_fun，如果这么调用就会有问题： my_fun( a=2, b=4, 1,3,5,9 ) 报错：SyntaxError: non-keyword arg after keyword arg” 意思是：关键字参数后面不能有非关键字参数，言外之意，关键字参数 * kwargs 必须位于 args 之后！","categories":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"}]},{"title":"Thinking In Python Language","slug":"Thinking In Python Language","date":"2018-10-18T05:54:54.070Z","updated":"2018-11-14T03:10:47.353Z","comments":true,"path":"2018/10/18/Thinking In Python Language/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/Thinking In Python Language/","excerpt":"","text":"1.前言本文诞生于利用 Topic Reading 方法读 Python 若干本技术书籍这个过程中结合自己的开发常见场景记录下来的一些笔记。 2.简介1. 为什么是 Python Python, 很大程度上是因为 Python 的快速开发。 当然，快速开发（这里的开发包含部署）这个词也往往会被误解。什么叫做快速？我用一个 CMS 框架快速搭建出一个网站这是否叫做快速？ 每一次部署的时候，如果使用 Java 或者是 Go, 部署的时候直接 maven 编译打包，接着把 War 包直接上传到 Tomcat 就结束了。而用 Python 则需要各种虚拟环境，各种稀里哗啦的配置。这种情况下是哪一种快速呢？Python 有什么好处呢？ 写代码效率高。 生态圈好。 写代码效率高，这指的是写 Python 代码，而不是运行时。3.写代码效率高，这指的是写 Python 代码，而不是运行时。 生态圈好，Web 开发用 Django/Flask , 数据抓取用 Requests , 数据分析清洗用 Pandas, 机器学习。 2. 工具链 Anaconda工具：https://www.anaconda.com/download/ 3. 文档 官方文档：https://docs.python.org/3/ 4. 社区 官方社区：https://www.python.org/community/ 4. 书籍 《python核心编程》，《python编程从入门到实践》 3. 基本概念 程序 = 算法 + 数据结构 这句话当然是不全面的，但并不影响这句话在计算机世界里面的地位。依我看来，对我的启发大致是：我会把 API 的调用和数据结构以及算法想清楚，然后才动手把代码分解成伪代码。 1.数据类型数据类型按照不同的划分标准可以进行不同的划分： 按照复杂性可以这么划分： 简单类型 复杂类型】 按照复杂性可以这么划分： 基本类型 引用类型 按照数据结构可以这么划分： 集合结构 : 串 线性结构 : 线性表 （单链表，静态链表，循环链表，双向链表，栈，队列) 树形结构 : 树（二叉树，B+ 树，红黑树） 图形结构 : 图 2. 操作对于一些基本的数据类型，操作为 加减乘除取余数位运算等等 对于复杂的一些数据类型，则需要对数据结构多一些了解。 比如，对队列而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对 hash 而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对字典而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对字符串而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？ 那字符串来说，Java 推荐使用 StringBuilder 来合并多个字符串，Python 推荐 join 多个字符串等等。 4.1.函数2.作用域3.模块模块，这个概念，可大可小，大的时候，把一个程序说成是模块，小的时候，可以把一个文件，甚至你说这一个函数是一个模块，也行。 这里的模块指的是一个包下的函数。 4.面向对象面向对象有三大概念： 封装 继承 多态5.错误 / 调试测试异常处理实际上可以考验一个程序员编写代码的健壮性。 事实上来说，代码写的健壮是一个程序员必备的素养。但其实在开发过程中，出于对项目进行赶工上线，需要对程序的健壮性做出一定的取舍。并且，在编写客户端，服务端，网页前端的时候基本上都会遇到这个问题。什么时候选择健壮的程序，什么时候选择是还可以的程序。需要自己的经验。 6. IO 编程7.进程和线程1.多线程 Python 多线程约等于并发。 2.多进程3.GILGlobal Interpreter Lock 并不是所有的解释器语言都有 GIL （尽管 Python 和 Ruby 里面都有）, 也并不是没有尝试过去除 GIL, 但是每次去除都会导致单线程性能的下降。所以暂时保留。 GIL 对程序中的影响： 一个线程运行 Python , 而其他 N 个睡眠或者等待 I/O - 同一时刻只有一个线程对共享资源进行存取 , Python 线程也可以等待 threading.Lock 或者线程模块中的其他同步对象； 协同式多任务处理如果有两个线程，同时进行 IO 请求，当其中一个线程连接之后，立即会主动让出 GIL, 其他线程就可以运行。 当N 个线程在网络 I/O 堵塞，或等待重新获取 GIL，而一个线程运行 Python。 让出之后还要执行代码呀，所以要有个收回 GIL 的动作。 抢占式多任务处理Python 2 GIL , 尝试收回 GIL 为 执行 1000 字节码。Python 3 GIL , 尝试收回 GIL 检测间隔为 15ms 线程安全原子操作：sort 之类不需要非原子操作：n=n+2 的字节码分为 加载 n , 加载 2 , 相加，存储 n, 四个步骤，由于不是原子性，很可能被由于 15 ms 而被打断。 当然，懒人一向是 : 优先级不决加括号，线程不决加 lock 对于 Java, 程序员努力在尽可能短的时间内加锁存取共享数据，减轻线程的争夺，实现最大并行。但 Python 中，线程无法并行运行，细粒度的锁就没有了优势。 8.正则表达式5.高级技巧6.标准库常用内建模块系统化模块IntroductionBuilt-in FunctionsBuilt-in ConstantsBuilt-in TypesBuilt-in ExceptionsText Processing ServicesBinary Data ServicesData TypesNumeric and Mathematical ModulesFunctional Programming ModulesFile and Directory AccessData PersistenceData Compression and ArchivingFile FormatsCryptographic ServicesGeneric Operating System ServicesConcurrent ExecutionInterprocess Communication and NetworkingInternet Data HandlingStructured Markup Processing ToolsInternet Protocols and SupportMultimedia ServicesInternationalizationProgram FrameworksGraphical User Interfaces with TkDevelopment ToolsDebugging and ProfilingSoftware Packaging and DistributionPython Runtime ServicesCustom Python InterpretersImporting ModulesPython Language ServicesMiscellaneous ServicesMS Windows Specific ServicesUnix Specific ServicesSuperseded ModulesUndocumented Modules 7.第三方库Requests : API 人性化 8.代码质量1.正确性 外部不该引用 protected member （单下划线）lambda 为一次使用，最好不要赋值。不要给 buildin 函数赋值py3 直接 super()for in else 如果不内置 break 则出会在最后 for in 为 empty 的时候再执行 else 中的语句context exit 如果不 catch 掉异常让其自然向上一级抛出错误的话，必须为 (self, exception_type, exception_value, traceback):不要在 init 里面 return 数据不要混用 tab 和 space4 个 space 缩进staticmethod 直接是 参数，classmethod 第一个参数为 cls可变的 default value 是不能作为 参数的。（可能是解释器在确定函数的定义的时候完成赋值？)遵循 exception hierachy https://docs.python.org/3/library/exceptions.html#exception-hierarchydefaultdict defaultdict(lambda : 6) , 必须 callable尽量 unpack 赋值字典用获取用 get(“myk”,None) , 赋值用 dictionary.setdefault(“list”, []).append(“list_item”) 2.可维护性 避免使用 import * , 我觉得这点值得商榷 , 如果是某个模块下，完全可以先把模块拆分成多个，最后 import 进来，接着使用 all.getxxx 获取实际值，如果不为实际值，返回 None 显然不如 try catch 来的实在。避免使用 global命名要注意动态创建方法 , 我觉得这点值得商榷。 3.可读性 不要检查，如果可能有异常，尽量抛出异常来 trycatch 解决。a is None , if flagisinstance , not type(r) is types.ListType“{name}{city}”.format(**info_dict)for k , v in infodict.items()使用 poiinfo = namedtuple(“poiinfo”,[“name”,”lng”,”lat”]) 返回 poiinfo[‘上海’,121.00,23] 最后返回值打印 poi.name , poi.lng , poi latfor numbers_value, letters_value in zip(numbers, letters):enumerate如果能用 listcomp 则不使用 map 和 filter 4.安全性5.性能","categories":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"杂谈","slug":"杂谈","permalink":"https://zem12345678.github.io/tags/杂谈/"}]},{"title":"Jquery ajax, Axios, Fetch区别浅谈","slug":"Jquery ajax, Axios, Fetch区别浅谈","date":"2018-10-16T10:57:03.472Z","updated":"2018-10-16T12:26:10.813Z","comments":true,"path":"2018/10/16/Jquery ajax, Axios, Fetch区别浅谈/","link":"","permalink":"https://zem12345678.github.io/2018/10/16/Jquery ajax, Axios, Fetch区别浅谈/","excerpt":"","text":"前端技术是一个发展飞快的领域,JQuery ajax早已不能专美于前，axios和fetch都已经开始分别抢占“请求”这个前端高地。 1 JQuery ajax：廉颇老矣。尚能饭，但总有饭不动的一天。1234567891011$.ajax(&#123; type: &apos;POST&apos;, url: url, data: data, dataType: dataType, success: function () &#123;&#125;, error: function () &#123;&#125;&#125;); 这个我就不用多言了把，是对原生XHR的封装，除此以外还增添了对JSONP的支持。有一说一的说一句，JQuery ajax经过多年的更新维护，真的已经是非常的方便了，优点无需多言；如果是硬要举出几个缺点，那可能只有: 本身是针对MVC的编程,不符合现在前端MVVM的浪潮 基于原生的XHR开发，XHR本身的架构不清晰，已经有了fetch的替代方案 JQuery整个项目太大，单纯使用ajax却要引入整个JQuery非常的不合理（采取个性化打包的方案又不能享受CDN服务） 尽管JQuery对我们前端的开发工作曾有着（现在也仍然有着）深远的影响，但是我们可以看到随着VUE，REACT新一代框架的兴起，以及ES规范的完善，更多API的更新，JQuery这种大而全的JS库，未来的路会越走越窄。 2 Axios： 谁敢横刀立马，唯我Axios大将军！1234567891011121314axios(&#123; method: &apos;post&apos;, url: &apos;/user/12345&apos;, data: &#123; firstName: &apos;Fred&apos;, lastName: &apos;Flintstone&apos; &#125;&#125;).then(function (response) &#123; console.log(response);&#125;).catch(function (error) &#123; console.log(error);&#125;); Vue2.0之后，尤雨溪推荐大家用axios替换JQuery ajax，想必让Axios进入了很多人的目光中。Axios本质上也是对原生XHR的封装，只不过它是Promise的实现版本，符合最新的ES规范，从它的官网上可以看到它有以下几条特性： 从 node.js 创建 http 请求 支持 Promise API ； 客户端支持防止CSRF 提供了一些并发请求的接口（重要，方便了很多的操作） 这个支持防止CSRF其实挺好玩的，是怎么做到的呢，就是让你的每个请求都带一个从cookie中拿到的key, 根据浏览器同源策略，假冒的网站是拿不到你cookie中得key的，这样，后台就可以轻松辨别出这个请求是否是用户在假冒网站上的误导输入，从而采取正确的策略。Axios既提供了并发的封装，也没有下文会提到的fetch的各种问题，而且体积也较小，当之无愧现在最应该选用的请求的方式。 3 Fetch ：酋长的孩子,还需成长fetch号称是AJAX的替代品，它的好处在《传统 Ajax 已死，Fetch 永生》中提到有以下几点： 符合关注分离，没有将输入、输出和用事件来跟踪的状态混杂在一个对象里 更好更方便的写法，诸如： 1234try &#123; let response = await fetch(url); let data = response.json(); console.log(data);&#125; catch(e) &#123; console.log(&quot;Oops, error&quot;, e);&#125; 坦白说，上面的理由对我来说完全没有什么说服力，因为不管是Jquery还是Axios都已经帮我们把xhr封装的足够好，使用起来也足够方便，为什么我们还要花费大力气去学习fetch？我认为fetch的优势主要优势就是： 更加底层，提供的API丰富（request, response） 脱离了XHR，是ES规范里新的实现方式 偶尔觉得写的丑陋，但是在使用了JQuery和axios之后，已经对这一块完全无所谓了。当然，如果新的fetch能做的同样好，我为了不掉队也会选择使用fetch。这个道理其实很好理解：你有一架歼8，魔改了N次，性能达到了歼10的水准，但是要是有个人给你拿来一架新的歼10，你也会毫不犹豫的选择新的歼10——不仅仅是新，也代表了还有新的魔改潜力。但是我最近在使用fetch的时候，也遇到了不少的问题 fetch是一个低层次的API，你可以把它考虑成原生的XHR，所以使用起来并不是那么舒服，需要进行封装 例如： 1）fetch只对网络请求报错，对400，500都当做成功的请求，需要封装去处理2）fetch默认不会带cookie，需要添加配置项3）fetch不支持abort，不支持超时控制，使用setTimeout及Promise.reject的实现的超时控制并不能阻止请求过程继续在后台运行，造成了流量的浪费4）fetch没有办法原生监测请求的进度，而XHR可以 PS: fetch的具体问题大家可以参考：《fetch没有你想象的那么美》《fetch使用的常见问题及解决方法》 看到这里，你心里一定有个疑问，这鬼东西就是个半拉子工程嘛，我还是回去用Jquery或者Axios算了——其实我就是这么打算的。但是，必须要提出的是，我发现fetch在前端的应用上有一项xhr怎么也比不上的能力：跨域的处理。 我们都知道因为同源策略的问题，浏览器的请求是可能随便跨域的——一定要有跨域头或者借助JSONP，但是，fetch中可以设置mode为”no-cors”（不跨域），如下所示： 123fetch(&apos;/users.json&apos;, &#123; method: &apos;post&apos;, mode: &apos;no-cors&apos;, data: &#123;&#125;&#125;).then(function() &#123; /* handle response */ &#125;); 这样之后我们会得到一个type为“opaque”的返回。需要指出的是，这个请求是真正抵达过后台的，所以我们可以使用这种方法来进行信息上报，在我们之前的image.src方法中多出了一种选择，另外，我们在network中可以看到这个请求后台设置跨域头之后的实际返回，有助于我们提前调试接口（当然，通过chrome插件我们也可以做的到）。总之，fetch现在还不是很好用，我尝试过几个fetch封装的包，都还不尽如人意。 总结现在只需要知道无脑使用axios即可，Jquery老迈笨拙，fetch年轻稚嫩，只有Axios正当其年！","categories":[{"name":"前端","slug":"前端","permalink":"https://zem12345678.github.io/categories/前端/"}],"tags":[{"name":"前端","slug":"前端","permalink":"https://zem12345678.github.io/tags/前端/"},{"name":"跨域","slug":"跨域","permalink":"https://zem12345678.github.io/tags/跨域/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-10-16T03:24:15.311Z","updated":"2018-10-16T03:24:15.311Z","comments":true,"path":"2018/10/16/hello-world/","link":"","permalink":"https://zem12345678.github.io/2018/10/16/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}