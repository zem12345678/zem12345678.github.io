{"meta":{"title":"Enmin`s blog","subtitle":null,"description":null,"author":"Enmin","url":"https://zem12345678.github.io"},"pages":[{"title":"about","date":"2018-10-16T08:50:22.000Z","updated":"2018-10-19T03:00:32.812Z","comments":true,"path":"about/index.html","permalink":"https://zem12345678.github.io/about/index.html","excerpt":"","text":"技术爱玩发与部署环境为 DockerPython 3.6.3前端 Vue + Webpack + ES2015 + axios后端 Django 2.0 + DjangoRestFramework + Celery自动化部署选用工具 Ansible 以及 Docker，K8s后端组件ElasticSearch 用于搜索和推荐PostgreSQL 用于数据持久化Mysql/Mangodb，用于数据存储Redis 缓存RabbitMQ 分布式队列 / 定时任务Nginx 用于反向代理如果你也是追新的 Django 开发者，一起来提 PR"}],"posts":[{"title":"RabbitMQ简介","slug":"RabbitMQ简介","date":"2018-10-19T03:29:16.775Z","updated":"2018-10-19T04:28:44.379Z","comments":true,"path":"2018/10/19/RabbitMQ简介/","link":"","permalink":"https://zem12345678.github.io/2018/10/19/RabbitMQ简介/","excerpt":"","text":"RabbitMQ是一个由erlang开发的AMQP（Advanced Message Queue ）的开源实现。AMQP 的出现其实也是应了广大人民群众的需求，虽然在同步消息通讯的世界里有很多公开标准（如 COBAR的 IIOP ，或者是 SOAP 等），但是在异步消息处理中却不是这样，只有大企业有一些商业实现（如微软的 MSMQ ，IBM 的 Websphere MQ 等），因此，在 2006 年的 6 月，Cisco 、Redhat、iMatix 等联合制定了 AMQP 的公开标准。 RabbitMQ是由RabbitMQ Technologies Ltd开发并且提供商业支持的。该公司在2010年4月被SpringSource（VMWare的一个部门）收购。在2013年5月被并入Pivotal。其实VMWare，Pivotal和EMC本质上是一家的。不同的是VMWare是独立上市子公司，而Pivotal是整合了EMC的某些资源，现在并没有上市。 RabbitMQ的官网是http://www.rabbitmq.comRabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括： 什么叫消息队列(MQ)消息（Message）是指在应用间传送的数据。消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。 消息队列（Message Queue）是一种应用程序对应用程序的通信方法，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。通过消息队列通信，让A，B两个服务指间保持低耦合，实现业务的灵活拓展。 为何用消息队列(MQ)从上面的描述中可以看出消息队列是一种应用间的异步协作机制，那什么时候需要使用 MQ 呢？ 以常见的订单系统为例，用户点击【下单】按钮之后的业务逻辑可能包括：扣减库存、生成相应单据、发红包、发短信通知。在业务发展初期这些逻辑可能放在一起同步执行，随着业务的发展订单量增长，需要提升系统服务的性能，这时可以将一些不需要立即生效的操作拆分出来异步执行，比如发放红包、发短信通知等。这种场景下就可以用 MQ ，在下单的主流程（比如扣减库存、生成相应单据）完成之后发送一条消息到 MQ 让主流程快速完结，而由另外的单独线程拉取MQ的消息（或者由 MQ 推送消息），当发现 MQ 中有发红包或发短信之类的消息时，执行相应的业务逻辑。 以上是用于业务解耦的情况，其它常见场景包括最终一致性、广播、错峰流控等等。 RabbitMQ 特点 可靠性（Reliability）RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。 灵活的路由（Flexible Routing）在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。 消息集群（Clustering）多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。 高可用（Highly Available Queues）队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。 多种协议（Multi-protocol）RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。 多语言客户端（Many Clients）RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby ,python,等等。 管理界面（Management UI）RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。 跟踪机制（Tracing）如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。 插件机制（Plugin System）RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。 RabbitMQ 中的概念模型消息模型所有 MQ 产品从模型抽象上来说都是一样的过程：消费者（consumer）订阅某个队列。生产者（producer）创建消息，然后发布到队列（queue）中，最后将消息发送到监听的消费者。 RabbitMQ 基本概念上面只是最简单抽象的描述，具体到 RabbitMQ 则有更详细的概念需要解释。上面介绍过 RabbitMQ 是 AMQP 协议的一个开源实现，所以其内部实际上也是 AMQP 中的基本概念： Message消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher消息的生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。 Binding绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Queue消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Connection网络连接，比如一个TCP连接。 Channel信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker表示消息队列服务器实体。AMQP 中的消息路由AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。 Exchange 类型Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： direct 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routing key 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配、单播的模式。 fanout 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 topic topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“”。#匹配0个或多个单词，匹配不多不少一个单词。","categories":[{"name":"异步处理","slug":"异步处理","permalink":"https://zem12345678.github.io/categories/异步处理/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"https://zem12345678.github.io/tags/消息队列/"},{"name":"分布式","slug":"分布式","permalink":"https://zem12345678.github.io/tags/分布式/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://zem12345678.github.io/tags/RabbitMQ/"}]},{"title":"Hadoop、Storm、Spark","slug":"Hadoop、Storm、Spark","date":"2018-10-18T07:28:29.350Z","updated":"2018-10-18T07:32:03.219Z","comments":true,"path":"2018/10/18/Hadoop、Storm、Spark/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/Hadoop、Storm、Spark/","excerpt":"","text":"Storm与Spark、Hadoop这三种框架，各有各的优点，每个框架都有自己的最佳应用场景。所以，在不同的应用场景下，应该选择不同的框架。 StormStorm是最佳的流式计算框架，Storm由Java和Clojure写成，Storm的优点是全内存计算，所以它的定位是分布式实时计算系统，按照Storm作者的说法，Storm对于实时计算的意义类似于Hadoop对于批处理的意义。 Storm的适用场景：1）流数据处理Storm可以用来处理源源不断流进来的消息，处理之后将结果写入到某个存储中去。2）分布式RPC。由于Storm的处理组件是分布式的，而且处理延迟极低，所以可以作为一个通用的分布式RPC框架来使用。 sparkSparkSpark是一个基于内存计算的开源集群计算系统，目的是更快速的进行数据分析。Spark由加州伯克利大学AMP实验室Matei为主的小团队使用Scala开发开发，类似于Hadoop MapReduce的通用并行计算框架，Spark基于Map Reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的Map Reduce的算法。 Spark的适用场景：1）多次操作特定数据集的应用场合Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。2）粗粒度更新状态的应用由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如Web服务的存储或者是增量的Web爬虫和索引。就是对于那种增量修改的应用模型不适合。总的来说Spark的适用面比较广泛且比较通用。 hadoopHadoop是实现了MapReduce的思想，将数据切片计算来处理大量的离线数据数据。Hadoop处理的数据必须是已经存放在HDFS上或者类似HBase的数据库中，所以Hadoop实现的时候是通过移动计算到这些存放数据的机器上来提高效率。 Hadoop的适用场景：1）海量数据的离线分析处理2）大规模Web信息搜索3）数据密集型并行计算 简单来说：Hadoop适合于离线的批量数据处理适用于对实时性要求极低的场景Storm适合于实时流数据处理，实时性方面做得极好Spark是内存分布式计算框架，试图吞并Hadoop的Map-Reduce批处理框架和Storm的流处理框架，但是Spark已经做得很不错了，批处理方面性能优于Map-Reduce，但是流处理目前还是弱于Storm，产品仍在改进之中","categories":[{"name":"大数据","slug":"大数据","permalink":"https://zem12345678.github.io/categories/大数据/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://zem12345678.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://zem12345678.github.io/tags/Hadoop/"}]},{"title":"python基础小谈","slug":"python基础小谈","date":"2018-10-18T06:55:13.351Z","updated":"2018-10-18T07:07:25.385Z","comments":true,"path":"2018/10/18/python基础小谈/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/python基础小谈/","excerpt":"","text":"python语音是动态解释类型的，被称为胶水语言，再python的底层函数我们会经常看到两个形参*args,**kwargs，那么它们的本质是什么，什么使用它们呢？ 一 .*args 和 **kwargs 是什么？*args本质是一个tuple（元组），**kwargs本质是一个dict（字典）。 二.怎么用 *args 和 **kwargs?def my_fun(*args, **kwargs ): print (‘args = ‘, args) print (‘kwargs = ‘, kwargs) 调用就比较有意思了，传统的比如，c, c++, Java, C#，基本都是一对一传参，但是python靠这两个参数，可以实现多参的灵活传入。如下所示，我完全可以这么调用： my_fun(1,3,5,9, a=2, b=4) 这样打印的结果： args = (1,3,5,9) # 是一个元组 kwargs = { ‘a’: 2 , ‘b’:4 } #是一个字典 注意事项：上述函数 my_fun，如果这么调用就会有问题： my_fun( a=2, b=4, 1,3,5,9 ) 报错：SyntaxError: non-keyword arg after keyword arg” 意思是：关键字参数后面不能有非关键字参数，言外之意，关键字参数 * kwargs 必须位于 args 之后！","categories":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"}]},{"title":"Thinking In Python Language","slug":"Thinking In Python Language","date":"2018-10-18T05:54:54.070Z","updated":"2018-10-18T06:05:57.927Z","comments":true,"path":"2018/10/18/Thinking In Python Language/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/Thinking In Python Language/","excerpt":"","text":"1.前言本文诞生于利用 Topic Reading 方法读 Python 若干本技术书籍这个过程中结合自己的开发常见场景记录下来的一些笔记。 2.简介1. 为什么是 Python Python, 很大程度上是因为 Python 的快速开发。 当然，快速开发（这里的开发包含部署）这个词也往往会被误解。什么叫做快速？我用一个 CMS 框架快速搭建出一个网站这是否叫做快速？ 每一次部署的时候，如果使用 Java 或者是 Go, 部署的时候直接 maven 编译打包，接着把 War 包直接上传到 Tomcat 就结束了。而用 Python 则需要各种虚拟环境，各种稀里哗啦的配置。这种情况下是哪一种快速呢？Python 有什么好处呢？ 写代码效率高。 生态圈好。 写代码效率高，这指的是写 Python 代码，而不是运行时。3.写代码效率高，这指的是写 Python 代码，而不是运行时。 生态圈好，Web 开发用 Django/Flask , 数据抓取用 Requests , 数据分析清洗用 Pandas, 机器学习。 2. 工具链 Anaconda工具：https://www.anaconda.com/download/ 3. 文档 官方文档：https://docs.python.org/3/ 4. 社区 官方社区：https://www.python.org/community/ 4. 书籍 《python核心编程》，《python编程从入门到实践》 3. 基本概念 程序 = 算法 + 数据结构 这句话当然是不全面的，但并不影响这句话在计算机世界里面的地位。依我看来，对我的启发大致是：我会把 API 的调用和数据结构以及算法想清楚，然后才动手把代码分解成伪代码。 1.数据类型数据类型按照不同的划分标准可以进行不同的划分： 按照复杂性可以这么划分： 简单类型 复杂类型】 按照复杂性可以这么划分： 基本类型 引用类型 按照数据结构可以这么划分： 集合结构 : 串 线性结构 : 线性表 （单链表，静态链表，循环链表，双向链表，栈，队列) 树形结构 : 树（二叉树，B+ 树，红黑树） 图形结构 : 图 2. 操作对于一些基本的数据类型，操作为 加减乘除取余数位运算等等 对于复杂的一些数据类型，则需要对数据结构多一些了解。 比如，对队列而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对 hash 而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对字典而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对字符串而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？ 那字符串来说，Java 推荐使用 StringBuilder 来合并多个字符串，Python 推荐 join 多个字符串等等。 4.1.函数2.作用域3.模块模块，这个概念，可大可小，大的时候，把一个程序说成是模块，小的时候，可以把一个文件，甚至你说这一个函数是一个模块，也行。 这里的模块指的是一个包下的函数。 4.面向对象面向对象有三大概念： 封装 继承 多态5.错误 / 调试测试异常处理实际上可以考验一个程序员编写代码的健壮性。 事实上来说，代码写的健壮是一个程序员必备的素养。但其实在开发过程中，出于对项目进行赶工上线，需要对程序的健壮性做出一定的取舍。并且，在编写客户端，服务端，网页前端的时候基本上都会遇到这个问题。什么时候选择健壮的程序，什么时候选择是还可以的程序。需要自己的经验。 6. IO 编程7.进程和线程1.多线程 Python 多线程约等于并发。 2.多进程3.GILGlobal Interpreter Lock 并不是所有的解释器语言都有 GIL （尽管 Python 和 Ruby 里面都有）, 也并不是没有尝试过去除 GIL, 但是每次去除都会导致单线程性能的下降。所以暂时保留。 GIL 对程序中的影响： 一个线程运行 Python , 而其他 N 个睡眠或者等待 I/O - 同一时刻只有一个线程对共享资源进行存取 , Python 线程也可以等待 threading.Lock 或者线程模块中的其他同步对象； 协同式多任务处理如果有两个线程，同时进行 IO 请求，当其中一个线程连接之后，立即会主动让出 GIL, 其他线程就可以运行。 当N 个线程在网络 I/O 堵塞，或等待重新获取 GIL，而一个线程运行 Python。 让出之后还要执行代码呀，所以要有个收回 GIL 的动作。 抢占式多任务处理Python 2 GIL , 尝试收回 GIL 为 执行 1000 字节码。Python 3 GIL , 尝试收回 GIL 检测间隔为 15ms 线程安全原子操作：sort 之类不需要非原子操作：n=n+2 的字节码分为 加载 n , 加载 2 , 相加，存储 n, 四个步骤，由于不是原子性，很可能被由于 15 ms 而被打断。 当然，懒人一向是 : 优先级不决加括号，线程不决加 lock 对于 Java, 程序员努力在尽可能短的时间内加锁存取共享数据，减轻线程的争夺，实现最大并行。但 Python 中，线程无法并行运行，细粒度的锁就没有了优势。 8.正则表达式5.高级技巧6.标准库常用内建模块系统化模块IntroductionBuilt-in FunctionsBuilt-in ConstantsBuilt-in TypesBuilt-in ExceptionsText Processing ServicesBinary Data ServicesData TypesNumeric and Mathematical ModulesFunctional Programming ModulesFile and Directory AccessData PersistenceData Compression and ArchivingFile FormatsCryptographic ServicesGeneric Operating System ServicesConcurrent ExecutionInterprocess Communication and NetworkingInternet Data HandlingStructured Markup Processing ToolsInternet Protocols and SupportMultimedia ServicesInternationalizationProgram FrameworksGraphical User Interfaces with TkDevelopment ToolsDebugging and ProfilingSoftware Packaging and DistributionPython Runtime ServicesCustom Python InterpretersImporting ModulesPython Language ServicesMiscellaneous ServicesMS Windows Specific ServicesUnix Specific ServicesSuperseded ModulesUndocumented Modules 7.第三方库Requests : API 人性化 8.代码质量1.正确性 外部不该引用 protected member （单下划线）lambda 为一次使用，最好不要赋值。不要给 buildin 函数赋值py3 直接 super()for in else 如果不内置 break 则出会在最后 for in 为 empty 的时候再执行 else 中的语句context exit 如果不 catch 掉异常让其自然向上一级抛出错误的话，必须为 (self, exception_type, exception_value, traceback):不要在 init 里面 return 数据不要混用 tab 和 space4 个 space 缩进staticmethod 直接是 参数，classmethod 第一个参数为 cls可变的 default value 是不能作为 参数的。（可能是解释器在确定函数的定义的时候完成赋值？)遵循 exception hierachy https://docs.python.org/3/library/exceptions.html#exception-hierarchydefaultdict defaultdict(lambda : 6) , 必须 callable尽量 unpack 赋值字典用获取用 get(“myk”,None) , 赋值用 dictionary.setdefault(“list”, []).append(“list_item”) 2.可维护性 避免使用 import * , 我觉得这点值得商榷 , 如果是某个模块下，完全可以先把模块拆分成多个，最后 import 进来，接着使用 all.getxxx 获取实际值，如果不为实际值，返回 None 显然不如 try catch 来的实在。避免使用 global命名要注意动态创建方法 , 我觉得这点值得商榷。 3.可读性 不要检查，如果可能有异常，尽量抛出异常来 trycatch 解决。a is None , if flagisinstance , not type(r) is types.ListType“{name}{city}”.format(**info_dict)for k , v in infodict.items()使用 poiinfo = namedtuple(“poiinfo”,[“name”,”lng”,”lat”]) 返回 poiinfo[‘上海’,121.00,23] 最后返回值打印 poi.name , poi.lng , poi latfor numbers_value, letters_value in zip(numbers, letters):enumerate如果能用 listcomp 则不使用 map 和 filter 4.安全性5.性能","categories":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"杂谈","slug":"杂谈","permalink":"https://zem12345678.github.io/tags/杂谈/"}]},{"title":"Jquery ajax, Axios, Fetch区别浅谈","slug":"Jquery ajax, Axios, Fetch区别浅谈","date":"2018-10-16T10:57:03.472Z","updated":"2018-10-16T12:26:10.813Z","comments":true,"path":"2018/10/16/Jquery ajax, Axios, Fetch区别浅谈/","link":"","permalink":"https://zem12345678.github.io/2018/10/16/Jquery ajax, Axios, Fetch区别浅谈/","excerpt":"","text":"前端技术是一个发展飞快的领域,JQuery ajax早已不能专美于前，axios和fetch都已经开始分别抢占“请求”这个前端高地。 1 JQuery ajax：廉颇老矣。尚能饭，但总有饭不动的一天。1234567891011$.ajax(&#123; type: &apos;POST&apos;, url: url, data: data, dataType: dataType, success: function () &#123;&#125;, error: function () &#123;&#125;&#125;); 这个我就不用多言了把，是对原生XHR的封装，除此以外还增添了对JSONP的支持。有一说一的说一句，JQuery ajax经过多年的更新维护，真的已经是非常的方便了，优点无需多言；如果是硬要举出几个缺点，那可能只有: 本身是针对MVC的编程,不符合现在前端MVVM的浪潮 基于原生的XHR开发，XHR本身的架构不清晰，已经有了fetch的替代方案 JQuery整个项目太大，单纯使用ajax却要引入整个JQuery非常的不合理（采取个性化打包的方案又不能享受CDN服务） 尽管JQuery对我们前端的开发工作曾有着（现在也仍然有着）深远的影响，但是我们可以看到随着VUE，REACT新一代框架的兴起，以及ES规范的完善，更多API的更新，JQuery这种大而全的JS库，未来的路会越走越窄。 2 Axios： 谁敢横刀立马，唯我Axios大将军！1234567891011121314axios(&#123; method: &apos;post&apos;, url: &apos;/user/12345&apos;, data: &#123; firstName: &apos;Fred&apos;, lastName: &apos;Flintstone&apos; &#125;&#125;).then(function (response) &#123; console.log(response);&#125;).catch(function (error) &#123; console.log(error);&#125;); Vue2.0之后，尤雨溪推荐大家用axios替换JQuery ajax，想必让Axios进入了很多人的目光中。Axios本质上也是对原生XHR的封装，只不过它是Promise的实现版本，符合最新的ES规范，从它的官网上可以看到它有以下几条特性： 从 node.js 创建 http 请求 支持 Promise API ； 客户端支持防止CSRF 提供了一些并发请求的接口（重要，方便了很多的操作） 这个支持防止CSRF其实挺好玩的，是怎么做到的呢，就是让你的每个请求都带一个从cookie中拿到的key, 根据浏览器同源策略，假冒的网站是拿不到你cookie中得key的，这样，后台就可以轻松辨别出这个请求是否是用户在假冒网站上的误导输入，从而采取正确的策略。Axios既提供了并发的封装，也没有下文会提到的fetch的各种问题，而且体积也较小，当之无愧现在最应该选用的请求的方式。 3 Fetch ：酋长的孩子,还需成长fetch号称是AJAX的替代品，它的好处在《传统 Ajax 已死，Fetch 永生》中提到有以下几点： 符合关注分离，没有将输入、输出和用事件来跟踪的状态混杂在一个对象里 更好更方便的写法，诸如： 1234try &#123; let response = await fetch(url); let data = response.json(); console.log(data);&#125; catch(e) &#123; console.log(&quot;Oops, error&quot;, e);&#125; 坦白说，上面的理由对我来说完全没有什么说服力，因为不管是Jquery还是Axios都已经帮我们把xhr封装的足够好，使用起来也足够方便，为什么我们还要花费大力气去学习fetch？我认为fetch的优势主要优势就是： 更加底层，提供的API丰富（request, response） 脱离了XHR，是ES规范里新的实现方式 偶尔觉得写的丑陋，但是在使用了JQuery和axios之后，已经对这一块完全无所谓了。当然，如果新的fetch能做的同样好，我为了不掉队也会选择使用fetch。这个道理其实很好理解：你有一架歼8，魔改了N次，性能达到了歼10的水准，但是要是有个人给你拿来一架新的歼10，你也会毫不犹豫的选择新的歼10——不仅仅是新，也代表了还有新的魔改潜力。但是我最近在使用fetch的时候，也遇到了不少的问题 fetch是一个低层次的API，你可以把它考虑成原生的XHR，所以使用起来并不是那么舒服，需要进行封装 例如： 1）fetch只对网络请求报错，对400，500都当做成功的请求，需要封装去处理2）fetch默认不会带cookie，需要添加配置项3）fetch不支持abort，不支持超时控制，使用setTimeout及Promise.reject的实现的超时控制并不能阻止请求过程继续在后台运行，造成了流量的浪费4）fetch没有办法原生监测请求的进度，而XHR可以 PS: fetch的具体问题大家可以参考：《fetch没有你想象的那么美》《fetch使用的常见问题及解决方法》 看到这里，你心里一定有个疑问，这鬼东西就是个半拉子工程嘛，我还是回去用Jquery或者Axios算了——其实我就是这么打算的。但是，必须要提出的是，我发现fetch在前端的应用上有一项xhr怎么也比不上的能力：跨域的处理。 我们都知道因为同源策略的问题，浏览器的请求是可能随便跨域的——一定要有跨域头或者借助JSONP，但是，fetch中可以设置mode为”no-cors”（不跨域），如下所示： 123fetch(&apos;/users.json&apos;, &#123; method: &apos;post&apos;, mode: &apos;no-cors&apos;, data: &#123;&#125;&#125;).then(function() &#123; /* handle response */ &#125;); 这样之后我们会得到一个type为“opaque”的返回。需要指出的是，这个请求是真正抵达过后台的，所以我们可以使用这种方法来进行信息上报，在我们之前的image.src方法中多出了一种选择，另外，我们在network中可以看到这个请求后台设置跨域头之后的实际返回，有助于我们提前调试接口（当然，通过chrome插件我们也可以做的到）。总之，fetch现在还不是很好用，我尝试过几个fetch封装的包，都还不尽如人意。 总结现在只需要知道无脑使用axios即可，Jquery老迈笨拙，fetch年轻稚嫩，只有Axios正当其年！","categories":[{"name":"前端","slug":"前端","permalink":"https://zem12345678.github.io/categories/前端/"}],"tags":[{"name":"前端","slug":"前端","permalink":"https://zem12345678.github.io/tags/前端/"},{"name":"跨域","slug":"跨域","permalink":"https://zem12345678.github.io/tags/跨域/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-10-16T03:24:15.311Z","updated":"2018-10-16T03:24:15.311Z","comments":true,"path":"2018/10/16/hello-world/","link":"","permalink":"https://zem12345678.github.io/2018/10/16/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}