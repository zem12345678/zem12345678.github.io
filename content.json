{"meta":{"title":"Enmin`s blog","subtitle":null,"description":null,"author":"Enmin","url":"https://zem12345678.github.io"},"pages":[{"title":"about","date":"2018-10-16T08:50:22.000Z","updated":"2018-10-19T03:00:32.812Z","comments":true,"path":"about/index.html","permalink":"https://zem12345678.github.io/about/index.html","excerpt":"","text":"技术爱玩发与部署环境为 DockerPython 3.6.3前端 Vue + Webpack + ES2015 + axios后端 Django 2.0 + DjangoRestFramework + Celery自动化部署选用工具 Ansible 以及 Docker，K8s后端组件ElasticSearch 用于搜索和推荐PostgreSQL 用于数据持久化Mysql/Mangodb，用于数据存储Redis 缓存RabbitMQ 分布式队列 / 定时任务Nginx 用于反向代理如果你也是追新的 Django 开发者，一起来提 PR"}],"posts":[{"title":"Redis集群搭建","slug":"搭建Redis集群","date":"2019-03-15T04:14:36.433Z","updated":"2019-03-15T04:15:04.389Z","comments":true,"path":"2019/03/15/搭建Redis集群/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/搭建Redis集群/","excerpt":"","text":"搭建Redis集群为什么要有集群之前我们已经讲了主从的概念，一主可以多从，如果同时的访问量过大(1000w),主服务肯定就会挂掉，数据服务就挂掉了或者发生自然灾难大公司都会有很多的服务器(华东地区、华南地区、华中地区、华北地区、西北地区、西南地区、东北地区、台港澳地区机房) 集群的概念集群是一组相互独立的、通过高速网络互联的计算机，它们构成了一个组，并以单一系统的模式加以管理。一个客户与集群相互作用时，集群像是一个独立的服务器。集群配置是用于提高可用性和可缩放性。当请求到来首先由负载均衡服务器处理，把请求转发到另外的一台服务器上。 redis集群分类 软件层面硬件层面 软件层面：只有一台电脑，在这一台电脑上启动了多个redis服务。硬件层面：存在多台实体的电脑，每台电脑上都启动了一个redis或者多个redis服务。 ##搭建集群当前拥有两台主机172.16.179.130、172.16.179.131，这⾥的IP在使⽤时要改为实际值参考阅读redis集群搭建 http://www.cnblogs.com/wuxl360/p/5920330.html[Python]搭建redis集群 http://blog.5ibc.net/p/51020.html 配置机器1 在演示中，172.16.179.130为当前ubuntu机器的ip在172.16.179.130上进⼊Desktop⽬录，创建conf⽬录在conf⽬录下创建⽂件7000.conf，编辑内容如下 12345678port 7000bind 172.16.179.130daemonize yespidfile 7000.pidcluster-enabled yescluster-config-file 7000_node.confcluster-node-timeout 15000appendonly yes 在conf⽬录下创建⽂件7001.conf，编辑内容如下12345678port 7001bind 172.16.179.130daemonize yespidfile 7001.pidcluster-enabled yescluster-config-file 7001_node.confcluster-node-timeout 15000appendonly yes 在conf⽬录下创建⽂件7002.conf，编辑内容如下12345678port 7002bind 172.16.179.130daemonize yespidfile 7002.pidcluster-enabled yescluster-config-file 7002_node.confcluster-node-timeout 15000appendonly yes 总结：三个⽂件的配置区别在port、pidfile、cluster-config-file三项 使⽤配置⽂件启动redis服务 redis-server 7000.confredis-server 7001.confredis-server 7002.conf 查看进程如下图 配置机器2 在演示中，172.16.179.131为当前ubuntu机器的ip在172.16.179.131上进⼊Desktop⽬录，创建conf⽬录在conf⽬录下创建⽂件7003.conf，编辑内容如下12345678port 7003bind 172.16.179.131daemonize yespidfile 7003.pidcluster-enabled yescluster-config-file 7003_node.confcluster-node-timeout 15000appendonly yes 在conf⽬录下创建⽂件7004.conf，编辑内容如下12345678port 7004bind 172.16.179.131daemonize yespidfile 7004.pidcluster-enabled yescluster-config-file 7004_node.confcluster-node-timeout 15000appendonly yes 在conf⽬录下创建⽂件7005.conf，编辑内容如下12345678port 7005bind 172.16.179.131daemonize yespidfile 7005.pidcluster-enabled yescluster-config-file 7005_node.confcluster-node-timeout 15000appendonly yes 总结：三个⽂件的配置区别在port、pidfile、cluster-config-file三项 使⽤配置⽂件启动redis服务 redis-server 7003.confredis-server 7004.confredis-server 7005.conf 查看进程如下图 创建集群redis的安装包中包含了redis-trib.rb，⽤于创建集群接下来的操作在172.16.179.130机器上进⾏将命令复制，这样可以在任何⽬录下调⽤此命令 sudo cp /usr/share/doc/redis-tools/examples/redis-trib.rb /usr/local/bin/ 安装ruby环境，因为redis-trib.rb是⽤ruby开发的 sudo apt-get install ruby 在提示信息处输⼊y，然后回⻋继续安装运⾏如下命令创建集群 redis-trib.rb create –replicas 1 172.16.179.130:7000 172.16.179.130:7001 172.16.179.130:7002 172.16.179.131:7003 172.16.179.131:7004 172.16.179.131:7005 执⾏上⾯这个指令在某些机器上可能会报错,主要原因是由于安装的 ruby 不是最 新版本! 天朝的防⽕墙导致⽆法下载最新版本,所以需要设置 gem 的源 解决办法如下 – 先查看⾃⼰的 gem 源是什么地址gem source -l – 如果是https://rubygems.org/ 就需要更换– 更换指令为gem sources –add https://gems.ruby-china.org/ –remove https://rubygems.org/– 通过 gem 安装 redis 的相关依赖sudo gem install redis– 然后重新执⾏指令redis-trib.rb create –replicas 1 172.16.179.130:7000 172.16.179.130:7001 172.16.179.130:7002 172.16.179.131:7003 172.16.179.131:7004 172.16.179.131:7005 提示如下主从信息，输⼊yes后回⻋提示完成，集群搭建成功 数据验证根据上图可以看出，当前搭建的主服务器为7000、7001、7003，对应的从服务器是7004、7005、7002在172.16.179.131机器上连接7002，加参数-c表示连接到集群 redis-cli -h 172.16.179.131 -c -p 7002 写⼊数据 set name itheima⾃动跳到了7003服务器，并写⼊数据成功在7003可以获取数据，如果写入数据又重定向到7000(负载均衡) 在哪个服务器上写数据：CRC16 redis cluster在设计的时候，就考虑到了去中⼼化，去中间件，也就是说，集群中 的每个节点都是平等的关系，都是对等的，每个节点都保存各⾃的数据和整个集 群的状态。每个节点都和其他所有节点连接，⽽且这些连接保持活跃，这样就保 证了我们只需要连接集群中的任意⼀个节点，就可以获取到其他节点的数据Redis集群没有并使⽤传统的⼀致性哈希来分配数据，⽽是采⽤另外⼀种叫做哈希 槽 (hash slot)的⽅式来分配的。redis cluster 默认分配了 16384 个slot，当我们 set⼀个key 时，会⽤CRC16算法来取模得到所属的slot，然后将这个key 分到哈 希槽区间的节点上，具体算法就是：CRC16(key) % 16384。所以我们在测试的 时候看到set 和 get 的时候，直接跳转到了7000端⼝的节点Redis 集群会把数据存在⼀个 master 节点，然后在这个 master 和其对应的salve 之间进⾏数据同步。当读取数据时，也根据⼀致性哈希算法到对应的 master 节 点获取数据。只有当⼀个master 挂掉之后，才会启动⼀个对应的 salve 节点，充 当 master需要注意的是：必须要3个或以上的主节点，否则在创建集群时会失败，并且当存 活的主节点数⼩于总节点数的⼀半时，整个集群就⽆法提供服务了 Python交互安装包如下 pip install redis-py-cluster redis-py-cluster源码地址https://github.com/Grokzen/redis-py-cluster 创建⽂件redis_cluster.py，示例码如下12345678910111213141516171819from rediscluster import *if __name__ == &apos;__main__&apos;: try: # 构建所有的节点，Redis会使⽤CRC16算法，将键和值写到某个节点上 startup_nodes = [ &#123;&apos;host&apos;: &apos;192.168.26.128&apos;, &apos;port&apos;: &apos;7000&apos;&#125;, &#123;&apos;host&apos;: &apos;192.168.26.130&apos;, &apos;port&apos;: &apos;7003&apos;&#125;, &#123;&apos;host&apos;: &apos;192.168.26.128&apos;, &apos;port&apos;: &apos;7001&apos;&#125;, ] # 构建StrictRedisCluster对象 src=StrictRedisCluster(startup_nodes=startup_nodes,decode_responses=True) # 设置键为name、值为itheima的数据 result=src.set(&apos;name&apos;,&apos;itheima&apos;) print(result) # 获取键为name name = src.get(&apos;name&apos;) print(name) except Exception as e: print(e)","categories":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"Redis主从搭建","slug":"Redis主从搭建","date":"2019-03-15T03:57:22.091Z","updated":"2019-03-15T03:57:47.258Z","comments":true,"path":"2019/03/15/Redis主从搭建/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/Redis主从搭建/","excerpt":"","text":"Redis主从搭建主从概念⼀个master可以拥有多个slave，⼀个slave⼜可以拥有多个slave，如此下去，形成了强⼤的多级服务器集群架构master用来写数据，slave用来读数据，经统计：网站的读写比率是10:1通过主从配置可以实现读写分离master和slave都是一个redis实例(redis服务) 主从配置配置主查看当前主机的ip地址 config修改etc/redis/redis.conf文件 sudo vi redis.confbind 192.168.26.128 重启redis服务 sudo service redis stopredis-server redis.conf 配置从复制etc/redis/redis.conf文件 sudo cp redis.conf ./slave.conf 修改redis/slave.conf文件 sudo vi slave.conf 编辑内容 bind 192.168.26.128slaveof 192.168.26.128 6379port 6378 redis服务 sudo redis-server slave.conf 查看主从关系 redis-cli -h 192.168.26.128 info Replication ##数据操作在master和slave分别执⾏info命令，查看输出信息 进入主客户端 redis-cli -h 192.168.26.128 -p 6379 进入从的客户端 redis-cli -h 192.168.26.128 -p 6378 在master上写数据 get aa aa在slave上读数据 get aa","categories":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"Redis与Python交互","slug":"Redis与Python交互","date":"2019-03-15T03:27:29.930Z","updated":"2019-03-15T03:28:41.123Z","comments":true,"path":"2019/03/15/Redis与Python交互/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/Redis与Python交互/","excerpt":"","text":"Redis与Python交互StrictRedis对象⽅法 通过init创建对象，指定参数host、port与指定的服务器和端⼝连接，host默认为localhost，port默认为6379，db默认为01sr = StrictRedis(host=&apos;localhost&apos;, port=6379, db=0) 简写 sr=StrictRedis() 根据不同的类型，拥有不同的实例⽅法可以调⽤，与前⾯学的redis命令对应，⽅法需要的参数与命令的参数⼀致 string setsetexmsetappendgetmgetkeykeysexiststypedeleteexpiregetrangettl hash hsethmsethkeyshgethmgethvalshdel list lpushrpushlinsertlrangelsetlrem set saddsmemberssrem zset zaddzrangezrangebyscorezscorezremzremrangebyscore 使用StrictRedis对象对string类型数据进行增删改查在桌面上创建redis目录使用pycharm打开 redis目录创建redis_string.py文件12345678from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() except Exception as e: print(e) string-增加⽅法set，添加键、值，如果添加成功则返回True，如果添加失败则返回False编写代码如下1234567891011from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #添加键name，值为itheima result=sr.set(&apos;name&apos;,&apos;itheima&apos;) #输出响应结果，如果添加成功则返回True，否则返回False print(result) except Exception as e: print(e) string-获取⽅法get，添加键对应的值，如果键存在则返回对应的值，如果键不存在则返回None编写代码如下1234567891011from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #获取键name的值 result = sr.get(&apos;name&apos;) #输出键的值，如果键不存在则返回None print(result) except Exception as e: print(e) string-修改⽅法set，如果键已经存在则进⾏修改，如果键不存在则进⾏添加编写代码如下1234567891011from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #设置键name的值，如果键已经存在则进⾏修改，如果键不存在则进⾏添加 result = sr.set(&apos;name&apos;,&apos;itcast&apos;) #输出响应结果，如果操作成功则返回True，否则返回False print(result) except Exception as e: print(e) string-删除⽅法delete，删除键及对应的值，如果删除成功则返回受影响的键数，否则则返 回0编写代码如下1234567891011rom redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #设置键name的值，如果键已经存在则进⾏修改，如果键不存在则进⾏添加 result = sr.delete(&apos;name&apos;) #输出响应结果，如果删除成功则返回受影响的键数，否则则返回0 print(result) except Exception as e: print(e) 获取键⽅法keys，根据正则表达式获取键编写代码如下1234567891011from redis import *if __name__==&quot;__main__&quot;: try: #创建StrictRedis对象，与redis服务器建⽴连接 sr=StrictRedis() #获取所有的键 result=sr.keys() #输出响应结果，所有的键构成⼀个列表，如果没有键则返回空列表 print(result) except Exception as e: print(e)","categories":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"NoSql之Redis(2)","slug":"NoSql之Redis(2)","date":"2019-03-15T03:20:45.585Z","updated":"2019-03-15T03:28:22.598Z","comments":true,"path":"2019/03/15/NoSql之Redis(2)/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/NoSql之Redis(2)/","excerpt":"","text":"NoSql之Redis(2)服务器端和客户端的命令服务器端服务器端的命令为redis-server 可以使⽤help查看帮助⽂档 redis-server –help 个人习惯 ps aux | grep redis 查看redis服务器进程sudo kill -9 pid 杀死redis服务器sudo redis-server /etc/redis/redis.conf 指定加载的配置文件 客户端客户端的命令为redis-cli可以使⽤help查看帮助⽂档 redis-cli –help 连接redis redis-cli 运⾏测试命令 ping 切换数据库 数据库没有名称，默认有16个，通过0-15来标识，连接redis默认选择第一个数据库 select 10 数据结构redis是key-value的数据结构，每条数据都是⼀个键值对键的类型是字符串注意：键不能重复 值的类型分为五种： 字符串string哈希hash列表list集合set有序集合zset 数据操作行为 保存修改获取删除 点击中⽂官⽹查看命令⽂档http://redis.cn/commands.html string类型字符串类型是 Redis 中最为基础的数据存储类型，它在 Redis 中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据或Json对象描述信息等。在Redis中字符串类型的Value最多可以容纳的数据长度是512M。 保存如果设置的键不存在则为添加，如果设置的键已经存在则修改 设置键值 set key value 例1：设置键为name值为itcast的数据 set name itcast 设置键值及过期时间，以秒为单位 setex key seconds value 例2：设置键为aa值为aa过期时间为3秒的数据 setex aa 3 aa 设置多个键值 mset key1 value1 key2 value2 … 例3：设置键为’a1’值为’python’、键为’a2’值为’java’、键为’a3’值为’c’ mset a1 python a2 java a3 c 追加值 append key value 例4：向键为a1中追加值’ haha’ append ‘a1’ ‘haha’ 获取获取：根据键获取值，如果不存在此键则返回nil get key 例5：获取键’name’的值 get ‘name’ 根据多个键获取多个值 mget key1 key2 … 例6：获取键a1、a2、a3’的值 mget a1 a2 a3 键命令查找键，参数⽀持正则表达式 keys pattern 例1：查看所有键 keys * 例2：查看名称中包含a的键 keys ‘a*’ 判断键是否存在，如果存在返回1，不存在返回0 exists key1 例3：判断键a1是否存在 exists a1 查看键对应的value的类型 type key 例4：查看键a1的值类型，为redis⽀持的五种类型中的⼀种 type a1 删除键及对应的值 del key1 key2 … 例5：删除键a2、a3 del a2 a3设置过期时间，以秒为单位 如果没有指定过期时间则⼀直存在，直到使⽤DEL移除 expire key seconds 例6：设置键’a1’的过期时间为3秒 expire ‘a1’ 3查看有效时间，以秒为单位 ttl key 例7：查看键’bb’的有效时间 ttl bb hash类型hash⽤于存储对象，对象的结构为属性、值值的类型为string 增加、修改设置单个属性 hset key field value 例1：设置键 user的属性name为itheima hset user name itheima MISCONF Redis is configured to save RDB snapshots, but is currently not able to persist on disk. Commands that may modify the data set are disabled. Please check Redis logs for details about the error. Redis被配置为保存数据库快照，但它目前不能持久化到硬盘。用来修改集合数据的命令不能用 原因： 强制关闭Redis快照导致不能持久化。 解决方案： 运行config set stop-writes-on-bgsave-error no 命令后，关闭配置项stop-writes-on-bgsave-error解决该问题。 设置多个属性 hmset key field1 value1 field2 value2 … 例2：设置键u2的属性name为itcast、属性age为11 hmset u2 name itcast age 11 获取获取指定键所有的属性 hkeys key 例3：获取键u2的所有属性 hkeys u2 获取⼀个属性的值 hget key field 例4：获取键u2属性’name’的值 hget u2 ‘name’ 获取多个属性的值 hmget key field1 field2 … 例5：获取键u2属性’name’、’age的值 hmget u2 name age 获取所有属性的值 hvals key 例6：获取键’u2’所有属性的值 hvals u2 删除删除整个hash键及值，使⽤del命令删除属性，属性对应的值会被⼀起删除 hdel key field1 field2 … 例7：删除键’u2’的属性’age’ hdel u2 age list类型列表的元素类型为string按照插⼊顺序排序 增加在左侧插⼊数据 lpush key value1 value2 … 例1：从键为’a1’的列表左侧加⼊数据a 、 b 、c lpush a1 a b c 在右侧插⼊数据 rpush key value1 value2 … 例2：从键为’a1’的列表右侧加⼊数据0 1 rpush a1 0 1 在指定元素的前或后插⼊新元素 linsert key before或after 现有元素 新元素 例3：在键为’a1’的列表中元素’b’前加⼊’3’ linsert a1 before b 3&gt; 获取返回列表⾥指定范围内的元素 start、stop为元素的下标索引索引从左侧开始，第⼀个元素为0索引可以是负数，表示从尾部开始计数，如-1表示最后⼀个元素 lrange key start stop 例4：获取键为’a1’的列表所有元素 lrange a1 0 -1 设置指定索引位置的元素值索引从左侧开始，第⼀个元素为0索引可以是负数，表示尾部开始计数，如-1表示最后⼀个元素 lset key index value 例5：修改键为’a1’的列表中下标为1的元素值为’z’ lset a 1 z 删除删除指定元素 将列表中前count次出现的值为value的元素移除count &gt; 0: 从头往尾移除count &lt; 0: 从尾往头移除count = 0: 移除所有 lrem key count value 例6.1：向列表’a2’中加⼊元素’a’、’b’、’a’、’b’、’a’、’b’ lpush a2 a b a b a b例6.2：从’a2’列表右侧开始删除2个’b’ lrem a2 -2 b 例6.3：查看列表’py12’的所有元素 lrange a2 0 -1 set类型⽆序集合元素为string类型元素具有唯⼀性，不重复说明：对于集合没有修改操作 增加添加元素 sadd key member1 member2 … 例1：向键’a3’的集合中添加元素’zhangsan’、’lisi’、’wangwu’ sadd a3 zhangsan sili wangwu 获取返回所有的元素 smembers key 例2：获取键’a3’的集合中所有元素 smembers a3 删除删除指定元素 srem key 例3：删除键’a3’的集合中元素’wangwu’ srem a3 wangwu zset类型sorted set，有序集合元素为string类型元素具有唯⼀性，不重复每个元素都会关联⼀个double类型的score，表示权重，通过权重将元素从⼩到⼤排序说明：没有修改操作 增加添加 zadd key score1 member1 score2 member2 … 例1：向键’a4’的集合中添加元素’lisi’、’wangwu’、’zhaoliu’、’zhangsan’，权重分别为4、5、6、3 zadd a4 4 lisi 5 wangwu 6 zhaoliu 3 zhangsan 获取返回指定范围内的元素 start、stop为元素的下标索引 索引从左侧开始，第⼀个元素为0索引可以是负数，表示从尾部开始计数，如-1表示最后⼀个元素 zrange key start stop 例2：获取键’a4’的集合中所有元素 zrange a4 0 -1 返回score值在min和max之间的成员 zrangebyscore key min max 例3：获取键’a4’的集合中权限值在5和6之间的成员 zrangebyscore a4 5 6 返回成员member的score值 zscore key member 例4：获取键’a4’的集合中元素’zhangsan’的权重 zscore a4 zhangsan 删除指定元素 zrem key member1 member2 … 例5：删除集合’a4’中元素’zhangsan’ zrem a4 zhangsan删除权重在指定范围的元素 zremrangebyscore key min max 例6：删除集合’a4’中权限在5、6之间的元素 zremrangebyscore a4 5 6","categories":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"NoSql之Redis(1)","slug":"NoSql之Redis(1)","date":"2019-03-15T02:26:49.727Z","updated":"2019-03-15T02:29:07.208Z","comments":true,"path":"2019/03/15/NoSql之Redis(1)/","link":"","permalink":"https://zem12345678.github.io/2019/03/15/NoSql之Redis(1)/","excerpt":"","text":"NoSql之Redis(1)Nosql介绍NoSQL：一类新出现的数据库(not only sql)泛指非关系型的数据库不支持SQL语法存储结构跟传统关系型数据库中的那种关系表完全不同，nosql中存储的数据都是KV形式NoSQL的世界中没有一种通用的语言，每种nosql数据库都有自己的api和语法，以及擅长的业务场景NoSQL中的产品种类相当多： MongodbRedisHbase hadoopCassandra hadoop NoSQL和SQL数据库的比较： 适用场景不同：sql数据库适合用于关系特别复杂的数据查询场景，nosql反之“事务”特性的支持：sql对事务的支持非常完善，而nosql基本不支持事务两者在不断地取长补短，呈现融合趋势 Redis简介 Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。从2010年3月15日起，Redis的开发工作由VMware主持。从2013年5月开始，Redis的开发由Pivotal赞助。Redis是 NoSQL技术阵营中的一员，它通过多种键值数据类型来适应不同场景下的存储需求，借助一些高层级的接口使用其可以胜任，如缓存、队列系统的不同角色 Redis特性 Redis 与其他 key - value 缓存产品有以下三个特点：Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。Redis支持数据的备份，即master-slave模式的数据备份。 Redis 优势 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 Redis应用场景 用来做缓存(ehcache/memcached)——redis的所有数据是放在内存中的（内存数据库）可以在某些特定应用场景下替代传统数据库——比如社交类的应用在一些大型系统中，巧妙地实现一些特定的功能：session共享、购物车只要你有丰富的想象力，redis可以用在可以给你无限的惊喜……. Redis 安装当前redis最新稳定版本是4.0.9当前ubuntu虚拟机中已经安装好了redis，以下步骤可以跳过 最新稳定版本下载链接： http://download.redis.io/releases/redis-4.0.9.tar.gzstep1:下载 wget http://download.redis.io/releases/redis-4.0.9.tar.gz step2:解压 tar xzf redis-4.0.9.tar.gz step3:移动，放到usr/local⽬录下 sudo mv ./redis-4.0.9 /usr/local/redis/ step4:进⼊redis⽬录 cd /usr/local/redis/ step5:生成 sudo makestep7:安装,将redis的命令安装到/usr/local/bin/⽬录 sudo make install step8:安装完成后，我们进入目录/usr/local/bin中查看 cd /usr/local/binls -all redis-server redis服务器redis-cli redis命令行客户端redis-benchmark redis性能测试工具redis-check-aof AOF文件修复工具redis-check-rdb RDB文件检索工具 step9:配置⽂件，移动到/etc/⽬录下 配置⽂件⽬录为/usr/local/redis/redis.conf sudo cp /usr/local/redis/redis.conf /etc/redis/ Mac 上安装 Redis:安装 Homebrew：https://brew.sh/ 使用 brew 安装 Redishttps://www.cnblogs.com/cloudshadow/p/mac_brew_install_redis.html 配置Redis的配置信息在/etc/redis/redis.conf下。 查看 sudo vi /etc/redis/redis.conf 核心配置选项绑定ip：如果需要远程访问，可将此⾏注释，或绑定⼀个真实ip bind 127.0.0.1 端⼝，默认为6379 port 6379 是否以守护进程运⾏ 如果以守护进程运⾏，则不会在命令⾏阻塞，类似于服务如果以⾮守护进程运⾏，则当前终端被阻塞设置为yes表示守护进程，设置为no表示⾮守护进程推荐设置为yes daemonize yes 数据⽂件 dbfilename dump.rdb 数据⽂件存储路径 dir /var/lib/redis ⽇志⽂件 logfile “/var/log/redis/redis-server.log” 数据库，默认有16个 database 16 主从复制，类似于双机备份。 slaveof 参考资料redis配置信息http://blog.csdn.net/ljphilp/article/details/52934933","categories":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://zem12345678.github.io/tags/Redis/"}]},{"title":"Django 之Xadmin的使用","slug":"Django 之Xadmin的使用","date":"2019-03-14T15:26:30.264Z","updated":"2019-03-14T15:27:09.638Z","comments":true,"path":"2019/03/14/Django 之Xadmin的使用/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django 之Xadmin的使用/","excerpt":"","text":"Django 之Xadmin的使用xadmin是Django的第三方扩展，可是使Django的admin站点使用更方便。 1. 安装通过如下命令安装xadmin的最新版 pip install https://github.com/sshwsfc/xadmin/tarball/master 在配置文件中注册如下应用 1234567 INSTALLED_APPS = [ ... &apos;xadmin&apos;, &apos;crispy_forms&apos;, &apos;reversion&apos;, ...] xadmin有建立自己的数据库模型类，需要进行数据库迁移 python manage.py migrate在总路由中添加xadmin的路由信息1234567import xadminurlpatterns = [ # url(r&apos;^admin/&apos;, admin.site.urls), url(r&apos;xadmin/&apos;, include(xadmin.site.urls)), ...] 2. 使用xadmin不再使用Django的admin.py，而是需要编写代码在adminx.py文件中。xadmin的站点管理类不用继承admin.ModelAdmin，而是直接继承object即可。在goods应用中创建adminx.py文件。 站点的全局配置12345678910111213141516171819import xadminfrom xadmin import viewsfrom goods import modelsclass BaseSetting(object): &quot;&quot;&quot;xadmin的基本配置&quot;&quot;&quot; enable_themes = True # 开启主题切换功能 use_bootswatch = Truexadmin.site.register(views.BaseAdminView, BaseSetting)class GlobalSettings(object): &quot;&quot;&quot;xadmin的全局配置&quot;&quot;&quot; site_title = &quot;美多商城运营管理系统&quot; # 设置站点标题 site_footer = &quot;美多商城集团有限公司&quot; # 设置站点的页脚 menu_style = &quot;accordion&quot; # 设置菜单折叠xadmin.site.register(views.CommAdminView, GlobalSettings) 站点Model管理xadmin可以使用的页面样式控制基本与Django原生的admin一直。 list_display 控制列表展示的字段search_fields 控制可以通过搜索框搜索的字段名称，xadmin使用的是模糊查询list_filter 可以进行过滤操作的列ordering 默认排序的字段readonly_fields 在编辑页面的只读字段exclude 在编辑页面隐藏的字段list_editable 在列表页可以快速直接编辑的字段show_detail_fileds 在列表页提供快速显示详情信息refresh_times 指定列表页的定时刷新list_export 控制列表页导出数据的可选格式data_charts *控制显示图标的样式model_icon 控制菜单的图标1）model_icon1234class SKUAdmin(object): model_icon = &apos;fa fa-gift&apos;xadmin.site.register(models.SKU, SKUAdmin) 可选的图标样式参考http://fontawesome.dashgame.com/2） list_display1list_display = [&apos;id&apos;, &apos;name&apos;, &apos;price&apos;, &apos;stock&apos;, &apos;sales&apos;, &apos;comments&apos;] 3）search_fields1search_fields = [&apos;id&apos;,&apos;name&apos;] 4）list_filter1list_filter = [&apos;category&apos;] 5）list_editable 1list_editable = [&apos;price&apos;, &apos;stock&apos;] 6）show_detail_fields 1show_detail_fields = [&apos;name&apos;] 7）list_export 1list_export = [&apos;xls&apos;, &apos;csv&apos;, &apos;xml&apos;] 8）refresh_times 123456lass OrderAdmin(object): list_display = [&apos;order_id&apos;, &apos;create_time&apos;, &apos;total_amount&apos;, &apos;pay_method&apos;, &apos;status&apos;] refresh_times = [3, 5] # 可选以支持按多长时间(秒)刷新页面``` ![enter image description here](https://ww1.sinaimg.cn/large/007rAy9hly1g12qk7eyyuj30lq04dt9e.jpg)9）data_charts data_charts = { &quot;order_amount&quot;: {&apos;title&apos;: &apos;订单金额&apos;, &quot;x-field&quot;: &quot;create_time&quot;, &quot;y-field&quot;: (&apos;total_amount&apos;,), &quot;order&quot;: (&apos;create_time&apos;,)}, &quot;order_count&quot;: {&apos;title&apos;: &apos;订单量&apos;, &quot;x-field&quot;: &quot;create_time&quot;, &quot;y-field&quot;: (&apos;total_count&apos;,), &quot;order&quot;: (&apos;create_time&apos;,)}, } 12345678&gt;title 控制图标名称x-field 控制x轴字段y-field 控制y轴字段，可以是多个值order 控制默认排序![enter image description here](https://ww1.sinaimg.cn/large/007rAy9hgy1g12qld0x5qj30la07vjru.jpg)10）readonly_fields class SKUAdmin(object): … readonly_fields = [‘sales’, ‘comments’]12345678910111213![enter image description here](https://ww1.sinaimg.cn/large/007rAy9hgy1g12qm5ep5sj30l704f744.jpg)站点保存对象数据方法重写在Django的原生admin站点中，如果想要在站点保存或删除数据时，补充自定义行为，可以重写如下方法：&gt;save_model(self, request, obj, form, change)delete_model(self, request, obj)而在xadmin中，需要重写如下方法：&gt;save_models(self)delete_model(self)在方法中，如果需要用到当前处理的模型类对象，需要通过self.obj来获取，如 class SKUSpecificationAdmin(object): def save_models(self): # 保存数据对象 obj = self.new_obj obj.save() # 补充自定义行为 from celery_tasks.html.tasks import generate_static_sku_detail_html generate_static_sku_detail_html.delay(obj.sku.id) def delete_model(self): # 删除数据对象 obj = self.obj sku_id = obj.sku.id obj.delete() # 补充自定义行为 from celery_tasks.html.tasks import generate_static_sku_detail_html generate_static_sku_detail_html.delay(sku_id) 123456### 自定义用户管理xadmin会自动为admin站点添加用户User的管理配置xadmin使用xadmin.plugins.auth.UserAdmin来配置如果需要自定义User配置的话，需要先unregister(User)，在添加自己的User配置并注册 import xadmin Register your models here.from users.models import Userfrom xadmin.plugins import auth class UserAdmin(auth.UserAdmin): list_display = [‘id’, ‘username’, ‘mobile’, ‘email’, ‘date_joined’] readonly_fields = [‘last_login’, ‘date_joined’] search_fields = (‘username’, ‘first_name’, ‘last_name’, ‘email’, ‘mobile’) style_fields = {‘user_permissions’: ‘m2m_transfer’, ‘groups’: ‘m2m_transfer’} def get_model_form(self, **kwargs): if self.org_obj is None: self.fields = [&apos;username&apos;, &apos;mobile&apos;, &apos;is_staff&apos;] return super().get_model_form(**kwargs) xadmin.site.unregister(User)xadmin.site.register(User, UserAdmin)`","categories":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/categories/Django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"}]},{"title":"使用近似算法解决旅行商（TSP）问题","slug":"使用近似算法解决旅行商（TSP）问题","date":"2019-03-14T14:49:14.098Z","updated":"2019-03-14T14:50:40.372Z","comments":true,"path":"2019/03/14/使用近似算法解决旅行商（TSP）问题/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/使用近似算法解决旅行商（TSP）问题/","excerpt":"","text":"使用近似算法解决旅行商（TSP）问题什么是TSP问题？谓TSP问题（Travelling Salesman Problem）旅行商问题，即最短路径问题，就是在给定的起始点S到终止点T的通路集合中，寻求距离最小的通路，这样的通路称为S点到T点的最短路径。TSP是一种完全NP问题如果旅行商问题的权值函数满足三角不等式，即c(u,w)≤c(u,v) + c(v,w)对任意u,v,w都成立，则称它满足三角不等式。无论旅行商问题是否满足三角不等式，它均是NP-完全问题。相关定理表明，不满足三角不等式的旅行商问题不存在常数近似比的近似算法，除非NP=P。 近似算法设计近似算法是指能够在多项式时间内给出优化问题的近似优化解的算法，近似算法不仅可用于近似求解NP-完全问题，也可用于近似求解复杂度较高的P问题。 任意选择V中的一个顶点r，作为树根节点; 调用Prim算法得到图G(V,E)的最小生成树T*; 先序遍历T，访问T中的每条边两遍，得到顶点序列L; 删除L中的重复顶点形成哈密顿环C； 输出C. 算法的性能分析 时间复杂度近似算法的性能分析包括时间复杂度分析、空间复杂度分析和近似精度分析，其中时间（空间）复杂度的分析同精确复杂度相同。近似精度分析是近似算法特有的，它主要用于刻画近似算法给出的近似解相比于问题优化解的优劣程度。目前，存在三种刻画近似精度的度量，即近似比、相对误差界和1+ε近似。 该算法的时间复杂度为O(|V|^2 log|V|)。事实上，第2步开销为O(|E| log|V|)且图G是完全图，O(|E| log|V|)等于O(|V|^2 log|V|)。第3~4步的开销为O(|V|)，因为最小生成树恰有|V| - 1条边。 近似精度近似比：设A是一个优化问题的近似算法，A具有近似比（ratio bound） p(n), 如果max{C/C, C/C} ≤ p(n)。其中n是输入大小，C是A产生的解的代价，C是优化解的代价。相对误差：对于任意输入，近似算法的相对误差定义为|C - C|/C,其中C是近似解的代价，C是优化解的代价。相对误差界：一个近似算法的相对误差界为ε(n),如果|C-C|/C ≤ ε(n)。近似模式：一个优化问题的近似模式是一个以问题实例I和ε&gt;0位输入的算法。对于任意固定的ε，近似模式是一个(1+ε)-近似算法。一个近似模式A(I,ε)称为一个多项式时间近似模式，如果对于任意ε&gt;0, A(I,ε)的运行时间是|I|的多项式。一个近似模式称为完全多项式时间近似模式，如果它的运行时间是关于I/ε和输入实例大小n的多项式。对于近似解C，由于L是遍历T的每条边两次得到的回路，则有c(L) = 2 c(T), 而C又是关于L删除某些重复边后得到的结果，因此C ≤ 2 c(T)。 对于优化解C，由于C是一个简单环，则删除任一条边便可生成树，而且该树的代价一定不低于最小生成树的代价，因此有c(C) ≥ c(T)。综上，有C ≤ 2 c(T) ≤ 2 C。所以C / C ≤ 2，该算法的近似比为2。 近似算法解决TSP问题过程TSP问题的实质可以抽象为在一个带权重的完全无向图中，找到一个权值总和最小的哈密顿回路TSP问题翻译为数学语言为，在N个城市的完全无向图G中 其中每个城市之间的距离矩阵为 目标函数为 需要求解的变量为w，w是使得目标函数达到最小值的一个排列 且w的最后一项满足回到出发城市 满足三角不等式的TSP模型和算法步骤我们从费用函数出发，费用函数也叫代价函数，指的是两个城市之间的费用指数或者代价程度的量化。在大多数的实际情况中，从一个地方u直接到另一个地方w，这个走法花费的代价总是最小的，而如果从u到w需要经过某个中转站v，则这种走法花费的代价却不可能比直接到达的走法花费的代价更小将上述的理论转化为数学语言为其中c是费用函数，这个方程说明了，直接从u-&gt;w花费的代价，要比从u-&gt;v-&gt;w花费的代价要小，我们称这个费用函数满足三角不等式三角不等式的定义为：任意一个欧拉平面的三角形两边之和始终大于第三边，这是一个非常自然的不等式，其中欧拉平面上任意两点之间的欧式距离就满足三角不等式，为此，我们只要设TSP中的费用函数为欧式距离，即可将TSP问题转化为满足三角不等式的TSP模型 近似算法的解题步骤求解上述TSP模型的步骤（1）选择G的任意一个顶点r作为根节点(出发/结束点)（2）用Prim算法找出G的一棵以r为根的最小生成树T（3）前序遍历访问树T，得到遍历顺序组成的顶点表L（4）将r加到顶点表L的末尾，按L中顶点的次序组成哈密顿回路H数学上已经证明，当费用函数满足三角不等式时，上述近似算法找出的哈密顿回路产生的总费用，不会超过最优回路的2倍 图的存储结构首先我们需要将图表示为我们熟悉的数据结构，图可以使用两种存储结构，分别是邻接链表和邻接矩阵邻接链表：是一个由链表组成的一维数组，数组中每个元素都存储以每个顶点为表头的链表邻接矩阵：以矩阵的形式存储图中所有顶点之间的关系用链表表示图的关系，会显得数据结构较为复杂，但节省空间的开销，而用矩阵来表示图的关系就显得非常清晰，但空间开销较大，这里我们选择邻接矩阵来表示TSP案例中的无向图G我们设欧式距离为费用函数，矩阵中的每一行代表G中每一个的顶点到其余各个顶点的费用(欧式距离)，如果出现到达不了或者自身到达自身的情况，我们用无穷大inf来填充表示不可达123456789101112def price_cn(vec1, vec2): return np.linalg.norm(np.array(vec1) - np.array(vec2))# 从去过的点中，找到连接到未去过的点的边里，最小的代价边(贪心算法)def find_min_edge(visited_ids, no_visited_ids): min_weight, min_from, min_to = np.inf, np.inf, np.inf for from_index in visited_ids: for to_index, weight in enumerate(G[from_index]): if from_index != to_index and weight &lt; min_weight and to_index in no_visited_ids: min_to = to_index min_from = from_index min_weight = G[min_from][min_to] return (min_from, min_to), min_weight Prim最小生成树算法两点可以确定一条直线，则最小生成树的定义为：用n-1条边连接具有n个顶点的无向图G，并且使得边长的总和最小接下来我们需要找到G中的这颗最小生成树T，从T的定义可知，T满足（1）T有且只有n-1条边（2）T的总长度达到最小值这里我们使用Prim算法来生成T，Prim算法的策略步骤为（1）设集合V是G中所有的顶点，集合U是G中已经走过的顶点，集合U-V是G中没有走过的顶点（2）从G中的起点a开始遍历，将a加入到集合U中，并将a从集合U-V替出（3）在集合U-V中剩余的n-1个顶点中寻找与集合U中的a关联，且权重最小的那条边的终点b，将b加入到集合U中，并将b从集合U-V替出（4）同理，在集合U-V中剩余的n-2个顶点中寻找与集合U中的a或b关联，且权重最小的那条边的终点c，将c加入到集合U中，并将c从集合U-V替出（5）重复步骤(4)，直到G中所有的顶点都加入到集合U，且集合U-V为空，则集合U中的顶点就构成了T显然，Prim算法的策略属于贪心算法，因为每一步所加入的边，都必须是使得当前数T的总权重增加量最小的边12345678910111213def prim(G, root_index=0): visited_ids = [root_index] # 初始化去过的点的集合 T_path = [] while len(visited_ids) != G.shape[0]: no_visited_ids = contain_no_visited_ids(G, visited_ids) # 维护未去过的点的集合 (min_from, min_to), min_weight = find_min_edge(visited_ids, no_visited_ids) visited_ids.append(min_to) # 维护去过的点的集合 T_path.append((min_from, min_to)) T = np.full_like(G, np.inf) # 最小生成树的矩阵形式，n-1条边组成 for (from_, to_) in T_path: T[from_][to_] = G[from_][to_] T[to_][from_] = G[to_][from_] return T, T_path 遍历序遍历：访问根节点—&gt;前序遍历左子树—&gt;前序遍历右子树123456789101112def preorder_tree_walk(T, root_index=0): is_visited = [False] * T.shape[0] stack = [root_index] T_walk = [] while len(stack) != 0: node = stack.pop() T_walk.append(node) is_visited[node] = True nodes = np.where(T[node] != np.inf)[0] if len(nodes) &gt; 0: [stack.append(node) for node in reversed(nodes) if is_visited[node] is False] return T_walk 哈密顿回路哈密顿回路的定义为：由指定的起点前往指定的终点，途中经过的城市有且只经过一次，所以一个无向图中含有若干个哈密顿回路按照近似算法的最后一步，我们将根节点加入到顶点表的末尾，将顶点表的顶点顺序依次连接，就得到哈密顿回路123456789def create_H(G, L): H = np.full_like(G, np.inf) H_path = [] for i, from_node in enumerate(L[0:-1]): to_node = L[i + 1] H[from_node][to_node] = G[from_node][to_node] H[to_node][from_node] = G[to_node][from_node] H_path.append((from_node, to_node)) return H, H_path 完成了近似算法的计算，找到了一个在G中从a出发，最后回到a，中间每个城市只经过一次的最小费用的行程走法，即计算出了目标函数的顶点排列w为 结果分析从结果上可以看出，近似算法是解决TSP问题的一种有效方法，它可以在多项式时间内计算出一个近似解，来逼近真实的最优解，这个近似解尽量的逼近满足TSP的条件（1）从开始点回到开始点，每个点都要经过且只经过一次（2）行程的总费用达到最小值近似算法求解TSP问题（3）近似算法是求解TSP问题的一个渐进式算法（4）近似解法求出的近似解和实际最优解的近似比不超过2，即w的总代价，在最优总代价的2倍之内 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&apos;&apos;&apos;TSP—近似算法1、选择G的任意一个顶点2、Prim算法找出找出最小生成树T3、前序遍历树T得到的顶点表L4、将根节点添加到L的末尾，按表L中顶点的次序组成哈密顿回路H&apos;&apos;&apos;import numpy as npimport matplotlib.pyplot as plt# 代价函数（具有三角不等式性质）def price_cn(vec1, vec2): return np.linalg.norm(np.array(vec1) - np.array(vec2))# 从去过的点中，找到连接到未去过的点的边里，最小的代价边(贪心算法)def find_min_edge(visited_ids, no_visited_ids): min_weight, min_from, min_to = np.inf, np.inf, np.inf for from_index in visited_ids: for to_index, weight in enumerate(G[from_index]): if from_index != to_index and weight &lt; min_weight and to_index in no_visited_ids: min_to = to_index min_from = from_index min_weight = G[min_from][min_to] return (min_from, min_to), min_weight# 维护未走过的点的集合def contain_no_visited_ids(G, visited_ids): no_visited_ids = [] # 还没有走过的点的索引集合 [no_visited_ids.append(idx) for idx, _ in enumerate(G) if idx not in visited_ids] return no_visited_ids# 生成最小生成树Tdef prim(G, root_index=0): visited_ids = [root_index] # 初始化去过的点的集合 T_path = [] while len(visited_ids) != G.shape[0]: no_visited_ids = contain_no_visited_ids(G, visited_ids) # 维护未去过的点的集合 (min_from, min_to), min_weight = find_min_edge(visited_ids, no_visited_ids) visited_ids.append(min_to) # 维护去过的点的集合 T_path.append((min_from, min_to)) T = np.full_like(G, np.inf) # 最小生成树的矩阵形式，n-1条边组成 for (from_, to_) in T_path: T[from_][to_] = G[from_][to_] T[to_][from_] = G[to_][from_] return T, T_path# 先序遍历图(最小生成树)的路径，得到顶点列表Ldef preorder_tree_walk(T, root_index=0): is_visited = [False] * T.shape[0] stack = [root_index] T_walk = [] while len(stack) != 0: node = stack.pop() T_walk.append(node) is_visited[node] = True nodes = np.where(T[node] != np.inf)[0] if len(nodes) &gt; 0: [stack.append(node) for node in reversed(nodes) if is_visited[node] is False] return T_walk# 生成哈密尔顿回路Hdef create_H(G, L): H = np.full_like(G, np.inf) H_path = [] for i, from_node in enumerate(L[0:-1]): to_node = L[i + 1] H[from_node][to_node] = G[from_node][to_node] H[to_node][from_node] = G[to_node][from_node] H_path.append((from_node, to_node)) return H, H_path# 可视化画出哈密顿回路def draw_H(citys, H_path): fig = plt.figure() ax = fig.add_subplot(111) plt.xlim(0, 7) plt.ylim(0, 7) for (from_, to_) in H_path: p1 = plt.Circle(citys[from_], 0.2, color=&apos;red&apos;) p2 = plt.Circle(citys[to_], 0.2, color=&apos;red&apos;) ax.add_patch(p1) ax.add_patch(p2) ax.plot((citys[from_][0], citys[to_][0]), (citys[from_][1], citys[to_][1]), color=&apos;red&apos;) ax.annotate(s=chr(97 + to_), xy=citys[to_], xytext=(-8, -4), textcoords=&apos;offset points&apos;, fontsize=20) ax.axis(&apos;equal&apos;) ax.grid() plt.show()if __name__ == &apos;__main__&apos;: citys = [(2, 6), (2, 4), (1, 3), (4, 6), (5, 5), (4, 4), (6, 4), (3, 2)] # 城市坐标 G = [] # 完全无向图 for i, curr_point in enumerate(citys): line = [] for j, other_point in enumerate(citys): line.append(price_cn(curr_point, other_point)) if i != j else line.append(np.inf) G.append(line) G = np.array(G) # 1、选择G的任意一个顶点 root_index = 0 # 2、Prim算法找出找出最小生成树T T, T_path = prim(G, root_index=root_index) # 3、前序遍历树T得到的顶点表L L = preorder_tree_walk(T, root_index=root_index) # 4、将根节点添加到L的末尾，按表L中顶点的次序组成哈密顿回路H L.append(root_index) H, H_path = create_H(G, L) print(&apos;最小生成树的路径为：&#123;&#125;&apos;.format(T_path)) [print(chr(97 + v), end=&apos;,&apos; if i &lt; len(L) - 1 else &apos;\\n&apos;) for i, v in enumerate(L)] print(&apos;哈密顿回路的路径为：&#123;&#125;&apos;.format(H_path)) print(&apos;哈密顿回路产生的代价为：&#123;&#125;&apos;.format(sum(G[from_][to_] for (from_, to_) in H_path))) # draw_H(citys, H_path)","categories":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/categories/算法/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"数据结构","slug":"数据结构","permalink":"https://zem12345678.github.io/tags/数据结构/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://zem12345678.github.io/tags/LeetCode/"}]},{"title":"Logistic回归对鸾尾花进行分类","slug":"Logistic回归对鸾尾花进行分类","date":"2019-03-14T14:22:10.666Z","updated":"2019-03-15T01:11:54.870Z","comments":true,"path":"2019/03/14/Logistic回归对鸾尾花进行分类/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Logistic回归对鸾尾花进行分类/","excerpt":"","text":"逻辑斯蒂回归对鸾尾花进行分类Sigmoid函数逻辑回归也被称为广义线性回归模型，它与线性回归模型的形式基本上相同，都具有 ax+b，其中a和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将ax+b作为因变量，即y = ax+b，而logistic回归则通过函数S将ax+b对应到一个隐状态p，p = S(ax+b)，然后根据p与1-p的大小决定因变量的值。这里的函数S就是Sigmoid函数 IRIS数据集介绍Iris也称鸢尾花卉数据集,是常用的分类实验数据集，由R.A. Fisher于1936年收集整理的。其中包含3种植物种类，分别是山鸢尾（setosa）变色鸢尾（versicolor）和维吉尼亚鸢尾（virginica），每类50个样本，共150个样本。 该数据集包含4个特征变量，1个类别变量。iris每个样本都包含了4个特征：花萼长度，花萼宽度，花瓣长度，花瓣宽度，以及1个类别变量（label）。我们需要建立一个分类器，分类器可以通过这4个特征来预测鸢尾花卉种类是属于山鸢尾，变色鸢尾还是维吉尼亚鸢尾。其中有一个类别是线性可分的，其余两个类别线性不可分，这在最后的分类结果绘制图中可观察到。 导入所需的包12345678import numpy as npfrom sklearn.linear_model import LogisticRegressionimport matplotlib.pyplot as pltimport matplotlib as mplfrom sklearn import preprocessingimport pandas as pdfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipeline pandas进行数据预处理12345678data = pd.read_csv(path, header=None) iris_types = data[4].unique() for i, type in enumerate(iris_types): data.set_value(data[4] == type, 4, i) x, y = np.split(data.values, (4,), axis=1) x = x.astype(np.float) y = y.astype(np.int) # 仅使用前两列特征 导入模型，调用逻辑回归LogisticRegression()函数。训练LogisticRegression分类器123456789101112131415161718x = x[:, :2] lr = Pipeline([(&apos;sc&apos;, StandardScaler()), (&apos;clf&apos;, LogisticRegression()) ]) lr.fit(x, y.ravel()) y_hat = lr.predict(x) y_hat_prob = lr.predict_proba(x) np.set_printoptions(suppress=True) print (&apos;y_hat = \\n&apos;, y_hat) print (&apos;y_hat_prob = \\n&apos;, y_hat_prob) print (u&apos;准确度：%.2f%%&apos; % (100*np.mean(y_hat == y.ravel()))) # 画图 N, M = 500, 500 # 横纵各采样多少个值 x1_min, x1_max = x[:, 0].min(), x[:, 0].max() # 第0列的范围 x2_min, x2_max = x[:, 1].min(), x[:, 1].max() # 第1列的范围 t1 = np.linspace(x1_min, x1_max, N) t2 = np.linspace(x2_min, x2_max, M) x1, x2 = np.meshgrid(t1, t2) # 生成网格采样点 x_test = np.stack((x1.flat, x2.flat), axis=1) # 测试点 训练结果可视化12345678910111213141516mpl.rcParams[&apos;font.sans-serif&apos;] = [u&apos;simHei&apos;] mpl.rcParams[&apos;axes.unicode_minus&apos;] = False cm_light = mpl.colors.ListedColormap([&apos;#77E0A0&apos;, &apos;#FF8080&apos;, &apos;#A0A0FF&apos;]) cm_dark = mpl.colors.ListedColormap([&apos;g&apos;, &apos;r&apos;, &apos;b&apos;]) y_hat = lr.predict(x_test) # 预测值 y_hat = y_hat.reshape(x1.shape) # 使之与输入的形状相同 plt.figure(facecolor=&apos;w&apos;) plt.pcolormesh(x1, x2, y_hat, cmap=cm_light) # 预测值的显示 plt.scatter(x[:, 0], x[:, 1], c=np.squeeze(y), edgecolors=&apos;k&apos;, s=50, cmap=cm_dark) # 样本的显示 plt.xlabel(u&apos;花萼长度&apos;, fontsize=14) plt.ylabel(u&apos;花萼宽度&apos;, fontsize=14) plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.grid() plt.title(u&apos;鸢尾花Logistic回归分类效果 - 标准化&apos;, fontsize=17) plt.show()","categories":[{"name":"Machine learning","slug":"Machine-learning","permalink":"https://zem12345678.github.io/categories/Machine-learning/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/tags/算法/"},{"name":"回归分类","slug":"回归分类","permalink":"https://zem12345678.github.io/tags/回归分类/"},{"name":"Machine learning","slug":"Machine-learning","permalink":"https://zem12345678.github.io/tags/Machine-learning/"}]},{"title":"Django Restful Framework(DRF)的开发思考（3）","slug":"Django Restful Framework(DRF)的开发思考（3）","date":"2019-03-14T13:24:04.046Z","updated":"2019-03-14T13:24:46.627Z","comments":true,"path":"2019/03/14/Django Restful Framework(DRF)的开发思考（3）/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django Restful Framework(DRF)的开发思考（3）/","excerpt":"","text":"Django Restful Framework(DRF)的开发思考（3）认证&amp;权限,限流,过滤&amp;排序 , 分页 ，异常 1）认证&amp;权限2）限流控制用户访问API接口的频率。 针对匿名用户和认证用户分别进行限流。 1234567891011# 限流(针对匿名用户和认证用户分别进行限流控制)'DEFAULT_THROTTLE_CLASSES': ( 'rest_framework.throttling.AnonRateThrottle', # 针对匿名用户 'rest_framework.throttling.UserRateThrottle' # 针对认证用户),# 限流频次设置'DEFAULT_THROTTLE_RATES': &#123; 'user': '5/minute', # 认证用户5次每分钟 'anon': '3/minute', # 匿名用户3次每分钟&#125;, 针对匿名用户和认证用户统一进行限流。 123456789# 限流(针对匿名用户和认证用户进行统一限流控制)'DEFAULT_THROTTLE_CLASSES': ( 'rest_framework.throttling.ScopedRateThrottle',),'DEFAULT_THROTTLE_RATES': &#123; 'contacts': '5/minute', 'upload': '3/minute',&#125;, 3）过滤&amp;排序4）分页两种分页方式PageNumberPagination和LimitOffsetPagination。 使用PageNumberPagination分页时，获取分页数据时可以通过page传递页码参数。如果想要分页时指定页容量，需要自定义分页类。 1234567class StandardResultPagination(PageNumberPagination): # 默认页容量 page_size = 3 # 指定页容量参数名称 page_size_query_param = 'page_size' # 最大页容量 max_page_size = 5 使用LimitOffsetPagination分页时，获取分页数据时可以传递参数offset(偏移量)和limit(限制条数)。 注：如果使用的全局分页设置，某个列表视图如果不需要分页，直接在视图类中设置pagination_class = None。 5）异常DRF自带异常处理功能，可以对某些特定的异常进行处理并返回给客户端组织好的错误信息。能够处理的异常如下: 12345678910APIException 所有异常的父类ParseError 解析错误AuthenticationFailed 认证失败NotAuthenticated 尚未认证PermissionDenied 权限决绝NotFound 未找到MethodNotAllowed 请求方式不支持NotAcceptable 要获取的数据格式不支持Throttled 超过限流次数ValidationError 校验失败 可以自定义DRF框架的异常处理函数(补充一些异常处理)并指定EXCEPTION_HANDLER配置项。","categories":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/categories/前后端分离/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Restful","slug":"Restful","permalink":"https://zem12345678.github.io/tags/Restful/"},{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/tags/前后端分离/"}]},{"title":"Django REST framework JWT","slug":"Django REST framework JWT","date":"2019-03-14T13:05:55.099Z","updated":"2019-03-14T13:06:46.855Z","comments":true,"path":"2019/03/14/Django REST framework JWT/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django REST framework JWT/","excerpt":"","text":"Django REST framework JWT我们在验证完用户的身份后（检验用户名和密码），需要向用户签发JWT，在需要用到用户身份信息的时候，还需核验用户的JWT。 关于签发和核验JWT，我们可以使用Django REST framework JWT扩展来完成。 文档网站http://getblimp.github.io/django-rest-framework-jwt/ 安装配置安装 pip install djangorestframework-jwt 配置1234567891011REST_FRAMEWORK = &#123; &apos;DEFAULT_AUTHENTICATION_CLASSES&apos;: ( &apos;rest_framework_jwt.authentication.JSONWebTokenAuthentication&apos;, &apos;rest_framework.authentication.SessionAuthentication&apos;, &apos;rest_framework.authentication.BasicAuthentication&apos;, ),&#125;JWT_AUTH = &#123; &apos;JWT_EXPIRATION_DELTA&apos;: datetime.timedelta(days=1),&#125; JWT_EXPIRATION_DELTA 指明token的有效期 使用Django REST framework JWT 扩展的说明文档中提供了手动签发JWT的方法1234567from rest_framework_jwt.settings import api_settingsjwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLERjwt_encode_handler = api_settings.JWT_ENCODE_HANDLERpayload = jwt_payload_handler(user)token = jwt_encode_handler(payload) 在注册成功后，连同返回token，需要在注册视图中创建token。 修改CreateUserSerializer序列化器，在create方法中增加手动创建token的方法123456789101112131415161718192021222324252627282930313233343536from rest_framework_jwt.settings import api_settingsclass CreateUserSerializer(serializers.ModelSerializer): &quot;&quot;&quot; 创建用户序列化器 &quot;&quot;&quot; ... token = serializers.CharField(label=&apos;登录状态token&apos;, read_only=True) # 增加token字段 class Meta： ... fields = (&apos;id&apos;, &apos;username&apos;, &apos;password&apos;, &apos;password2&apos;, &apos;sms_code&apos;, &apos;mobile&apos;, &apos;allow&apos;, &apos;token&apos;) # 增加token ... def create(self, validated_data): &quot;&quot;&quot; 创建用户 &quot;&quot;&quot; # 移除数据库模型类中不存在的属性 del validated_data[&apos;password2&apos;] del validated_data[&apos;sms_code&apos;] del validated_data[&apos;allow&apos;] user = super().create(validated_data) # 调用django的认证系统加密密码 user.set_password(validated_data[&apos;password&apos;]) user.save() # 补充生成记录登录状态的token jwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLER jwt_encode_handler = api_settings.JWT_ENCODE_HANDLER payload = jwt_payload_handler(user) token = jwt_encode_handler(payload) user.token = token return user 前端保存token我们可以将JWT保存在cookie中，也可以保存在浏览器的本地存储里，我们保存在浏览器本地存储中 浏览器的本地存储提供了sessionStorage 和 localStorage 两种： sessionStorage 浏览器关闭即失效localStorage 长期有效使用方法1234567sessionStorage.变量名 = 变量值 // 保存数据sessionStorage.变量名 // 读取数据sessionStorage.clear() // 清除所有sessionStorage保存的数据localStorage.变量名 = 变量值 // 保存数据localStorage.变量名 // 读取数据localStorage.clear() // 清除所有localStorage保存的数据 在前端js/register.js文件中增加保存token12345678910111213141516171819var vm = new Vue(&#123; ... methods: &#123; ... on_submit: function()&#123; axios.post(...) .then(response =&gt; &#123; // 记录用户的登录状态 sessionStorage.clear(); localStorage.clear(); localStorage.token = response.data.token; localStorage.username = response.data.username; localStorage.user_id = response.data.id; location.href = &apos;/index.html&apos;; &#125;) .catch(...) &#125; &#125;&#125;)","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Jwt","slug":"Jwt","permalink":"https://zem12345678.github.io/tags/Jwt/"},{"name":"Token","slug":"Token","permalink":"https://zem12345678.github.io/tags/Token/"}]},{"title":"FastDFS客户端与自定义文件存储系统","slug":"FastDFS客户端与自定义文件存储系统","date":"2019-03-14T12:56:29.267Z","updated":"2019-03-14T12:57:39.281Z","comments":true,"path":"2019/03/14/FastDFS客户端与自定义文件存储系统/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/FastDFS客户端与自定义文件存储系统/","excerpt":"","text":"FastDFS客户端与自定义文件存储系统1. FastDFS的Python客户端python版本的FastDFS客户端使用说明参考https://github.com/jefforeilly/fdfs_client-py 安装安装fdfs_client-py-master.zip到虚拟环境中 pip install fdfs_client-py-master.zippip install mutagenpip install requests 使用使用FastDFS客户端，需要有配置文件。我们在meiduo_mall/utils目录下新建fastdfs目录，将提供给大家的client.conf配置文件放到这个目录中。需要修改一下client.conf配置文件12base_path=FastDFS客户端存放日志文件的目录tracker_server=运行tracker服务的机器ip:22122 上传文件需要先创建fdfs_client.client.Fdfs_client的对象，并指明配置文件，如12from fdfs_client.client import Fdfs_clientclient = Fdfs_client(&apos;meiduo_mall/utils/fastdfs/client.conf&apos;) 通过创建的客户端对象执行上传文件的方法123client.upload_by_filename(文件名)或client.upload_by_buffer(文件bytes数据) 如：1234567&gt;&gt;&gt; ret = client.upload_by_filename(&apos;/Users/delron/Desktop/1.png&apos;)getting connection&lt;fdfs_client.connection.Connection object at 0x1098d4cc0&gt;&lt;fdfs_client.fdfs_protol.Tracker_header object at 0x1098d4908&gt;&gt;&gt;&gt; ret&#123;&apos;Group name&apos;: &apos;group1&apos;, &apos;Remote file_id&apos;: &apos;group1/M00/00/02/CtM3BVr-k6SACjAIAAJctR1ennA809.png&apos;, &apos;Status&apos;: &apos;Upload successed.&apos;, &apos;Local file name&apos;: &apos;/Users/delron/Desktop/1.png&apos;, &apos;Uploaded size&apos;: &apos;151.00KB&apos;, &apos;Storage IP&apos;: &apos;10.211.55.5&apos;&#125;&gt;&gt;&gt; Remote file_id 即为FastDFS保存的文件的路径 2. 自定义Django文件存储系统在学习Django框架的时候，我们已经讲过，Django自带文件存储系统，但是默认文件存储在本地，在本项目中，我们需要将文件保存到FastDFS服务器上，所以需要自定义文件存储系统。 自定义文件存储系统的方法如下： 1）需要继承自django.core.files.storage.Storage，如1234from django.core.files.storage import Storageclass FastDFSStorage(Storage): ... 2）支持Django不带任何参数来实例化存储类，也就是说任何设置都应该从django.conf.settings中获取 1234567891011from django.conf import settingsfrom django.core.files.storage import Storageclass FastDFSStorage(Storage): def __init__(self, base_url=None, client_conf=None): if base_url is None: base_url = settings.FDFS_URL self.base_url = base_url if client_conf is None: client_conf = settings.FDFS_CLIENT_CONF self.client_conf = client_conf 3）存储类中必须实现_open()和_save()方法，以及任何后续使用中可能用到的其他方法。 _open(name, mode=’rb’) 被Storage.open()调用，在打开文件时被使用。 _save(name, content) 被Storage.save()调用，name是传入的文件名，content是Django接收到的文件内容，该方法需要将content文件内容保存。 Django会将该方法的返回值保存到数据库中对应的文件字段，也就是说该方法应该返回要保存在数据库中的文件名称信息。 exists(name) 如果名为name的文件在文件系统中存在，则返回True，否则返回False。 url(name) 返回文件的完整访问URL delete(name) 删除name的文件 listdir(path) 列出指定路径的内容 size(name) 返回name文件的总大小 注意，并不是这些方法全部都要实现，可以省略用不到的方法。 4）需要为存储类添加django.utils.deconstruct.deconstructible装饰器我们在meiduo_mall/utils/fastdfs目录中创建fdfs_storage.py文件，实现可以使用FastDFS存储文件的存储类如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from django.conf import settingsfrom django.core.files.storage import Storagefrom django.utils.deconstruct import deconstructiblefrom fdfs_client.client import Fdfs_client@deconstructibleclass FastDFSStorage(Storage): def __init__(self, base_url=None, client_conf=None): &quot;&quot;&quot; 初始化 :param base_url: 用于构造图片完整路径使用，图片服务器的域名 :param client_conf: FastDFS客户端配置文件的路径 &quot;&quot;&quot; if base_url is None: base_url = settings.FDFS_URL self.base_url = base_url if client_conf is None: client_conf = settings.FDFS_CLIENT_CONF self.client_conf = client_conf def _save(self, name, content): &quot;&quot;&quot; 在FastDFS中保存文件 :param name: 传入的文件名 :param content: 文件内容 :return: 保存到数据库中的FastDFS的文件名 &quot;&quot;&quot; client = Fdfs_client(self.client_conf) ret = client.upload_by_buffer(content.read()) if ret.get(&quot;Status&quot;) != &quot;Upload successed.&quot;: raise Exception(&quot;upload file failed&quot;) file_name = ret.get(&quot;Remote file_id&quot;) return file_name def url(self, name): &quot;&quot;&quot; 返回文件的完整URL路径 :param name: 数据库中保存的文件名 :return: 完整的URL &quot;&quot;&quot; return self.base_url + name def exists(self, name): &quot;&quot;&quot; 判断文件是否存在，FastDFS可以自行解决文件的重名问题 所以此处返回False，告诉Django上传的都是新文件 :param name: 文件名 :return: False &quot;&quot;&quot; return False 3. 在Django配置中设置自定义文件存储类在settings/dev.py文件中添加设置123456# django文件存储DEFAULT_FILE_STORAGE = &apos;meiduo_mall.utils.fastdfs.fdfs_storage.FastDFSStorage&apos;# FastDFSFDFS_URL = &apos;http://image.meiduo.site:8888/&apos; FDFS_CLIENT_CONF = os.path.join(BASE_DIR, &apos;utils/fastdfs/client.conf&apos;) 4. 添加image域名在/etc/hosts中添加访问FastDFS storage服务器的域名 127.0.0.1 image.meiduo.site","categories":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/categories/FastDFS/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/tags/FastDFS/"}]},{"title":"Django使用CKEditor富文本编辑器","slug":"Django使用CKEditor富文本编辑器","date":"2019-03-14T09:52:49.928Z","updated":"2019-03-14T09:56:27.558Z","comments":true,"path":"2019/03/14/Django使用CKEditor富文本编辑器/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django使用CKEditor富文本编辑器/","excerpt":"","text":"Django使用CKEditor富文本编辑器在运营后台，运营人员需要录入商品并编辑商品的详情信息，而商品的详情信息不是普通的文本，可以是包含了HTML语法格式的字符串。为了快速简单的让用户能够在页面中编辑带格式的文本，我们引入富文本编辑器。富文本即具备丰富样式格式的文本。 我们使用功能强大的CKEditor富文本编辑器。 1. 安装 pip install django-ckeditor 2. 添加应用在INSTALLED_APPS中添加123456INSTALLED_APPS = [ ... &apos;ckeditor&apos;, # 富文本编辑器 &apos;ckeditor_uploader&apos;, # 富文本编辑器上传图片模块 ...] 3. 添加CKEditor设置在settings/dev.py中添加123456789# 富文本编辑器ckeditor配置CKEDITOR_CONFIGS = &#123; &apos;default&apos;: &#123; &apos;toolbar&apos;: &apos;full&apos;, # 工具条功能 &apos;height&apos;: 300, # 编辑器高度 # &apos;width&apos;: 300, # 编辑器宽 &#125;,&#125;CKEDITOR_UPLOAD_PATH = &apos;&apos; # 上传图片保存路径，使用了FastDFS，所以此处设为&apos;&apos; 4. 添加ckeditor路由在总路由中添加 url(r’^ckeditor/‘, include(‘ckeditor_uploader.urls’)), 5. 为模型类添加字段ckeditor提供了两种类型的Django模型类字段 ckeditor.fields.RichTextField 不支持上传文件的富文本字段ckeditor_uploader.fields.RichTextUploadingField 支持上传文件的富文本字段在商品模型类（SPU）中，要保存商品的详细介绍、包装信息、售后服务，这三个字段需要作为富文本字段 1234567891011from ckeditor.fields import RichTextFieldfrom ckeditor_uploader.fields import RichTextUploadingFieldclass Goods(BaseModel): &quot;&quot;&quot; 商品SPU &quot;&quot;&quot; ... desc_detail = RichTextUploadingField(default=&apos;&apos;, verbose_name=&apos;详细介绍&apos;) desc_pack = RichTextField(default=&apos;&apos;, verbose_name=&apos;包装信息&apos;) desc_service = RichTextUploadingField(default=&apos;&apos;, verbose_name=&apos;售后服务&apos;) 6. 修改Bug我们将通过Django上传的图片保存到了FastDFS中，而保存在FastDFS中的文件名没有后缀名，ckeditor在处理上传后的文件名按照有后缀名来处理，所以会出现bug错误， 修正方法找到虚拟环境目录中的ckeditor_uploader/views.py文件，如 ~/.virtualenvs/meiduo/lib/python3.5/site-packages/ckeditor_uploader/views.py 将第95行代码修改如下","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"},{"name":"富文本","slug":"富文本","permalink":"https://zem12345678.github.io/tags/富文本/"}]},{"title":"使用Docker安装FastDFS","slug":"使用Docker安装FastDFS","date":"2019-03-14T09:45:54.351Z","updated":"2019-03-14T09:54:26.402Z","comments":true,"path":"2019/03/14/使用Docker安装FastDFS/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/使用Docker安装FastDFS/","excerpt":"","text":"使用Docker安装FastDFS1. 获取镜像可以利用已有的FastDFS Docker镜像来运行FastDFS。 获取镜像可以通过下载 docker image pull delron/fastdfs 加载好镜像后，就可以开启运行FastDFS的tracker和storage了。 2. 运行tracker执行如下命令开启tracker 服务 docker run -dti –network=host –name tracker -v /var/fdfs/tracker:/var/fdfs delron/fastdfs tracker 我们将fastDFS tracker运行目录映射到本机的 /var/fdfs/tracker目录中。执行如下命令查看tracker是否运行起来 docker container ls 如果想停止tracker服务，可以执行如下命令 docker container stop tracker 停止后，重新运行tracker，可以执行如下命令 docker container start tracker 3. 运行storage执行如下命令开启storage服务 docker run -dti –network=host –name storage -e TRACKER_SERVER=10.211.55.5:22122 -v /var/fdfs/storage:/var/fdfs delron/fastdfs storage TRACKER_SERVER=本机的ip地址:22122 本机ip地址不要使用127.0.0.1我们将fastDFS storage运行目录映射到本机的/var/fdfs/storage目录中 执行如下命令查看storage是否运行起来 docker container ls 如果想停止storage服务，可以执行如下命令 docker container stop storage 停止后，重新运行storage，可以执行如下命令 docker container start storage 注意：如果无法重新运行，可以删除/var/fdfs/storage/data目录下的fdfs_storaged.pid 文件，然后重新运行storage。","categories":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/categories/FastDFS/"}],"tags":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/tags/FastDFS/"},{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/tags/Docker/"}]},{"title":"Django项目部署","slug":"Django项目部署","date":"2019-03-14T09:37:31.966Z","updated":"2019-03-14T09:39:11.170Z","comments":true,"path":"2019/03/14/Django项目部署/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django项目部署/","excerpt":"","text":"Django项目部署 1.静态文件开发阶段: live-server 部署阶段: nginx当Django运行在生产模式时，将不再提供静态文件的支持，需要将静态文件交给静态文件服务器。 我们先收集所有静态文件。项目中的静态文件除了我们使用的前端项目front_end_pc中之外，django本身还有自己的静态文件，如果rest_framework、xadmin、admin、ckeditor等。我们需要收集这些静态文件，集中一起放到静态文件服务器中。 我们要将收集的静态文件放到front_end_pc目录下的static目录中，所以先创建目录static。Django提供了收集静态文件的方法。先在配置文件中配置收集之后存放的目录 STATIC_ROOT = os.path.join(os.path.dirname(os.path.dirname(BASE_DIR)), ‘front_end_pc/static’) 然后执行收集命令 python manage.py collectstatic 我们使用Nginx服务器作为静态文件服务器打开Nginx的配置文件 sudo vim /usr/local/nginx/conf/nginx.conf 在server部分中配置1234567891011server &#123; listen 80; server_name www.meiduo.site; location / &#123; root /home/python/Desktop/front_end_pc; index index.html index.htm; &#125; # 余下省略&#125; 重启Nginx服务器 sudo /usr/local/nginx/sbin/nginx -s reload 首次启动nginx服务器 sudo /usr/local/nginx/sbin/nginx 停止nginx服务器 sudo /usr/local/nginx/sbin/nginx -s stop 2. 动态接口开发阶段：Django提供的开发web服务器 python manage.py runserver 部署阶段：uwsgi(遵循wsgi协议的web服务器)。 域名: api.meiduo.site 使用uwsgi: 安装 pip install uwsgi 配置 使用 启动 uwsgi –ini uwsgi.ini 停止 uwsgi –stop uwsgi.pid 在项目中复制开发配置文件dev.py 到生产配置prod.py 修改配置文件prod.py中 DEBUG = True ALLOWED_HOSTS = […, ‘www.meiduo.site&#39;] # 添加www.meiduo.site CORS_ORIGIN_WHITELIST = ( ‘127.0.0.1:8080’, ‘localhost:8080’, ‘www.meiduo.site:8080&#39;, ‘www.meiduo.site&#39;, # 添加) 修改wsgi.py文件 os.environ.setdefault(“DJANGO_SETTINGS_MODULE”, “meiduo_mall.settings.prod”) django的程序通常使用uwsgi服务器来运行安装uwsgi pip install uwsgi 在项目目录/meiduo_mall 下创建uwsgi配置文件 uwsgi.ini123456789101112131415161718192021[uwsgi]#使用nginx连接时使用，Django程序所在服务器地址socket=127.0.0.1:8001#直接做web服务器使用，Django程序所在服务器地址#http=127.0.0.1:8001#项目目录chdir=/Users/delron/Desktop/meiduo/meiduo_mall#项目中wsgi.py文件的目录，相对于项目目录wsgi-file=meiduo_mall/wsgi.py# 进程数processes=4# 线程数threads=2# uwsgi服务器的角色master=True# 存放进程编号的文件pidfile=uwsgi.pid# 日志文件，因为uwsgi可以脱离终端在后台运行，日志看不见。我们以前的runserver是依赖终端的daemonize=uwsgi.log# 指定依赖的虚拟环境virtualenv=/Users/smart/.virtualenvs/meiduo 启动uwsgi服务器 uwsgi –ini uwsgi.ini 注意如果想要停止服务器，除了可以使用kill命令之外，还可以通过 uwsgi –stop uwsgi.pid 修改Nginx配置文件，让Nginx接收到请求后转发给uwsgi服务器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647upstream meiduo &#123; server 10.211.55.2:8001; # 此处为uwsgi运行的ip地址和端口号 # 如果有多台服务器，可以在此处继续添加服务器地址 &#125; #gzip on; server &#123; listen 8000; server_name api.meiduo.site; location / &#123; include uwsgi_params; uwsgi_pass meiduo; &#125; &#125; server &#123; listen 80; server_name www.meiduo.site; #charset koi8-r; #access_log logs/host.access.log main; location /xadmin &#123; include uwsgi_params; uwsgi_pass meiduo; &#125; location /ckeditor &#123; include uwsgi_params; uwsgi_pass meiduo; &#125; location / &#123; root /home/python/Desktop/front_end_pc; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; 重启nginx sudo /usr/local/nginx/sbin/nginx -s reload","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"Nginx","slug":"Nginx","permalink":"https://zem12345678.github.io/tags/Nginx/"}]},{"title":"Docker的简单操作","slug":"Docker的简单操作","date":"2019-03-14T08:41:39.252Z","updated":"2019-03-14T08:43:04.952Z","comments":true,"path":"2019/03/14/Docker的简单操作/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Docker的简单操作/","excerpt":"","text":"Docker的简单操作1. 在Ubuntu中安装Docker更新ubuntu的apt源索引 sudo apt-get update 安装包允许apt通过HTTPS使用仓库 sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common 添加Docker官方GPG key curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 设置Docker稳定版仓库 sudo add-apt-repository \\ “deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable” 添加仓库后，更新apt源索引 sudo apt-get update 安装最新版Docker CE（社区版） sudo apt-get install docker-ce 检查Docker CE是否安装正确 sudo docker run hello-world 出现如下信息，表示安装成功为了避免每次命令都输入sudo，可以设置用户权限，注意执行后须注销重新登录 sudo usermod -a -G docker $USER 2. 启动与停止安装完成Docker后，默认已经启动了docker服务，如需手动控制docker服务的启停，可执行如下命令 启动docker sudo service docker start 停止docker sudo service docker stop 重启docker sudo service docker restart 3. Docker镜像操作什么是Docker镜像Docker 镜像是由文件系统叠加而成(是一种文件的存储形式)。最底端是一个文件引 导系统，即 bootfs，这很像典型的 Linux/Unix 的引导文件系统。Docker 用户几乎永远不会和 引导系统有什么交互。实际上，当一个容器启动后，它将会被移动到内存中，而引导文件系 统则会被卸载，以留出更多的内存供磁盘镜像使用。Docker 容器启动是需要一些文件的， 而这些文件就可以称为 Docker 镜像。Docker 把应用程序及其依赖，打包在 image 文件里面。只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 image 是二进制文件。实际开发中，一个 image 文件往往通过继承另一个 image 文件，加上一些个性化设置而生成。举例来说，你可以在 Ubuntu 的 image 基础上，往里面加入 Apache 服务器，形成你的 image。 image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。 为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 Docker Hub 是最重要、最常用的 image 仓库。此外，出售自己制作的 image 文件也是可以的。 列出镜像 docker image ls REPOSITORY：镜像所在的仓库名称TAG：镜像标签IMAGEID：镜像IDCREATED：镜像的创建日期(不是获取该镜像的日期)SIZE：镜像大小 为了区分同一个仓库下的不同镜像，Docker 提供了一种称为标签(Tag)的功能。每个 镜像在列出来时都带有一个标签，例如latest、 12.10、12.04 等等。每个标签对组成特定镜像的一 些镜像层进行标记(比如，标签 12.04 就是对所有 Ubuntu12.04 镜像层的标记)。这种机制 使得同一个仓库中可以存储多个镜像。— 版本号 我们在运行同一个仓库中的不同镜像时，可以通过在仓库名后面加上一个冒号和标签名 来指定该仓库中的某一具体的镜像，例如 docker run –name custom_container_name –i –t docker.io/ubunto:12.04 /bin/bash，表明从镜像 Ubuntu:12.04 启动一个容器，而这个镜像的操 作系统就是 Ubuntu:12.04。在构建容器时指定仓库的标签也是一个好习惯。 拉取镜像Docker维护了镜像仓库，分为共有和私有两种，共有的官方仓库Docker Hub(https://hub.docker.com/)是最重要最常用的镜像仓库。私有仓库（Private Registry）是开发者或者企业自建的镜像存储库，通常用来保存企业 内部的 Docker 镜像，用于内部开发流程和产品的发布、版本控制。 要想获取某个镜像，我们可以使用pull命令，从仓库中拉取镜像到本地，如 docker image pull library/hello-world 上面代码中，docker image pull是抓取 image 文件的命令。library/hello-world是 image 文件在仓库里面的位置，其中library是 image 文件所在的组，hello-world是 image 文件的名字。 由于 Docker 官方提供的 image 文件，都放在library组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。 docker image pull hello-world 删除镜像docker image rm 镜像名或镜像id如 docker image rm hello-world 4. Docker 容器操作创建容器docker run [option] 镜像名 [向启动容器中传入的命令]常用可选参数说明： -i 表示以“交互模式”运行容器-t 表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即 分配一个伪终端。–name 为创建的容器命名-v 表示目录映射关系(前者是宿主机目录，后者是映射到宿主机上的目录，即 宿主机目录:容器中目录)，可以使 用多个-v 做多个目录或文件映射。注意:最好做目录映射，在宿主机上做修改，然后 共享到容器上。-d 在run后面加上-d参数,则会创建一个守护式容器在后台运行(这样创建容器后不 会自动登录容器，如果只加-i -t 两个参数，创建后就会自动进去容器)。-p 表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p 做多个端口映射-e 为容器设置环境变量–network=host 表示将主机的网络环境映射到容器中，容器的网络与主机相同 交互式容器例如，创建一个交互式容器，并命名为myubuntu docker run -it –name=myubuntu ubuntu /bin/bash 在容器中可以随意执行linux命令，就是一个ubuntu的环境，当执行exit命令退出时，该容器也随之停止。 守护式容器创建一个守护式容器:如果对于一个需要长期运行的容器来说，我们可以创建一个守护式容器。在容器内部exit退出时，容器也不会停止。 docker run -dit –name=myubuntu2 ubuntu 进入已运行的容器 docker exec -it 容器名或容器id 进入后执行的第一个命令 如 docker exec -it myubuntu2 /bin/bash 查看容器列出本机正在运行的容器 docker container ls 列出本机所有容器，包括已经终止运行的 docker container ls –all 停止与启动容器停止一个已经在运行的容器 docker container stop 容器名或容器id 启动一个已经停止的容器 docker container start 容器名或容器id kill掉一个已经在运行的容器 docker container kill 容器名或容器id 删除容器 docker container rm 容器名或容器id 5. 将容器保存为镜像我们可以通过如下命令将容器保存为镜像 docker commit 容器名 镜像名 6. 镜像备份与迁移我们可以通过save命令将镜像打包成文件，拷贝给别人使用 docker save -o 保存的文件名 镜像名 如 docker save -o ./ubuntu.tar ubuntu 在拿到镜像文件后，可以通过load方法，将镜像加载到本地 docker load -i ./ubuntu.tar","categories":[{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/categories/Docker/"}],"tags":[{"name":"容器","slug":"容器","permalink":"https://zem12345678.github.io/tags/容器/"},{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/tags/Docker/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://zem12345678.github.io/tags/虚拟化/"}]},{"title":"Django 导出Excel文件","slug":"Django 导出Excel文件","date":"2019-03-14T08:18:02.834Z","updated":"2019-03-14T08:22:28.344Z","comments":true,"path":"2019/03/14/Django 导出Excel文件/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Django 导出Excel文件/","excerpt":"","text":"Django 导出Excel文件 12设置HTTPResponse的类型reposnse = HttpResponse(content_type=&apos;application/vnd.ms-excel&apos;) 代码123456789101112131415161718192021222324252627282930313233343536373839404142def get(self,request): filename = &apos;导入模板&apos; + &apos;.xls&apos; # 设置HTTPResponse的类型 reposnse = HttpResponse(content_type=&apos;application/vnd.ms-excel&apos;) # 创建一个文件对象 reposnse[&apos;Content-Disposition&apos;] = &apos;attachment;filename=&apos;+filename # 创建一个sheet对象 wb = xlwt.Workbook(encoding=&apos;utf-8&apos;) sheet = wb.add_sheet(&apos;order-sheet&apos;) # 设置文件头的样式 style_heading = xlwt.easyxf(&quot;&quot;&quot; font: name Arial, colour_index white, bold on, height 0xA0; align: wrap off, vert center, horiz center; pattern: pattern solid, fore-colour 0x19; borders: left THIN, right THIN, top THIN, bottom THIN; &quot;&quot;&quot;) # 写入文件标题 sheet.write(0, 0, &apos;姓名&apos;, style_heading) sheet.write(0, 1, &apos;手机号&apos;, style_heading) sheet.write(0, 2, &apos;身份证号&apos;, style_heading) sheet.write(0, 3, &apos;区域号&apos;, style_heading) # 写出到IO output = BytesIO() wb.save(output) output.seek(0) reposnse.write(output.getvalue()) logger.info(&apos;用户:%s 导出了导入模板&apos; % (request.user.username)) return reposnse","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"}]},{"title":"关于Docker","slug":"关于Docker","date":"2019-03-14T08:06:22.488Z","updated":"2019-03-14T08:08:08.051Z","comments":true,"path":"2019/03/14/关于Docker/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/关于Docker/","excerpt":"","text":"关于Docker1.什么是Docker 容器技术在计算机的世界中，容器拥有一段漫长且传奇的历史。容器与管理程序虚拟化 (hypervisor virtualization，HV)有所不同，管理程序虚拟化通过中间层将一台或者多台独立 的机器虚拟运行与物理硬件之上，而容器则是直接运行在操作系统内核之上的用户空间。因 此，容器虚拟化也被称为“操作系统级虚拟化”，容器技术可以让多个独立的用户空间运行 在同一台宿主机上。 由于“客居”于操作系统，容器只能运行与底层宿主机相同或者相似的操作系统，这看 起来并不是非常灵活。例如:可以在 Ubuntu 服务中运行 Redhat Enterprise Linux，但无法再 Ubuntu 服务器上运行 Microsoft Windows。 相对于彻底隔离的管理程序虚拟化，容器被认为是不安全的。而反对这一观点的人则认 为，由于虚拟容器所虚拟的是一个完整的操作系统，这无疑增大了攻击范围，而且还要考虑 管理程序层潜在的暴露风险。 尽管有诸多局限性，容器还是被广泛部署于各种各样的应用场合。在超大规模的多租户 服务部署、轻量级沙盒以及对安全要求不太高的隔离环境中，容器技术非常流行。最常见的 一个例子就是“权限隔离监牢”(chroot jail)，它创建一个隔离的目录环境来运行进程。 如果权限隔离监牢正在运行的进程被入侵者攻破，入侵者便会发现自己“身陷囹圄”，因为 权限不足被困在容器所创建的目录中，无法对宿主机进一步破坏。 最新的容器技术引入了 OpenVZ、Solaris Zones 以及 Linux 容器(LXC)。使用这些新技 术，容器不在仅仅是一个单纯的运行环境。在自己的权限类内，容器更像是一个完整的宿主 机。对 Docker 来说，它得益于现代 Linux 特性，如控件组(control group)、命名空间 (namespace)技术，容器和宿主机之间的隔离更加彻底，容器有独立的网络和存储栈，还 拥有自己的资源管理能力，使得同一台宿主机中的多个容器可以友好的共存。 容器被认为是精益技术，因为容器需要的开销有限。和传统虚拟化以及半虚拟化相比， 容器不需要模拟层(emulation layer)和管理层(hypervisor layer)，而是使用操作系统的系 统调用接口。这降低了运行单个容器所需的开销，也使得宿主机中可以运行更多的容器。 尽管有着光辉的历史，容器仍未得到广泛的认可。一个很重要的原因就是容器技术的复 杂性:容器本身就比较复杂，不易安装，管理和自动化也很困难。而 Docker 就是为了改变 这一切而生的。 Docker特点1）上手快用户只需要几分钟，就可以把自己的程序“Docker 化”。Docker 依赖于“写时复制” (copy-on-write)模型，使修改应用程序也非常迅速，可以说达到“随心所致，代码即改” 的境界。 随后，就可以创建容器来运行应用程序了。大多数 Docker 容器只需要不到 1 秒中即可 启动。由于去除了管理程序的开销，Docker 容器拥有很高的性能，同时同一台宿主机中也 可以运行更多的容器，使用户尽可能的充分利用系统资源。 2）职责的逻辑分类使用 Docker，开发人员只需要关心容器中运行的应用程序，而运维人员只需要关心如 何管理容器。Docker 设计的目的就是要加强开发人员写代码的开发环境与应用程序要部署 的生产环境一致性。从而降低那种“开发时一切正常，肯定是运维的问题(测试环境都是正 常的，上线后出了问题就归结为肯定是运维的问题)” 3）快速高效的开发生命周期Docker 的目标之一就是缩短代码从开发、测试到部署、上线运行的周期，让你的应用 程序具备可移植性，易于构建，并易于协作。(通俗一点说，Docker 就像一个盒子，里面 可以装很多物件，如果需要这些物件的可以直接将该大盒子拿走，而不需要从该盒子中一件 件的取。) 4）鼓励使用面向服务的架构Docker 还鼓励面向服务的体系结构和微服务架构。Docker 推荐单个容器只运行一个应 用程序或进程，这样就形成了一个分布式的应用程序模型，在这种模型下，应用程序或者服 务都可以表示为一系列内部互联的容器，从而使分布式部署应用程序，扩展或调试应用程序 都变得非常简单，同时也提高了程序的内省性。(当然，可以在一个容器中运行多个应用程 序) 2. Docker组件1）Docker 客户端和服务器Docker 是一个客户端-服务器(C/S)架构程序。Docker 客户端只需要向 Docker 服务器 或者守护进程发出请求，服务器或者守护进程将完成所有工作并返回结果。Docker 提供了 一个命令行工具 Docker 以及一整套 RESTful API。你可以在同一台宿主机上运行 Docker 守护 进程和客户端，也可以从本地的 Docker 客户端连接到运行在另一台宿主机上的远程 Docker 守护进程。 2）Docker镜像镜像是构建 Docker 的基石。用户基于镜像来运行自己的容器。镜像也是 Docker 生命周 期中的“构建”部分。镜像是基于联合文件系统的一种层式结构，由一系列指令一步一步构 建出来。例如: 添加一个文件; 执行一个命令; 打开一个窗口。 也可以将镜像当作容器的“源代码”。镜像体积很小，非常“便携”，易于分享、存储和更 新。 3）Registry（注册中心）Docker 用 Registry 来保存用户构建的镜像。Registry 分为公共和私有两种。Docker 公司 运营公共的 Registry 叫做 Docker Hub。用户可以在 Docker Hub 注册账号，分享并保存自己的 镜像(说明:在 Docker Hub 下载镜像巨慢，可以自己构建私有的 Registry)。 4）Docker容器Docker 可以帮助你构建和部署容器，你只需要把自己的应用程序或者服务打包放进容 器即可。容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，镜 像是Docker生命周期中的构建或者打包阶段，而容器则是启动或者执行阶段。 容器基于 镜像启动，一旦容器启动完成后，我们就可以登录到容器中安装自己需要的软件或者服务。所以 Docker 容器就是: 一个镜像格式; 一些列标准操作; 一个执行环境。 Docker 借鉴了标准集装箱的概念。标准集装箱将货物运往世界各地，Docker 将这个模 型运用到自己的设计中，唯一不同的是:集装箱运输货物，而 Docker 运输软件。 和集装箱一样，Docker 在执行上述操作时，并不关心容器中到底装了什么，它不管是 web 服务器，还是数据库，或者是应用程序服务器什么的。所有的容器都按照相同的方式将 内容“装载”进去。 Docker 也不关心你要把容器运到何方:我们可以在自己的笔记本中构建容器，上传到 Registry，然后下载到一个物理的或者虚拟的服务器来测试，在把容器部署到具体的主机中。 像标准集装箱一样，Docker 容器方便替换，可以叠加，易于分发，并且尽量通用。 使用 Docker，我们可以快速的构建一个应用程序服务器、一个消息总线、一套实用工 具、一个持续集成(CI)测试环境或者任意一种应用程序、服务或工具。我们可以在本地构 建一个完整的测试环境，也可以为生产或开发快速复制一套复杂的应用程序栈。 3. 使用Docker做什么容器提供了隔离性，结论是，容器可以为各种测试提供很好的沙盒环境。并且，容器本 身就具有“标准性”的特征，非常适合为服务创建构建块。Docker 的一些应用场景如下: 加速本地开发和构建流程，使其更加高效、更加轻量化。本地开发人员可以构建、 运行并分享 Docker 容器。容器可以在开发环境中构建，然后轻松的提交到测试环境中，并 最终进入生产环境。能够让独立的服务或应用程序在不同的环境中，得到相同的运行结果。这一点在 面向服务的架构和重度依赖微型服务的部署由其实用。用 Docker 创建隔离的环境来进行测试。例如，用 Jenkins CI 这样的持续集成工具 启动一个用于测试的容器。Docker 可以让开发者先在本机上构建一个复杂的程序或架构来进行测试，而不是 一开始就在生产环境部署、测试。","categories":[{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/categories/Docker/"}],"tags":[{"name":"容器","slug":"容器","permalink":"https://zem12345678.github.io/tags/容器/"},{"name":"Docker","slug":"Docker","permalink":"https://zem12345678.github.io/tags/Docker/"},{"name":"虚拟化","slug":"虚拟化","permalink":"https://zem12345678.github.io/tags/虚拟化/"}]},{"title":"FastDFS分布式文件系统","slug":"FastDFS分布式文件系统","date":"2019-03-14T07:43:06.160Z","updated":"2019-03-14T08:11:36.272Z","comments":true,"path":"2019/03/14/FastDFS分布式文件系统/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/FastDFS分布式文件系统/","excerpt":"","text":"FastDFS分布式文件系统1. 什么是FastDFSFastDFS 是用 c 语言编写的一款开源的分布式文件系统。FastDFS 为互联网量身定制， 充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用 FastDFS 很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。 FastDFS 架构包括 Tracker server 和 Storage server。客户端请求 Tracker server 进行文 件上传、下载，通过 Tracker server 调度最终由 Storage server 完成文件上传和下载。 Tracker server 作用是负载均衡和调度，通过 Tracker server 在文件上传时可以根据一些 策略找到 Storage server 提供文件上传服务。可以将 tracker 称为追踪服务器或调度服务器。 Storage server 作用是文件存储，客户端上传的文件最终存储在 Storage 服务器上， Storageserver 没有实现自己的文件系统而是利用操作系统 的文件系统来管理文件。可以将 storage 称为存储服务器。服务端两个角色: Tracker: 管理集群，tracker 也可以实现集群。每个 tracker 节点地位平等。收集 Storage 集群的状态。Storage: 实际保存文件， Storage 分为多个组，每个组之间保存的文件是不同的。每 个组内部可以有多个成员，组成员内部保存的内容是一样的，组成员的地位是一致的，没有 主从的概念。 2. 文件上传流程客户端上传文件后存储服务器将文件 ID 返回给客户端，此文件 ID 用于以后访问该文 件的索引信息。文件索引信息包括:组名，虚拟磁盘路径，数据两级目录，文件名。 组名：文件上传后所在的 storage 组名称，在文件上传成功后有 storage 服务器返回， 需要客户端自行保存。虚拟磁盘路径：storage 配置的虚拟路径，与磁盘选项 store_path*对应。如果配置了 store_path0 则是 M00，如果配置了 store_path1 则是 M01，以此类推。数据两级目录：storage 服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据 文件。文件名：与文件上传时不同。是由存储服务器根据特定信息生成，文件名包含:源存储 服务器 IP 地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息。 3. 简易FastDFS构建","categories":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/categories/FastDFS/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://zem12345678.github.io/tags/分布式/"},{"name":"FastDFS","slug":"FastDFS","permalink":"https://zem12345678.github.io/tags/FastDFS/"}]},{"title":"关于JWT","slug":"关于JWT","date":"2019-03-14T06:18:00.631Z","updated":"2019-03-14T06:25:48.447Z","comments":true,"path":"2019/03/14/关于JWT/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/关于JWT/","excerpt":"","text":"关于JWT在用户注册或登录后，我们想记录用户的登录状态，或者为用户创建身份认证的凭证。我们不再使用Session认证机制，而使用Json Web Token认证机制。 [TOC] 什么是JWT Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密。 起源 说起JWT，我们应该来谈一谈基于token的认证和传统的session认证的区别。 ##传统的session认证我们知道，http协议本身是一种无状态的协议，而这就意味着如果用户向我们的应用提供了用户名和密码来进行用户认证，那么下一次请求时，用户还要再一次进行用户认证才行，因为根据http协议，我们并不能知道是哪个用户发出的请求，所以为了让我们的应用能识别是哪个用户发出的请求，我们只能在服务器存储一份用户登录的信息，这份登录信息会在响应时传递给浏览器，告诉其保存为cookie,以便下次请求时发送给我们的应用，这样我们的应用就能识别请求来自哪个用户了,这就是传统的基于session认证。但是这种基于session的认证使应用本身很难得到扩展，随着不同客户端用户的增加，独立的服务器已无法承载更多的用户，而这时候基于session认证应用的问题就会暴露出来. ###基于session认证所显露的问题Session: 每个用户经过我们的应用认证之后，我们的应用都要在服务端做一次记录，以方便用户下次请求的鉴别，通常而言session都是保存在内存中，而随着认证用户的增多，服务端的开销会明显增大。 扩展性: 用户认证之后，服务端做认证记录，如果认证的记录被保存在内存中的话，这意味着用户下次请求还必须要请求在这台服务器上,这样才能拿到授权的资源，这样在分布式的应用上，相应的限制了负载均衡器的能力。这也意味着限制了应用的扩展能力。 CSRF: 因为是基于cookie来进行用户识别的, cookie如果被截获，用户就会很容易受到跨站请求伪造的攻击。 基于token的鉴权机制基于token的鉴权机制类似于http协议也是无状态的，它不需要在服务端去保留用户的认证信息或者会话信息。这就意味着基于token认证机制的应用不需要去考虑用户在哪一台服务器登录了，这就为应用的扩展提供了便利。 流程上是这样的： 用户使用用户名密码来请求服务器服务器进行验证用户的信息服务器通过验证发送给用户一个token客户端存储token，并在每次请求时附送上这个token值服务端验证token值，并返回数据这个token必须要在每次请求时传递给服务端，它应该保存在请求头里， 另外，服务端要支持CORS(跨来源资源共享)策略，一般我们在服务端这么做就可以了Access-Control-Allow-Origin: *。 那么我们现在回到JWT的主题上。 JWT长什么样？JWT是由三段信息构成的，将这三段信息文本用.链接一起就构成了Jwt字符串。就像这样: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ JWT的构成第一部分我们称它为头部（header),第二部分我们称其为载荷（payload, 类似于飞机上承载的物品)，第三部分是签证（signature). headerwt的头部承载两部分信息： 声明类型，这里是jwt声明加密的算法 通常直接使用 HMAC SHA256完整的头部就像下面这样的JSON： { ‘typ’: ‘JWT’, ‘alg’: ‘HS256’} 然后将头部进行base64加密（该加密是可以对称解密的),构成了第一部分. eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 payload载荷就是存放有效信息的地方。这个名字像是特指飞机上承载的货品，这些有效信息包含三个部分 标准中注册的声明公共的声明私有的声明标准中注册的声明 (建议但不强制使用) ： iss: jwt签发者sub: jwt所面向的用户aud: 接收jwt的一方exp: jwt的过期时间，这个过期时间必须要大于签发时间nbf: 定义在什么时间之前，该jwt都是不可用的.iat: jwt的签发时间jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。公共的声明 ： 公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密.私有的声明 ： 私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。 定义一个payload: { “sub”: “1234567890”, “name”: “John Doe”, “admin”: true} 然后将其进行base64加密，得到JWT的第二部分。 eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9 signatureJWT的第三部分是一个签证信息，这个签证信息由三部分组成： header (base64后的)payload (base64后的)secret 这个部分需要base64加密后的header和base64加密后的payload使用.连接组成的字符串，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第三部分。 // javascriptvar encodedString = base64UrlEncode(header) + ‘.’ + base64UrlEncode(payload); var signature = HMACSHA256(encodedString, ‘secret’); // TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 将这三部分用.连接成一个完整的字符串,构成了最终的jwt: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 注意：secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用来进行jwt的签发和jwt的验证，所以，它就是你服务端的私钥，在任何场景都不应该流露出去。一旦客户端得知这个secret, 那就意味着客户端是可以自我签发jwt了。 如何应用一般是在请求头里加入Authorization，并加上Bearer标注： fetch(‘api/user/1’, { headers: { ‘Authorization’: ‘Bearer ‘ + token }})服务端会验证token，如果验证通过就会返回相应的资源。整个流程就是这样的:总结优点因为json的通用性，所以JWT是可以进行跨语言支持的，像JAVA,JavaScript,NodeJS,PHP等很多语言都可以使用。因为有了payload部分，所以JWT可以在自身存储一些其他业务逻辑所必要的非敏感信息。便于传输，jwt的构成非常简单，字节占用很小，所以它是非常便于传输的。它不需要在服务端保存会话信息, 所以它易于应用的扩展 安全相关 不应该在jwt的payload部分存放敏感信息，因为该部分是客户端可解密的部分。保护好secret私钥，该私钥非常重要。如果可以，请使用https协议","categories":[{"name":"JWT","slug":"JWT","permalink":"https://zem12345678.github.io/categories/JWT/"}],"tags":[{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"},{"name":"Token","slug":"Token","permalink":"https://zem12345678.github.io/tags/Token/"},{"name":"权限认证","slug":"权限认证","permalink":"https://zem12345678.github.io/tags/权限认证/"}]},{"title":"flask项目使用 Gunicorn + Nginx 进行布署","slug":"flask项目使用 Gunicorn + Nginx 进行布署","date":"2019-03-14T05:50:18.325Z","updated":"2019-03-14T06:26:08.221Z","comments":true,"path":"2019/03/14/flask项目使用 Gunicorn + Nginx 进行布署/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/flask项目使用 Gunicorn + Nginx 进行布署/","excerpt":"","text":"flask项目使用 Gunicorn + Nginx 进行布署基于ubuntu 16.04系统，使用 Gunicorn + Nginx 进行布署，云服务器为阿里云 [TOC] Markdown简介 选择云服务器:阿里云服务器。(https://zh.wikipedia.org/wiki/Markdown)个人免费获取 [https://free.aliyun.com/] 创建服务器选择ubuntu16.04 64位的操作系统 进入控制台,查看实例创建情况给安全组配置规则，添加5000端口(一并加上5001端口)利用命令行进行远程服务器登录 ssh 用户名@ip地址 ##相关环境安装以下操作都在远程服务器上进行操作 (ubuntu 16.04)先更新 apt 相关源 sudo apt-get update mysql安装 apt-get install mysql-serverapt-get install libmysqlclient-dev redis安装 sudo apt-get install redis-server 安装虚拟环境 pip install virtualenvpip install virtualenvwrapper 使得安装的virtualenvwrapper生效，编辑~/.bashrc文件，内容如下: export WORKON_HOME=$HOME/.virtualenvsexport PROJECT_HOME=$HOME/workspacesource /usr/local/bin/virtualenvwrapper.sh 使编辑后的文件生效 source ~/.bashrc12345678910@requires_authorizationdef somefunc(param1='', param2=0): '''A docstring''' if param1 &gt; param2: # interesting print 'Greater' return (param2 - param1 + 1) or Noneclass SomeClass: pass&gt;&gt;&gt; message = '''interpreter... prompt''' requirements文件Python 项目中可以包含一个 requirements.txt 文件，用于记录所有依赖包及其精确的版本号，以便在新环境中进行部署操作。 在虚拟环境使用以下命令将当前虚拟环境中的依赖包以版本号生成至文件中： pip freeze &gt; requirements.txt 当需要创建这个虚拟环境的完全副本，可以创建一个新的虚拟环境，并在其上运行以下命令： pip install -r requirements.txt 在安装 Flask-MySQLdb 的时候可能会报错，可能是依赖包没有安装，执行以下命令安装依赖包： sudo apt-get build-dep python-mysqldb Nginx采用 C 语言编写实现分流、转发、负载均衡相关操作安装 $ sudo apt-get install nginx运行及停止 /etc/init.d/nginx start #启动/etc/init.d/nginx stop #停止 配置文件 编辑文件:/etc/nginx/sites-available/default123456789101112131415161718192021222324252627# 如果是多台服务器的话，则在此配置，并修改 location 节点下面的 proxy_pass upstream flask &#123; server 127.0.0.1:5000; server 127.0.0.1:5001;&#125;server &#123; # 监听80端口 listen 80 default_server; listen [::]:80 default_server; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name _; location / &#123; # 请求转发到gunicorn服务器 proxy_pass http://127.0.0.1:5000; # 请求转发到多个gunicorn服务器 # proxy_pass http://flask; # 设置请求头，并将头信息传递给服务器端 proxy_set_header Host $host; # 设置请求头，传递原始请求ip给 gunicorn 服务器 proxy_set_header X-Real-IP $remote_addr; &#125;&#125; Gunicorn Gunicorn（绿色独角兽）是一个Python WSGI的HTTP服务器从Ruby的独角兽（Unicorn ）项目移植该Gunicorn服务器与各种Web框架兼容，实现非常简单，轻量级的资源消耗Gunicorn直接用命令启动，不需要编写配置文件 相关操作安装 pip install gunicorn 查看选项 gunicorn -h 运行 -w: 表示进程（worker） -b：表示绑定ip地址和端口号（bind）gunicorn -w 2 -b 127.0.0.1:5000 运行文件名称:Flask程序实例名 参考阅读： Gunicorn相关配置：https://blog.csdn.net/y472360651/article/details/78538188 其他操作拷贝本地代码到远程 scp -r 本地文件路径 root@39.106.21.198:远程保存路径","categories":[{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/categories/Flask/"}],"tags":[{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/tags/Flask/"},{"name":"Nginx","slug":"Nginx","permalink":"https://zem12345678.github.io/tags/Nginx/"},{"name":"Gunicorn","slug":"Gunicorn","permalink":"https://zem12345678.github.io/tags/Gunicorn/"}]},{"title":"flask-jwt-extended使用详解","slug":"flask-jwt-extended使用详解","date":"2019-03-14T02:59:34.702Z","updated":"2019-03-14T06:30:57.770Z","comments":true,"path":"2019/03/14/flask-jwt-extended使用详解/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/flask-jwt-extended使用详解/","excerpt":"","text":"flask-jwt-extended使用详解 相关配置注册jwt123app.config[&apos;JWT_SECRET_KEY&apos;] = &apos;jwt-secret-attendance&apos;jwt = JWTManager(app)app.config[&apos;JWT_BLACKLIST_ENABLED&apos;] = False 产生token12access_token = create_access_token(identity=username)return jsonify(msg=&quot;登录成功&quot;,access_token=access_token) 获取当前用户1username = get_jwt_identity() 高级用法自定义密钥和加秘方式12345678910access_token = encode_access_token(identity=jwt_manager._user_identity_callback(username), secret='revoked-secret', algorithm=config.algorithm, expires_delta=None, fresh=False, user_claims=jwt_manager._user_claims_callback(username), csrf=config.csrf_protect, identity_claim_key=config.identity_claim_key, user_claims_key=config.user_claims_key, json_encoder=config.json_encoder) 令牌撤销 app.config[‘JWT_BLACKLIST_TOKEN_CHECKS’] = [‘access’, ‘refresh’] 创建我们的函数以检查令牌是否已被列入黑名单。在这简单情况下，我们将只存储在Redis的令JTI（唯一标识符）每当我们创建一个新令牌时（撤销状态为’false’）。这个 function将返回令牌的撤销状态。如果令牌没有 存在于这个商店，我们不知道它来自哪里（因为我们正在新添加 创建令牌到我们的商店，撤销状态为’false’）。在这种情况下 出于安全考虑，我们会考虑撤销令牌。12revoked_store = redis.StrictRedis(host='localhost', port=6379, db=0, decode_responses=True) 1234567@jwt.token_in_blacklist_loaderdef check_if_token_is_revoked(decrypted_token): jti = decrypted_token['jti'] entry = revoked_store.get(jti) if entry is None: return True return entry == 'true' 产生token1234567access_token = create_access_token(identity=username)将令牌存储在redis中，状态目前未被撤销。我们可以使用`get_jti（）`方法获取唯一标识符字符串每个令牌。我们还可以在redis中设置这些令牌的到期时间，所以它们会在到期后自动删除。我们将设定令牌过期后不久将自动删除的所有内容access_jti = get_jti(encoded_token=access_token)revoked_store.set(access_jti, 'false', ACCESS_EXPIRES * 1.2) 撤销视图123456@app.route('/auth/access_revoke', methods=['DELETE'])@jwt_requireddef logout(): jti = get_raw_jwt()['jti'] revoked_store.set(jti, 'true', ACCESS_EXPIRES * 1.2) return jsonify(&#123;\"msg\": \"Access token revoked\"&#125;), 200 在令牌中存储和获取数据自定义一个基类存储角色和用户id12345class UserObject: def __init__(self,id,username, role): self.id = id self.username = username self.role = role 1234567891011创建一个将在create_access_token时调用的函数用来。它将采取任何传递到的对象create_access_token方法，让我们定义什么是自定义声明应添加到访问令牌中 @jwt.user_claims_loader def add_claims_to_access_token(user): return &#123;'role': user.role,'id':user.id&#125;创建一个将在create_access_token时调用的函数用来。它将采取任何传递到的对象create_access_token方法，让我们定义什么标识应该是访问令牌的 @jwt.user_identity_loader def user_identity_lookup(user): return user.username 产生令牌123data = UserObject(id=user.id,username=username,role=user.role)expires = datetime.timedelta(days=2)access_token = create_access_token(identity=data, expires_delta=expires) 如何获取数据123username = get_jwt_claims()user_id = get_jwt_claims()['id']user_role = get_jwt_claims()['role']","categories":[{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/categories/Flask/"}],"tags":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/tags/前后端分离/"},{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/tags/Flask/"},{"name":"Jwt","slug":"Jwt","permalink":"https://zem12345678.github.io/tags/Jwt/"}]},{"title":"神兽保佑,代码无bug　","slug":"神兽保佑,代码无bug","date":"2019-03-14T01:59:47.216Z","updated":"2019-03-14T06:26:22.700Z","comments":true,"path":"2019/03/14/神兽保佑,代码无bug/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/神兽保佑,代码无bug/","excerpt":"","text":"神兽保佑,代码无bug ┏┓ ┏┓+ + ┏┛┻━━━┛┻┓ + + ┃ ┃ ┃ ━ ┃ ++ + + + ████━████ ┃+ ┃ ┃ + ┃ ┻ ┃ ┃ ┃ + + ┗━┓ ┏━┛ ┃ ┃ ┃ ┃ + + + + ┃ ┃ Code is far away from bug with the animal protecting ┃ ┃ + 神兽保佑,代码无bug ┃ ┃ ┃ ┃ + ┃ ┗━━━┓ + + ┃ ┣┓ ┃ ┏┛ ┗┓┓┏━┳┓┏┛ + + + + ┃┫┫ ┃┫┫ ┗┻┛ ┗┻┛+ + + +","categories":[{"name":"杂谈","slug":"杂谈","permalink":"https://zem12345678.github.io/categories/杂谈/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"https://zem12345678.github.io/tags/随笔/"}]},{"title":"Flask导出excel文件","slug":"Flask导出excel文件","date":"2019-03-14T01:50:50.660Z","updated":"2019-03-14T06:27:01.352Z","comments":true,"path":"2019/03/14/Flask导出excel文件/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/Flask导出excel文件/","excerpt":"","text":"Flask导出excel文件 1response = make_response(output.getvalue()) 提示：flask中没有Httpreponse类型这一点与django不同我们需哟啊使用flask中make_responese来导入数据流]。 代码1234567891011121314151617181920212223242526272829303132333435now = datetime.now() time = datetime.strftime(now, &apos;%Y%m%d%H%M%S&apos;) 把时间格式化 filename = time + &apos;.xls&apos; # 创建一个sheet对象 wb = xlwt.Workbook(encoding=&apos;utf-8&apos;) sheet = wb.add_sheet(&apos;order-sheet&apos;) # 写入文件标题 sheet.write(0, 0, &apos;职工号&apos;) sheet.write(0, 1, &apos;姓名&apos;) sheet.write(0, 2, &apos;性别&apos;) sheet.write(0, 3, &apos;身份证号&apos;) sheet.write(0, 4, &apos;手机&apos;) sheet.write(0, 5, &apos;办公室&apos;) sheet.write(0, 6, &apos;邮箱&apos;) sheet.write(0, 7, &apos;QQ&apos;) data_row = 1 for teacher in Teacher.query.all(): sheet.write(data_row, 0, teacher.to_full_dict()[&apos;sno&apos;]) sheet.write(data_row, 1, teacher.to_full_dict()[&apos;name&apos;]) sheet.write(data_row, 2, teacher.to_full_dict()[&apos;sex&apos;]) sheet.write(data_row, 3, teacher.to_full_dict()[&apos;idcard_num&apos;]) sheet.write(data_row, 4, teacher.to_full_dict()[&apos;phone_num&apos;]) sheet.write(data_row, 5, teacher.to_full_dict()[&apos;office&apos;]) sheet.write(data_row, 6, teacher.to_full_dict()[&apos;email&apos;]) sheet.write(data_row, 7, teacher.to_full_dict()[&apos;qq&apos;]) data_row = data_row + 1 # 写出到IO output = BytesIO() wb.save(output) output.seek(0) response = make_response(output.getvalue()) response.headers[&apos;content_type=&apos;] = &apos; application/vnd.ms-excel&apos; # 创建一个文件对象 response.headers[&apos;Content-Disposition&apos;] = &apos;attachment;filename=&#123;&#125;&apos;.format(filename.encode().decode(&apos;latin-1&apos;)) return response","categories":[{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/categories/Flask/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"},{"name":"Flask","slug":"Flask","permalink":"https://zem12345678.github.io/tags/Flask/"}]},{"title":"django导入excel文件使用pandas处理并批量插入","slug":"django导入excel文件使用pandas处理并批量插入","date":"2019-03-14T01:26:04.463Z","updated":"2019-03-14T06:31:19.557Z","comments":true,"path":"2019/03/14/django导入excel文件使用pandas处理并批量插入/","link":"","permalink":"https://zem12345678.github.io/2019/03/14/django导入excel文件使用pandas处理并批量插入/","excerpt":"","text":"django导入excel文件使用pandas处理并批量插入导入库：123import pandas as pdimport xlwtfrom io import BytesIO #io数据流 django视图类1234567891011121314151617181920212223242526272829class ImportFarmerData(View): def post(self,request): excel_raw_data = pd.read_excel(request.FILES.get(&apos;file&apos;,&apos;&apos;),header=None) 删除第一行的标题 获取每列 excel_raw_data.drop([0,0],inplace=True) name_col = excel_raw_data.iloc[:,[0]] card_id_col = excel_raw_data.iloc[:,[1]] phone_col = excel_raw_data.iloc[:,[2]] area_num_col = excel_raw_data.iloc[:,[3]]对每一列数据进行处理，从DataFrame类型转换为list类型 name_list = name_col.values.tolist() card_id_list = card_id_col.values.tolist() phone_list = phone_col.values.tolist()对每一列的每一行的数据进行转换，转换为str类型 for i in range(len(name_list)): name_list_index = name_list[i] card_id_list_index = card_id_list[i] phone_list_index = phone_list[i] area_num_index = area_num_list[i] farmer_profile = FarmersProfile() farmer_profile.name = name_list_index[0] farmer_profile.card_id = card_id_list_index[0] farmer_profile.phone = phone_list_index[0] farmer_profile.area_num = area_num_index[0] farmer_profile.address_id = address.id farmer_profile.save() return HttpResponse(json.dumps(&#123;&apos;code&apos;:&apos;200&apos;,&apos;msg&apos;:&apos;导入成功&apos;&#125;) 由于前端使用leiui返回格式必须为json格式 HTML：1&lt;button type=&quot;button&quot; class=&quot;layui-btn&quot; id=&quot;test4&quot; name=&quot;excel_data&quot;&gt;&lt;i class=&quot;layui-icon&quot;&gt;&lt;/i&gt;导入excel&lt;/button&gt; ##ajax:123456789101112131415161718192021layui.use(&apos;upload&apos;, function()&#123; var $ = layui.jquery, upload = layui.upload; //指定允许上传的文件类型 upload.render(&#123; //允许上传的文件后缀 elem: &apos;#test4&apos;, type: &apos;post&apos;, url: &apos;&#123;% url &apos;users:import_famer&apos; %&#125;&apos;, accept: &apos;file&apos;, //普通文件, exts: &apos;xls&apos;, //只允许上传压缩文件, data: &#123;&apos;csrfmiddlewaretoken&apos;: &apos;&#123;&#123; csrf_token &#125;&#125;&apos;&#125;, done: function(res) &#123; if (res.code == 200 ) &#123; layer.msg(res.msg); &#125; &#125; ,error:function (res) &#123; &#125; &#125;);","categories":[{"name":"django","slug":"django","permalink":"https://zem12345678.github.io/categories/django/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"Web","slug":"Web","permalink":"https://zem12345678.github.io/tags/Web/"}]},{"title":"最长公共子序列（LCS）问题","slug":"最长公共子序列（LCS）问题","date":"2019-03-13T15:41:20.313Z","updated":"2019-03-14T14:50:35.724Z","comments":true,"path":"2019/03/13/最长公共子序列（LCS）问题/","link":"","permalink":"https://zem12345678.github.io/2019/03/13/最长公共子序列（LCS）问题/","excerpt":"","text":"最长公共子序列（LCS）问题给定两个 1 到 n 的排列 A,B （即长度为 n 的序列，其中 [1,n] 之间的所有数都出现了恰好一次）。 求 它们的最长公共子序列长度。 子序列： 一个序列A ＝ a1,a2,……an,中任意删除若干项，剩余的序列叫做A的一个子序列。也可以认为是从序列A按原顺序保留任意若干项得到的序列。求解算法对于母串X=&lt;x1,x2,⋯,xm&gt;, Y=&lt;y1,y2,⋯,yn&gt;，求LCS与最长公共子串。 暴力解法假设 m&lt;n， 对于母串X，我们可以暴力找出2的m次方个子序列，然后依次在母串Y中匹配，算法的时间复杂度会达到指数级O(n∗2的m次)。显然，暴力求解不太适用于此类问题。 动态规划假设Z=&lt;z1,z2,⋯,zk&gt;是X与Y的LCS， 我们观察到如果Xm=Yn，则Zk=Xm=Yn，有Zk−1是Xm−1与Yn−1的LCS；如果Xm≠Yn，则Zk是Xm与Yn−1的LCS，或者是Xm−1与Yn的LCS。因此，求解LCS的问题则变成递归求解的两个子问题。但是，上述的递归求解的办法中，重复的子问题多，效率低下。改进的办法——用空间换时间，用数组保存中间状态，方便后面的计算。先假设有 C[i,j] = | LCS(x[1…i] , y(1…j)) |，则有 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// n：表示两序列长度// a：描述序列 a（这里需要注意的是，由于 a 的下标从 1 开始，因此 a[0] 的值为-1，你可以忽略它的值，只需知道我们从下标 1 开始存放有效信息即可） // b：描述序列b（同样地，b[0] 的值为 -1）// 返回值：最长公共子序列的长度#include &lt;cstdio&gt; #include &lt;cstring&gt; #include &lt;cmath&gt; #include &lt;cstdlib&gt; #include &lt;algorithm&gt; #include &lt;queue&gt; #include &lt;stack&gt; #include &lt;map&gt; #include &lt;set&gt; #include &lt;vector&gt; #include &lt;iostream&gt;#include &lt;utility&gt;using namespace std;#define PII pair&lt;int,int&gt; #define PIII pair&lt;pair&lt;int,int&gt;,int&gt; #define mp make_pair#define pb push_back#define sz size() #define fi first#define se secondtypedef unsigned int ui;typedef long long ll;typedef unsigned long long ull;vector&lt;int&gt; pos,a,b,f;int getAns(int n,vector&lt;int&gt;a,vector&lt;int&gt;b)&#123; f.resize(n+1);pos.resize(n+1); for(int i=0;i&lt;=n;++i) f[i]=n+2; for(int i=1;i&lt;=n;++i)&#123; pos[b[i]]=i; &#125; for(int i=1;i&lt;=n;++i)&#123; a[i]=pos[a[i]]; &#125; f[0]=0; for(int i=1;i&lt;=n;++i) *lower_bound(f.begin(),f.end(),a[i])=a[i]; return int(lower_bound(f.begin(),f.end(),n+1)-f.begin())-1;&#125;int main()&#123; int n;scanf(&quot;%d&quot;,&amp;n); a.resize(n+1);b.resize(n+1); for(int i=1;i&lt;=n;++i)&#123; scanf(&quot;%d&quot;,&amp;a[i]); &#125; for(int i=1;i&lt;=n;++i)&#123; scanf(&quot;%d&quot;,&amp;b[i]); &#125; printf(&quot;%d\\n&quot;,getAns(n,a,b)); return 0;&#125; #输出结果123451 2 4 3 55 2 3 4 12","categories":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://zem12345678.github.io/tags/算法/"},{"name":"C/C++","slug":"C-C","permalink":"https://zem12345678.github.io/tags/C-C/"},{"name":"数据结构","slug":"数据结构","permalink":"https://zem12345678.github.io/tags/数据结构/"},{"name":"LeetCood","slug":"LeetCood","permalink":"https://zem12345678.github.io/tags/LeetCood/"}]},{"title":"Django Restful Framework(DRF)的开发思考（2）","slug":"Django Restful Framework(DRF)的开发思考（2）","date":"2019-03-13T14:59:11.338Z","updated":"2019-03-14T06:28:27.431Z","comments":true,"path":"2019/03/13/Django Restful Framework(DRF)的开发思考（2）/","link":"","permalink":"https://zem12345678.github.io/2019/03/13/Django Restful Framework(DRF)的开发思考（2）/","excerpt":"","text":"Restful一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制： CORS CORS是现代浏览亲支持快于请求的一种方式，全称是“跨域资源共享“(Cross-origin resource sharing),当使用XMLHttpRequest发送请求时，浏览器发现该请求不符合同源测率，会给该请求头：Origin，后台进行以系列处理，如果确定请求则在返回结果加入一个响应头：Access-Control-Allow-Origin；浏览器判断该响应头中是否包含Origin的值，如果有浏览器会处理响应，我们就可以拿到响应数据，如果不包含浏览器之间驳回，这时我们无法拿到响应数据 JWT ：Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密；FastDFS ：FastDFS 是用 c 语言编写的一款开源的分布式文件系统。FastDFS 为互联网量身定制， 充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用 FastDFS 很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。FastDFS 架构包括 Tracker server 和 Storage server。客户端请求 Tracker server 进行文 件上传、下载，通过 Tracker server 调度最终由 Storage server 完成文件上传和下载。 [TOC] Django认证系统|跨域请求|Celery提供了用户模型类User和User的相关操作方法。 自定义User模型类之后，需要设置配置项AUTH_USER_MODEL。 1. 前后端域名设置域名和IP是对应的关系。 DNS解析：获取域名对应的IP 通过域名访问网站时，先到本地的hosts中查找IP和域名的对应关系，如果查到直接根据IP访问网站，如果查不到再进行DNS域名解析。 前端服务器域名: www.meiduo.site 后端API服务器域名：api.meiduo.site 2. 短信验证码123456URL: GET /sms_codes/(?P&lt;mobile&gt;1[3-9]\\d&#123;9&#125;)/参数: url地址中传递mobile响应: &#123; \"message\": \"OK\" &#125; 3. 跨域请求浏览器的同源策略: 协议、主机IP和端口PORT相同的地址是同源，否则是非同源。 当发起请求的页面地址和被请求的地址不是同源，那么这个请求就是跨域请求。 在发起请求时，如果浏览器发现请求是跨域请求，那么在请求的报文头中，会添加如下信息: Origin: 源请求IP地址 例如：Origin: http://www.meiduo.site:8080 在被请求的服务器返回的响应中，如果响应头中包含如下信息： Access-Control-Allow-Origin: 源请求IP地址 例如：Access-Control-Allow-Origin: http://www.meiduo.site:8080 那么浏览器认为被请求服务器支持来源地址对其进行跨域请求，否则认为不支持，浏览器会将请求直接驳回。 Django跨域请求扩展使用。 4. celery异步任务队列本质: ​ 使用进程或协程调用函数实现异步。 基本概念： ​ 发出者：发出所有执行的任务(任务就是函数)。 ​ (中间人)任务队列：存放所要执行的任务信息。 ​ 处理者：也就是工作的进程或协程，负责监听任务队列，发现任务便执行对应的任务函数。 特点： ​ 1）任务发送者和处理者可以分布在不同的电脑上，通过中间人进行信息交换。 ​ 2）任务队列中的任务会进行排序，先添加的任务会被先执行。 使用： ​ 1）安装 pip install celery ​ 2）创建Celery对象并配置中间人地址 ​ from celery import Celery ​ celery_app = Celery(‘demo’) ​ 配置文件：broker_url=’中间人地址’ ​ celery_app.config_from_object(‘配置文件路径’) ​ 3）定义任务函数 ​ @celery_app.task(name=’my_first_task’) ​ def my_task(a, b): ​ print(‘任务函数被执行’) ​ … ​ 4）启动worker ​ celery -A ‘celery_app文件路径’ worker -l info ​ 5）发出任务 ​ my_task.delay(2, 3) 5. 用户注册12345678910111213141516URL: POST /users/参数: &#123; \"username\": \"用户名\", \"password\": \"密码\", \"password2\": \"重复密码\", \"mobile\": \"手机号\", \"sms_code\": \"短信验证码\", \"allow\": \"是否同意协议\" &#125;响应: &#123; \"id\": \"用户id\", \"username\": \"用户名\", \"mobile\": \"手机号\" &#125; 补充(axios请求发送)1234567891011121314151617axios.get('url地址', [config]) .then(response =&gt; &#123; // 请求成功，可通过response.data获取响应数据 &#125;) .catch(error =&gt; &#123; // 请求失败，可通过error.response获取响应对象 // error.response.data获取响应数据 &#125;)axios.post('url地址', [data], [config]) .then(response =&gt; &#123; // 请求成功，可通过response.data获取响应数据 &#125;) .catch(error =&gt; &#123; // 请求失败，可通过error.response获取响应对象 // error.response.data获取响应数据 &#125;) 注册|JWT Token|普通登录1. 用户注册12345678910111213141516URL: POST /users/参数: &#123; \"username\": \"用户名\", \"password\": \"密码\", \"password2\": \"重复密码\", \"mobile\": \"手机号\", \"sms_code\": \"短信验证码\", \"allow\": \"是否同意协议\" &#125;响应: &#123; \"id\": \"用户id\", \"username\": \"用户名\", \"mobile\": \"手机号\" &#125; 创建新用户：User.objects.create_user(username, email=None, password=None, *\\extra_fields*) 2. JWT token1）session认证 12345671. 接收用户名和密码2. 判断用户名和密码是否正确3. 保存用户的登录状态(session) session['user_id'] = 1 session['username'] = 'smart' session['mobile'] = '13155667788'4. 返回应答，登录成功 session认证的一些问题: session存储在服务器端，如果登录的用户过多，服务器开销比较大。 session依赖于cookie，session的标识存在cookie中，可能会有CSRF(跨站请求伪造)。 2）jwt 认证机制(替代session认证) 12341. 接收用户名和密码2. 判断用户名和密码是否正确3. 生成(签发)一个jwt token(token中保存用户的身份信息) 公安局(服务器)=&gt;签发身份证(jwt token)4. 返回应答，将jwt token信息返回给客户端。 如果之后需要进行身份认证，客户端需要将jwt token发送给服务器，由服务器验证jwt token的有效性。 3）jwt 的数据格式 12eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFO a）headers头部 1234&#123; &quot;token类型声明&quot;, &quot;加密算法&quot;&#125; base64加密(很容易被解密) b）payload(载荷)：用来保存有效信息 123456&#123; &quot;user_id&quot;: 1, &quot;username&quot;: &quot;smart&quot;, &quot;mobile&quot;: &quot;13155667788&quot;, &quot;exp&quot;: &quot;有效时间&quot;&#125; base64加密 c）signature(签名)：防止jwt token被伪造 将headers和payload进行拼接，用.隔开，使用一个密钥(secret key)进行加密，加密之后的内容就是签名。 jwt token是由服务器生成，密钥保存在服务器端。 3. jwt 扩展签发jwt token1234567from rest_framework_jwt.settings import api_settingsjwt_payload_handler = api_settings.JWT_PAYLOAD_HANDLERjwt_encode_handler = api_settings.JWT_ENCODE_HANDLERpayload = jwt_payload_handler(user)token = jwt_encode_handler(payload) 4. 用户登录1）jwt 扩展的登录视图obtain_jwt_token： ​ 接收username和password，进行用户名和密码验证，正确情况下签发jwt token并返回给客户端。 2）更改obtain_jwt_token组织响应数据的函数： 123456789101112131415def jwt_response_payload_handler(token, user=None, request=None): \"\"\" 自定义jwt认证成功返回数据 \"\"\" return &#123; 'token': token, 'user_id': user.id, 'username': user.username &#125;# JWT扩展配置JWT_AUTH = &#123; # ... 'JWT_RESPONSE_PAYLOAD_HANDLER': 'users.utils.jwt_response_payload_handler',&#125; 3）登录支持用户名和手机号 123obtain_jwt_token-&gt; from django.contrib.auth import authenticate-&gt; from django.contrib.auth.backends import ModelBackend(authenticate: 根据用户名和密码进行校验) 自定义Django的认证系统后端，同时设置配置项AUTHENTICATION_BACKENDS。 ##QQ登录 1. QQ登录-预备工作1）成为QQ开发者 2）创建开发者应用 3）查询QQ登录开发文档 2. QQ登录-开发关键点获取QQ登录用户的唯一身份标识(openid)，然后根据openid进行处理。 判断该QQ用户是否绑定过本网站的用户，如果绑定过，直接登录，如果未绑定过，先进行绑定。 QQ用户绑定: 将openid 和 用户user_id 对应关系存下来。 id user_id openid 1 2 Akdk19389kDkdkk99939kdk 2 2 AKdkk838e8jdiafkdkkFKKKFf 注：一个用户可以绑定多个qq账户。 3. QQ登录API1）获取QQ登录网址API 12345API: GET /oauth/qq/authorization/?next=&lt;url&gt;响应:&#123; \"login_url\": \"QQ登录网址\"&#125; 2）获取QQ登录用户openid并进行处理API 12345678910111213API: GET /oauth/qq/user/?code=&lt;code&gt;参数: code响应: 1）如果openid已经绑定过本网站的用户，直接签发jwt token，返回 &#123; 'user_id': &lt;用户id&gt;, 'username': &lt;用户名&gt;, 'token': &lt;token&gt; &#125; 2）如果openid没有绑定过本网站的用户，先对openid进行加密生成token，把加密的内容返回 &#123; 'access_token': &lt;token&gt; &#125; 3）绑定QQ登录用户的信息API 1234567891011121314API: POST /oauth/qq/user/参数: &#123; \"mobile\": &lt;手机号&gt;, \"password\": &lt;密码&gt;, \"sms_code\": &lt;短信验证码&gt;, \"access_token\": &lt;access_token&gt; &#125;响应: &#123; 'id': &lt;用户id&gt;, 'username': &lt;用户名&gt;, 'token': &lt;token&gt; &#125; ###4. 相关模块的使用 1234567891011# 将python字典转化为查询字符串from urllib.parse import urlencode # 将查询字符串转换成python字典from urllib.parse import parse_qs# 发起网络请求from urllib.request import urlopen# itsdangerous: 加密和解密from itsdangerous import TimedJSONWebSignatureSerializer 邮件发送|省市三级联动个人信息获取1）给User添加email_active字段，用于记录邮箱email是否被激活。 2）API接口：获取用户个人信息。 123456789101112请求方式和URL地址: GET /user/前端传递参数: 在请求头中携带jwt token。返回值: &#123; \"id\": \"&lt;用户id&gt;\", \"username\": \"&lt;用户名&gt;\", \"mobile\": \"&lt;手机号&gt;\", \"email\": \"&lt;邮箱&gt;\", \"email_active\": \"&lt;激活标记&gt;\" &#125;注: DRF中`JSONWebTokenAuthentication`认证机制会根据jwt token对用户身份进行认证，如果认证失败返回401错误，如果是权限禁止返回403错误。 3）使用RetrieveAPIView时，其获取单个对象时是根据pk获取，我们这里所有获取的是当前登录的用户，所以需要把get_object方法进行重写。 4）request对象的user属性。 对于request对象，有一个user属性。这个user属性： ​ a）如果用户认证成功，request.user是User模型类的实例对象，存放的是当前登录用户的信息。 ​ b）如果用户未认证，request.user是AnonymousUser类的实例对象。 ###用户邮箱设置 1）API接口：设置用户个人邮箱。 123456789请求方式和URL地址: PUT /email/前端传递参数: 1）在请求头中携带jwt token。 2）邮箱email返回值: &#123; \"id\": \"&lt;用户id&gt;\", \"email\": \"&lt;邮箱&gt;\", &#125; 2）处理流程: ​ a）接收参数并进行校验(email是否传递，格式是否正确) ​ b）保存用户的邮箱信息并发送激活邮件 ​ c）返回应答，设置邮箱成功 3）发送激活邮件 ​ a）生成激活链接：在激活链接中需要保存待激活用户的id和email，但是为了安全，需要对信息进行加密。 ​ b）发送邮件：配置文件中先进行邮件发送配置，在使用django的send_mail方法发送邮件，为了不影响邮件设置过程，邮件采用celery发送。 12from django.core.mail import send_mailsend_mail(subject='邮件主题', message='正文', from_email='发件人', recipient_list='收件人列表', html_message='html邮件正文') 个人邮箱激活1）API接口：激活用户个人邮箱。 1234567请求方式和URL地址: GET /emails/verification/?token=xxx前端传递参数: 1）token返回值: &#123; \"message\": \"处理结果\" &#125; 2）处理流程: ​ a）接收参数token并进行校验(token是否传递，token是否有效) ​ b）将用户邮箱激活标记设置为已激活。 ​ c）返回应答，邮箱激活成功。 省市县三级信息1）信息存储(自关联) 地区的自关联其实是一个特殊一对多的关系。 一个省包含很多个市，一个市包含很多县。 id(地区id) name(地区名) parent_id(父级地区ID) 320000 江苏省 NULL 320200 无锡市 320000 320282 宜兴市 320200 2）模型类自关联(地区的自关联其实是一个特殊的一对多) 123456class Area(models.Model): \"\"\" 行政区域 \"\"\" name = models.CharField(max_length=20, verbose_name='名称') parent = models.ForeignKey('self', on_delete=models.SET_NULL, related_name='subs', null=True, blank=True, verbose_name='上级行政区域') 注：模型类自关联，ForeignKey第一个参数传self。 另外之前的关联查询中，有了一个area对象之后，查询关联的下级地区信息和父级地区，例如: 123area = Area.objects.get(id='320200')area.parent # 父级地区，由多查一，对象名.关联属性area.area_set.all() # 子级地区，由一查多，对象名.多类名_set.all() 当ForeignKey创建关联属性时，指定了related_name=&#39;subs&#39;之后，再查询和area对象关联的子级地区，不在使用area.area_set.all()，而是使用area.subs.all()。 3）定义导入地区信息shell脚本 12#! /bin/bashmysql -u'&lt;用户名&gt;' -p'&lt;密码&gt;' -h'&lt;数据库主机IP&gt;' '&lt;数据库名&gt;' &lt; '&lt;sql文件&gt;' 4）地区视图集。 补充(客户端发送请求时携带JWT token数据)12345678910111213axios.get(this.host + '/user/', &#123; headers: &#123; // 通过请求头向后端传递JWT token的方法 'Authorization': 'JWT ' + &lt;JWT token数据&gt; &#125;, responseType: 'json',&#125;).then(response =&gt; &#123; ...&#125;).catch(error =&gt; &#123; ...&#125;);","categories":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/categories/前后端分离/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Restful","slug":"Restful","permalink":"https://zem12345678.github.io/tags/Restful/"},{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/tags/前后端分离/"}]},{"title":"Django Restful Framework(DRF)的开发思考（1）","slug":"Django Restful Framework(DRF)的开发思考（1）","date":"2019-03-13T14:19:12.518Z","updated":"2019-03-14T06:29:15.596Z","comments":true,"path":"2019/03/13/Django Restful Framework(DRF)的开发思考（1）/","link":"","permalink":"https://zem12345678.github.io/2019/03/13/Django Restful Framework(DRF)的开发思考（1）/","excerpt":"","text":"Restful一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制： CORS CORS是现代浏览亲支持快于请求的一种方式，全称是“跨域资源共享“(Cross-origin resource sharing),当使用XMLHttpRequest发送请求时，浏览器发现该请求不符合同源测率，会给该请求头：Origin，后台进行以系列处理，如果确定请求则在返回结果加入一个响应头：Access-Control-Allow-Origin；浏览器判断该响应头中是否包含Origin的值，如果有浏览器会处理响应，我们就可以拿到响应数据，如果不包含浏览器之间驳回，这时我们无法拿到响应数据 JWT ：Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于JSON的开放标准（(RFC 7519).该token被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该token也可直接被用于认证，也可被加密； FastDFS ：FastDFS 是用 c 语言编写的一款开源的分布式文件系统。FastDFS 为互联网量身定制， 充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用 FastDFS 很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。FastDFS 架构包括 Tracker server 和 Storage server。客户端请求 Tracker server 进行文 件上传、下载，通过 Tracker server 调度最终由 Storage server 完成文件上传和下载。 Web网站中开发模式（前后端分离&amp;前后端不分离）web开发模式 开发模式 说明 前后端不分离 前端展示的数据效果最终是由后端进行控制的，由后端使用模板进行模板的渲染，将渲染后的内容返回给客户端进行显示 前后端分离 后端服务器只返回前端所需要数据，前端获取数据之后自己控制数据怎么进行展示 注意：前端发起ajax-&gt;后端服务器返回分类新闻数据-&gt;前端进行页面拼接和展示。 RestAPI接口风格前后端分离开发中广泛采用的一种API设计风格。​关键点：url地址尽量使用名词，不要出现动词。采用不同请求方式，执行不同操作。 GET 获取​POST 新增PUT 修改​DELETE 删除GET /books/: 获取所有图书，返回所有图书信息POST /books/: 新建一本图书，返回新建的图书信息GET /books/id/: 获取指定图书，返回指定图书信息PUT /books/id/: 修改指定图书，返回修改图书信息DELETE /books/id/: 删除指定图书，返回空文档 过滤参数放在查询字符串中。响应状态码选择。 200：获取或修改​201：新建​204：删除​400：客户端请求有误​404：客户端请求的资源找不到​500：服务器出错 响应数据返回json。 使用Django知识自定义RestAPI接口准备工作-&gt;创建应用-&gt;定义模板类-&gt;生成数据表-&gt;添加测试数据。 使用Django知识自定义RestAPI接口: 获取所有图书的信息 GET /books/ 新增一本图书信息 POST /books/ 获取指定的图书信息 GET /books/(?P\\d+)/ 修改指定的图书信息 PUT /books/(?P\\d+)/ 删除指定的图书信息 DELETE /books/(?P\\d+)/ RestAPI接口开发时的工作(序列化和反序列化) 开发模式 说明 序列化 将程序中一种数据结构类型转化为其他的数据格式，这个过程叫做序列化。在我们前面例子中，将模型对象转化为python字典或json数据，这个过程可以认为是序列化过程。 反序列化: 将其他格式数据转换为程序中数据，这个过程叫做反序列化。在我们前面例子中，将客户端发送的数据保存在模型对象中，这个过程可以认为是反序列化过程。 主要工作 ​ 1）将请求数据保存在模型对象中(反序列化)。​ 2）操作数据库。​ 3）将模型对象转换为前端所需的格式(序列化)。 ## 序列化器|视图类|拓展类序列化类的功能：进行序列化和反序列化 序列化：将对象转化为字典数据反序列化：1）数据校验 2）数据保存(新增&amp;更新)。定义序列化器类：继承Serializer或ModelSerializer 1234567891011121314151617181920212223242526272829from rest_framework import serializers# serializers.Serializer：定义任何序列化器类时，都可以直接继承于Serializer# serializers.ModelSerializer：如果定义的序列化器类针对的是一个模型类，可以直接继承ModelSerializerclass User(object): def __init__(self, username, age): self.username = username self.age = ageclass UserSerializer(serializers.Serializer): &quot;&quot;&quot;序列化器类&quot;&quot;&quot; # 序列化器字段 = serializers.字段类型(选项参数) username = serializers.CharField() age = serializers.IntegerField()if __name__ == &apos;__main__&apos;: user = User(&apos;smart&apos;, 18) # &#123; # &quot;username&quot;: &quot;smart&quot;, # &quot;age&quot;: 18 # &#125; serializer = UserSerializer(user) # 获取序列化之后的字典数据 serializer.data &quot;hello&quot; 1. 序列化器-序列化1）序列化单个对象 123book = BookInfo.objects.get(id=1)serializer = BookInfoSerializer(book)serializer.data 2）序列化多个对象 123books = BookInfo.objects.all()serializer = BookInfoSerializer(books, many=True)serializer.data 3）关联对象的序列化 123451. 将关联对象序列化为关联对象主键 PrimaryKeyRelatedFieldå2. 将关联对象使用指定的序列化器进行序列化3. 将关联对象序列化为关联对象模型类__str__方法的返回值 StringRelatedField注意：如果关联对象有多个，定义字段时，需要添加many=True 2. 序列化器-反序列化1）反序列化之数据校验 1234567891011data = &#123;'btitle': 'python'&#125;serializer = BookInfoSerializer(data=data)serializer.is_valid() # 调用此方法会对传递的data数据内容进行校验，校验成功返回True, 否则返回Falseserializer.errors # 获取校验失败的错误信息serializer.validated_data # 获取校验之后的数据# 补充验证行为1. 对对应的字段指定validators2. 序列化器类中定义对应的方法validate_&lt;field_name&gt;对指定的字段进行校验3. 如果校验需要结合多个字段内容，定义validate方法 2）反序列化之数据保存(新增或更新) 1234567891011121314151617# 数据校验之后，调用此方法可以进行数据保存，可能会调用序列化器类中的create或updateserializer.save() # 调用create，创建序列化器对象时只传递了datadata = &#123;'btitle': 'python'&#125;serializer = BookInfoSerializer(data=data)serializer.is_valid()serializer.save()# 调用update，创建序列化器对象时传递了data和对象book = BookInfo.objects.get(id=1)data = &#123;'btitle': 'python'&#125;serializer = BookInfoSerializer(book, data=data)serializer.is_valid()serializer.save()# 注意: Serializer类中create和update没有进行实现，需要自己实现代码。 3. 使用Serializer改写Django自定义RestAPI接口将序列化和反序列化部分代码使用序列化器完成。 4. APIView视图类&amp;Request对象&amp;Response对象1）APIView视图类 ​ APIView是DRF框架中所有视图类的父类。 ​ 继承自APIView之后，处理函数中的request参数不再是Django原始的HttpRequest对象，而是由DRF框架封装的Request类的对象。 ​ 进行异常处理。 ​ 认证&amp;权限&amp;限流。 2）Request类| 属性 | 说明 || :——– | ——–:|| equest.data: | 包含传递的请求体数据(比如之前的request.body和request.POST)，并且已经转换成了字典或类字典类型。 || 前后端分离 | 包含查询字符串参数(相当于之前的request.GET) |​3）Response类 ​ 通过Response返回响应数据时，会根据前端请求头中的Accept转化成对应的响应数据类型，仅支持json和html，默认返回json。 4）补充 类视图对象`self.kwargs`保存这从url地址中提取的所有命名参数。 5. 使用APIView改写Django自定义RestAPI接口将获取参数以及响应部分代码进行改写。 6. GenericAPIView视图类APIView类的子类，封装了操作序列化器和操作数据库的方法，经常和Mixin扩展类配合使用。 过滤&amp;分页。 属性： ​serializer_class: 指定视图使用的序列化器类​queryset: 指定视图使用的查询集 方法: get_serializer_class: 返回当前视图使用的序列化器类​get_serializer: 创建一个序列化器类的对象​get_queryset: 获取当前视图使用的查询集​get_object: 获取单个对象，默认根据主键进行查询 ##子类视图类|视图集|路由Router| 类名 | 说明 || ————– | ———————————————————— || APIView | 1）继承自View，封装了Django 本身的HttpRequest对象为Request对象。2）统一的异常处理。 3）认证&amp;权限&amp;限流。 || GenericAPIView | 1）继承自APIView，提供了操作序列化器和数据库数据的方法，通常和Mixin扩展类配合使用。2）过滤&amp;分页。 | 1. Mixin扩展类DRF提供了5个扩展类，封装了5个通用的操作流程。 类名 说明 ListModelMixin 提供了一个list方法，封装了返回模型数据列表信息的通用流程。 CreateModelMixin 提供了一个create方法，封装了创建一个模型对象数据信息的通用流程。 RetrieveModelMixin 提供了一个retrieve方法，封装了获取一个模型对象数据信息的通用流程。 UpdateModelMixin 提供了一个update方法，封装了更新一个模型对象数据信息的通用流程。 DestroyModelMixin 提供了一个destroy方法，封装了删除一个模型对象数据信息的通用流程。 2. 子类视图为了方便我们开发RestAPI，DRF框架除了提供APIView和GenericAPIView视图类之外，还提供了一些子类视图类，这些子类视图类同时继承了GenericAPIView和对应的Mixin扩展类，并且提供了对应的请求方法。 类名 说明 ListAPIView 1）继承自ListModelMixin和GenericAPIView。2）如果想定义一个视图只提供列出模型列表信息的接口，继承此视图类是最快的方式。 CreateAPIView 1）继承自CreateModelMixin和GenericAPIView。2）如果想定义一个视图只提供创建一个模型信息的接口，继承此视图类是最快的方式。 RetrieveAPIView 1）继承自RetrieveModelMixin和GenericAPIView。2）如果想定义一个视图只提供获取一个模型信息的接口，继承此视图类是最快的方式。 UpdateAPIView 1）继承自UpdateModelMixin和GenericAPIView。2）如果只想定义一个视图只提供更新一个模型信息的接口，继承此视图类是最快的方式。 DestroyAPIView 1）继承自DestroyModelMixin和GenericAPIView。2）如果只想定义一个视图只提供删除一个模型信息的接口，继承此视图类是最快的方式。 ListCreateAPIView 1）继承自ListModelMixin，CreateModelMixin和GenericAPIView。2）如果只想定义一个视图提供列出模型列表和创建一个模型信息的接口，继承此视图类是最快的方式。 RetrieveUpdateAPIView 1）继承自RetrieveModelMixin，UpdateModelMixin和GenericAPIView。2）如果只想定义一个视图提供获取一个模型信息和更新一个模型信息的接口，继承此视图类是最快的方式。 RetrieveDestroyAPIView 1）继承自RetrieveModelMixin，DestroyModelMixin和GenericAPIView。2）如果只想定义一个视图提供获取一个模型信息和删除一个模型信息的接口，继承此视图类是最快的方式。 RetrieveUpdateDestoryAPIView 1）继承自RetrieveModelMixin，UpdateModelMixin，DestroyModelMixin和GenericAPIView。2）如果只想定义一个视图提供获取一个模型信息和更新一个模型信息和删除一个模型信息的接口，继承此视图类是最快的方式。 示例1： 123456789101112131415161718# 需求1：写一个视图，提供一个接口 1. 获取一组图书数据 GET /books/ class BookListView(ListAPIView): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer # 需求2：写一个视图，提供一个接口 1. 获取指定的图书数据 GET /books/(?P&lt;pk&gt;\\d+)/ class BookDetailView(RetrieveAPIView): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer # 需求3：写一个视图，提供两个接口 1. 获取指定的图书数据 GET /books/(?P&lt;pk&gt;\\d+)/ 2. 更新指定的图书数据 PUT /books/(?P&lt;pk&gt;\\d+)/ class BookDetailView(RetrieveUpdateAPIView): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer 3. 视图集类将操作同一资源的处理方法放在同一个类中(视图集)，处理方法不要以请求方式命名，而是以对应的action命名， ​ list: 提供一组数据 ​ create: 创建一条新数据 ​ retrieve: 获取指定的数据 ​ update: 更新指定的数据 ​ destroy: 删除指定的数据 进行url配置时需要指明请求方法和处理函数之间的对应关系。 类名 说明 ViewSet 1）继承自ViewSetMixin和APIView。2）如果使用视图集时不涉及数据库和序列化器的操作，可以直接继承此类。 GenericViewSet 1）继承自ViewSetMixin和GenericAPIView。2）如果使用视图集涉及数据库和序列化器的操作，可以直接继承此类。 ModelViewSet 1）继承自5个Mixin扩展类和GenericViewSet。2）如果使用视图集想一次提供通用的5种操作，继承这个类是最快的。 ReadOnlyModelViewSet 1）继承自ListModelMixin，RetrieveModelMixin和GenericViewSet。2）如果使用视图集想一次提供list操作和retrieve操作，继承这个类是最快的。 示例1： 123456789101112131415161718192021# 需求1：写一个视图集，提供以下两种操作 1. 获取一组图书信息(list) GET /books/ 2. 新建一本图书信息(create) POST /books/ class BookInfoViewSet(ListModelMixin, CreateModelMixin, GenericViewSet): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer# 需求2：写一个视图集，提供以下两种操作 1. 获取一组图书信息(list) GET /books/ 2. 获取指定图书信息(retrieve) GET /books/(?P&lt;pk&gt;\\d+)/ class BookInfoViewSet(ReadOnlyModelViewSet): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer # 需求3：写一个视图集，提供以下三种操作 1. 获取一组图书信息(list) GET /books/ 2. 获取指定图书信息(retrieve) GET /books/(?P&lt;pk&gt;\\d+)/ 3. 更新指定图书信息(update) PUT /books/(?P&lt;pk&gt;\\d+)/ class BookInfoViewSet(UpdateModelMixin, ReadOnlyModelViewSet): queryset = BookInfo.objects.all() serializer_class = BookInfoSerializer 注: 除了常见的5种基本操作之外，如果想给一个视图集中添加其他处理方法，直接在视图集中定义即可。 4. 路由Router1）路由Router是专门配合视图集来使用的，可以使用Router自动生成视图集中相应处理函数对应的URL配置项。 2）使用Router自动生成视图集中相应处理函数对应的URL配置项时，除了常见的5种基本操作之外，如果视图集中有添加的其他处理方法，则需要给这些方法加上action装饰器之后，才会动态生成其对应的URL配置项。 其他功能1）认证&amp;权限 2）限流 控制用户访问API接口的频率。 针对匿名用户和认证用户分别进行限流。 1234567891011# 限流(针对匿名用户和认证用户分别进行限流控制)'DEFAULT_THROTTLE_CLASSES': ( 'rest_framework.throttling.AnonRateThrottle', # 针对匿名用户 'rest_framework.throttling.UserRateThrottle' # 针对认证用户),# 限流频次设置'DEFAULT_THROTTLE_RATES': &#123; 'user': '5/minute', # 认证用户5次每分钟 'anon': '3/minute', # 匿名用户3次每分钟&#125;, 针对匿名用户和认证用户统一进行限流。 123456789# 限流(针对匿名用户和认证用户进行统一限流控制)'DEFAULT_THROTTLE_CLASSES': ( 'rest_framework.throttling.ScopedRateThrottle',),'DEFAULT_THROTTLE_RATES': &#123; 'contacts': '5/minute', 'upload': '3/minute',&#125;, 3）过滤&amp;排序 4）分页 两种分页方式PageNumberPagination和LimitOffsetPagination。 使用PageNumberPagination分页时，获取分页数据时可以通过page传递页码参数。如果想要分页时指定页容量，需要自定义分页类。 1234567class StandardResultPagination(PageNumberPagination): # 默认页容量 page_size = 3 # 指定页容量参数名称 page_size_query_param = 'page_size' # 最大页容量 max_page_size = 5 使用LimitOffsetPagination分页时，获取分页数据时可以传递参数offset(偏移量)和limit(限制条数)。 注：如果使用的全局分页设置，某个列表视图如果不需要分页，直接在视图类中设置pagination_class = None。 5）异常 DRF自带异常处理功能，可以对某些特定的异常进行处理并返回给客户端组织好的错误信息。能够处理的异常如下: 12345678910APIException 所有异常的父类ParseError 解析错误AuthenticationFailed 认证失败NotAuthenticated 尚未认证PermissionDenied 权限决绝NotFound 未找到MethodNotAllowed 请求方式不支持NotAcceptable 要获取的数据格式不支持Throttled 超过限流次数ValidationError 校验失败 可以自定义DRF框架的异常处理函数(补充一些异常处理)并指定EXCEPTION_HANDLER配置项。 6）接口文档","categories":[{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/categories/前后端分离/"}],"tags":[{"name":"Django","slug":"Django","permalink":"https://zem12345678.github.io/tags/Django/"},{"name":"Restful","slug":"Restful","permalink":"https://zem12345678.github.io/tags/Restful/"},{"name":"前后端分离","slug":"前后端分离","permalink":"https://zem12345678.github.io/tags/前后端分离/"}]},{"title":"SpringcloudVue项目学习笔记","slug":"SpringcloudVue项目学习笔记","date":"2018-11-14T12:21:21.836Z","updated":"2019-03-13T15:15:09.301Z","comments":true,"path":"2018/11/14/SpringcloudVue项目学习笔记/","link":"","permalink":"https://zem12345678.github.io/2018/11/14/SpringcloudVue项目学习笔记/","excerpt":"","text":"springcloud-vue-project架构图","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zem12345678.github.io/categories/学习笔记/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://zem12345678.github.io/tags/SpringCloud/"},{"name":"Vue","slug":"Vue","permalink":"https://zem12345678.github.io/tags/Vue/"},{"name":"微服务","slug":"微服务","permalink":"https://zem12345678.github.io/tags/微服务/"},{"name":"Java","slug":"Java","permalink":"https://zem12345678.github.io/tags/Java/"}]},{"title":"day01-springboot","slug":"day01-springboot","date":"2018-11-14T12:13:33.696Z","updated":"2019-03-13T15:20:27.469Z","comments":true,"path":"2018/11/14/day01-springboot/","link":"","permalink":"https://zem12345678.github.io/2018/11/14/day01-springboot/","excerpt":"","text":"0.学习目标 了解SpringBoot的作用 掌握java配置的方式 了解SpringBoot自动配置原理 掌握SpringBoot的基本使用 了解Thymeleaf的基本使用 1. 了解SpringBoot在这一部分，我们主要了解以下3个问题： 什么是SpringBoot 为什么要学习SpringBoot SpringBoot的特点 1.1.什么是SpringBootSpringBoot是Spring项目中的一个子工程，与我们所熟知的Spring-framework 同属于spring的产品: 我们可以看到下面的一段介绍： Takes an opinionated view of building production-ready Spring applications. Spring Boot favors convention over configuration and is designed to get you up and running as quickly as possible. 翻译一下： 用一些固定的方式来构建生产级别的spring应用。Spring Boot 推崇约定大于配置的方式以便于你能够尽可能快速的启动并运行程序。 其实人们把Spring Boot 称为搭建程序的脚手架。其最主要作用就是帮我们快速的构建庞大的spring项目，并且尽可能的减少一切xml配置，做到开箱即用，迅速上手，让我们关注与业务而非配置。 1.2.为什么要学习SpringBootjava一直被人诟病的一点就是臃肿、麻烦。当我们还在辛苦的搭建项目时，可能Python程序员已经把功能写好了，究其原因注意是两点： 复杂的配置， 项目各种配置其实是开发时的损耗， 因为在思考 Spring 特性配置和解决业务问题之间需要进行思维切换，所以写配置挤占了写应用程序逻辑的时间。 一个是混乱的依赖管理。 项目的依赖管理也是件吃力不讨好的事情。决定项目里要用哪些库就已经够让人头痛的了，你还要知道这些库的哪个版本和其他库不会有冲突，这难题实在太棘手。并且，依赖管理也是一种损耗，添加依赖不是写应用程序代码。一旦选错了依赖的版本，随之而来的不兼容问题毫无疑问会是生产力杀手。 而SpringBoot让这一切成为过去！ Spring Boot 简化了基于Spring的应用开发，只需要“run”就能创建一个独立的、生产级别的Spring应用。Spring Boot为Spring平台及第三方库提供开箱即用的设置（提供默认设置，存放默认配置的包就是启动器），这样我们就可以简单的开始。多数Spring Boot应用只需要很少的Spring配置。 我们可以使用SpringBoot创建java应用，并使用java –jar 启动它，就能得到一个生产级别的web工程。 1.3.SpringBoot的特点Spring Boot 主要目标是： 为所有 Spring 的开发者提供一个非常快速的、广泛接受的入门体验 开箱即用（启动器starter-其实就是SpringBoot提供的一个jar包），但通过自己设置参数（.properties），即可快速摆脱这种方式。 提供了一些大型项目中常见的非功能性特性，如内嵌服务器、安全、指标，健康检测、外部化配置等 绝对没有代码生成，也无需 XML 配置。 更多细节，大家可以到官网查看。 2.快速入门接下来，我们就来利用SpringBoot搭建一个web工程，体会一下SpringBoot的魅力所在！ 2.1.创建工程我们先新建一个空的工程： 工程名为demo： 新建一个model： 使用maven来构建： 然后填写项目坐标： 目录结构： 项目结构： 2.2.添加依赖看到这里很多同学会有疑惑，前面说传统开发的问题之一就是依赖管理混乱，怎么这里我们还需要管理依赖呢？难道SpringBoot不帮我们管理吗？ 别着急，现在我们的项目与SpringBoot还没有什么关联。SpringBoot提供了一个名为spring-boot-starter-parent的工程，里面已经对各种常用依赖（并非全部）的版本进行了管理，我们的项目需要以这个项目为父工程，这样我们就不用操心依赖的版本问题了，需要什么依赖，直接引入坐标即可！ 2.2.1.添加父工程坐标12345&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;&lt;/parent&gt; 2.2.2.添加web启动器为了让SpringBoot帮我们完成各种自动配置，我们必须引入SpringBoot提供的自动配置依赖，我们称为启动器。因为我们是web项目，这里我们引入web启动器： 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 需要注意的是，我们并没有在这里指定版本信息。因为SpringBoot的父工程已经对版本进行了管理了。 这个时候，我们会发现项目中多出了大量的依赖： 这些都是SpringBoot根据spring-boot-starter-web这个依赖自动引入的，而且所有的版本都已经管理好，不会出现冲突。 2.2.3.管理jdk版本默认情况下，maven工程的jdk版本是1.5，而我们开发使用的是1.8，因此这里我们需要修改jdk版本，只需要简单的添加以下属性即可： 123&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt;&lt;/properties&gt; 2.2.4.完整pom123456789101112131415161718192021222324252627&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.leyou.demo&lt;/groupId&gt; &lt;artifactId&gt;springboot-demo&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 2.3.启动类Spring Boot项目通过main函数即可启动，我们需要创建一个启动类： 然后编写main函数： 123456@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 2.4.编写controller接下来，我们就可以像以前那样开发SpringMVC的项目了！ 我们编写一个controller： 代码： 12345678@RestControllerpublic class HelloController &#123; @GetMapping(\"hello\") public String hello()&#123; return \"hello, spring boot!\"; &#125;&#125; 2.5.启动测试接下来，我们运行main函数，查看控制台： 并且可以看到监听的端口信息： 1）监听的端口是8080 2）SpringMVC的映射路径是：/ 3）/hello路径已经映射到了HelloController中的hello()方法 打开页面访问：http://localhost:8080/hello 测试成功了！ 3.Java配置在入门案例中，我们没有任何的配置，就可以实现一个SpringMVC的项目了，快速、高效！ 但是有同学会有疑问，如果没有任何的xml，那么我们如果要配置一个Bean该怎么办？比如我们要配置一个数据库连接池，以前会这么玩： 1234567&lt;!-- 配置连接池 --&gt;&lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" init-method=\"init\" destroy-method=\"close\"&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\" /&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\" /&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\" /&gt;&lt;/bean&gt; 现在该怎么做呢？ 3.1.回顾历史事实上，在Spring3.0开始，Spring官方就已经开始推荐使用java配置来代替传统的xml配置了，我们不妨来回顾一下Spring的历史： Spring1.0时代 在此时因为jdk1.5刚刚出来，注解开发并未盛行，因此一切Spring配置都是xml格式，想象一下所有的bean都用xml配置，细思极恐啊，心疼那个时候的程序员2秒 Spring2.0时代 Spring引入了注解开发，但是因为并不完善，因此并未完全替代xml，此时的程序员往往是把xml与注解进行结合，貌似我们之前都是这种方式。 Spring3.0及以后 3.0以后Spring的注解已经非常完善了，因此Spring推荐大家使用完全的java配置来代替以前的xml，不过似乎在国内并未推广盛行。然后当SpringBoot来临，人们才慢慢认识到java配置的优雅。 有句古话说的好：拥抱变化，拥抱未来。所以我们也应该顺应时代潮流，做时尚的弄潮儿，一起来学习下java配置的玩法。 3.2.尝试java配置java配置主要靠java类和一些注解，比较常用的注解有： @Configuration：声明一个类作为配置类，代替xml文件 @Bean：声明在方法上，将方法的返回值加入Bean容器，代替&lt;bean&gt;标签 @value：属性注入 @PropertySource：指定外部属性文件， 我们接下来用java配置来尝试实现连接池配置： 首先引入Druid连接池依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 创建一个jdbc.properties文件，编写jdbc属性： 1234jdbc.driverClassName=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://127.0.0.1:3306/leyoujdbc.username=rootjdbc.password=123 然后编写代码： 1234567891011121314151617181920212223@Configuration@PropertySource(\"classpath:jdbc.properties\")public class JdbcConfig &#123; @Value(\"$&#123;jdbc.url&#125;\") String url; @Value(\"$&#123;jdbc.driverClassName&#125;\") String driverClassName; @Value(\"$&#123;jdbc.username&#125;\") String username; @Value(\"$&#123;jdbc.password&#125;\") String password; @Bean public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setDriverClassName(driverClassName); dataSource.setUsername(username); dataSource.setPassword(password); return dataSource; &#125;&#125; 解读： @Configuration：声明我们JdbcConfig是一个配置类 @PropertySource：指定属性文件的路径是:classpath:jdbc.properties 通过@Value为属性注入值 通过@Bean将 dataSource()方法声明为一个注册Bean的方法，Spring会自动调用该方法，将方法的返回值加入Spring容器中。 然后我们就可以在任意位置通过@Autowired注入DataSource了！ 我们在HelloController中测试： 1234567891011@RestControllerpublic class HelloController &#123; @Autowired private DataSource dataSource; @GetMapping(\"hello\") public String hello() &#123; return \"hello, spring boot!\" + dataSource; &#125;&#125; 然后Debug运行并查看： 属性注入成功了！ 3.3.SpringBoot的属性注入在上面的案例中，我们实验了java配置方式。不过属性注入使用的是@Value注解。这种方式虽然可行，但是不够强大，因为它只能注入基本类型值。 在SpringBoot中，提供了一种新的属性注入方式，支持各种java基本数据类型及复杂类型的注入。 1）我们新建一个类，用来进行属性注入： 123456789@ConfigurationProperties(prefix = \"jdbc\")public class JdbcProperties &#123; private String url; private String driverClassName; private String username; private String password; // ... 略 // getters 和 setters&#125; 在类上通过@ConfigurationProperties注解声明当前类为属性读取类 prefix=&quot;jdbc&quot;读取属性文件中，前缀为jdbc的值。 在类上定义各个属性，名称必须与属性文件中jdbc.后面部分一致 需要注意的是，这里我们并没有指定属性文件的地址，所以我们需要把jdbc.properties名称改为application.properties，这是SpringBoot默认读取的属性文件名： 2）在JdbcConfig中使用这个属性： 1234567891011121314@Configuration@EnableConfigurationProperties(JdbcProperties.class)public class JdbcConfig &#123; @Bean public DataSource dataSource(JdbcProperties jdbc) &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(jdbc.getUrl()); dataSource.setDriverClassName(jdbc.getDriverClassName()); dataSource.setUsername(jdbc.getUsername()); dataSource.setPassword(jdbc.getPassword()); return dataSource; &#125;&#125; 通过@EnableConfigurationProperties(JdbcProperties.class)来声明要使用JdbcProperties这个类的对象 然后你可以通过以下方式注入JdbcProperties： @Autowired注入 12@Autowiredprivate JdbcProperties prop; 构造函数注入 1234private JdbcProperties prop;public JdbcConfig(Jdbcproperties prop)&#123; this.prop = prop;&#125; 声明有@Bean的方法参数注入 1234@Beanpublic Datasource dataSource(JdbcProperties prop)&#123; // ...&#125; 本例中，我们采用第三种方式。 3）测试结果： 大家会觉得这种方式似乎更麻烦了，事实上这种方式有更强大的功能，也是SpringBoot推荐的注入方式。两者对比关系： 优势： Relaxed binding：松散绑定 不严格要求属性文件中的属性名与成员变量名一致。支持驼峰，中划线，下划线等等转换，甚至支持对象引导。比如：user.friend.name：代表的是user对象中的friend属性中的name属性，显然friend也是对象。@value注解就难以完成这样的注入方式。 meta-data support：元数据支持，帮助IDE生成属性提示（写开源框架会用到）。 ​ 3.4.更优雅的注入事实上，如果一段属性只有一个Bean需要使用，我们无需将其注入到一个类（JdbcProperties）中。而是直接在需要的地方声明即可： 1234567891011@Configurationpublic class JdbcConfig &#123; @Bean // 声明要注入的属性前缀，SpringBoot会自动把相关属性通过set方法注入到DataSource中 @ConfigurationProperties(prefix = \"jdbc\") public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); return dataSource; &#125;&#125; 我们直接把@ConfigurationProperties(prefix = &quot;jdbc&quot;)声明在需要使用的@Bean的方法上，然后SpringBoot就会自动调用这个Bean（此处是DataSource）的set方法，然后完成注入。使用的前提是：该类必须有对应属性的set方法！ 我们将jdbc的url改成：/heima，再次测试： 4.自动配置原理使用SpringBoot之后，一个整合了SpringMVC的WEB工程开发，变的无比简单，那些繁杂的配置都消失不见了，这是如何做到的？ 一切魔力的开始，都是从我们的main函数来的，所以我们再次来看下启动类： 我们发现特别的地方有两个： 注解：@SpringBootApplication run方法：SpringApplication.run() 我们分别来研究这两个部分。 4.1.了解@SpringBootApplication点击进入，查看源码： 这里重点的注解有3个： @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 4.1.1.@SpringBootConfiguration我们继续点击查看源码： 通过这段我们可以看出，在这个注解上面，又有一个@Configuration注解。通过上面的注释阅读我们知道：这个注解的作用就是声明当前类是一个配置类，然后Spring会自动扫描到添加了@Configuration的类，并且读取其中的配置信息。而@SpringBootConfiguration是来声明当前类是SpringBoot应用的配置类，项目中只能有一个。所以一般我们无需自己添加。 4.1.2.@EnableAutoConfiguration关于这个注解，官网上有一段说明： The second class-level annotation is @EnableAutoConfiguration. This annotationtells Spring Boot to “guess” how you want to configure Spring, based on the jardependencies that you have added. Since spring-boot-starter-web added Tomcatand Spring MVC, the auto-configuration assumes that you are developing a webapplication and sets up Spring accordingly. 简单翻译以下： 第二级的注解@EnableAutoConfiguration，告诉SpringBoot基于你所添加的依赖，去“猜测”你想要如何配置Spring。比如我们引入了spring-boot-starter-web，而这个启动器中帮我们添加了tomcat、SpringMVC的依赖。此时自动配置就知道你是要开发一个web应用，所以就帮你完成了web及SpringMVC的默认配置了！ 总结，SpringBoot内部对大量的第三方库或Spring内部库进行了默认配置，这些配置是否生效，取决于我们是否引入了对应库所需的依赖，如果有那么默认配置就会生效。 所以，我们使用SpringBoot构建一个项目，只需要引入所需框架的依赖，配置就可以交给SpringBoot处理了。除非你不希望使用SpringBoot的默认配置，它也提供了自定义配置的入口。 4.1.3.@ComponentScan我们跟进源码： 并没有看到什么特殊的地方。我们查看注释： 大概的意思： 配置组件扫描的指令。提供了类似与&lt;context:component-scan&gt;标签的作用 通过basePackageClasses或者basePackages属性来指定要扫描的包。如果没有指定这些属性，那么将从声明这个注解的类所在的包开始，扫描包及子包 而我们的@SpringBootApplication注解声明的类就是main函数所在的启动类，因此扫描的包是该类所在包及其子包。因此，一般启动类会放在一个比较前的包目录中。 4.2.默认配置原理4.2.1默认配置类通过刚才的学习，我们知道@EnableAutoConfiguration会开启SpringBoot的自动配置，并且根据你引入的依赖来生效对应的默认配置。那么问题来了： 这些默认配置是在哪里定义的呢？ 为何依赖引入就会触发配置呢？ 其实在我们的项目中，已经引入了一个依赖：spring-boot-autoconfigure，其中定义了大量自动配置类： 还有： 非常多，几乎涵盖了现在主流的开源框架，例如： redis jms amqp jdbc jackson mongodb jpa solr elasticsearch … 等等 我们来看一个我们熟悉的，例如SpringMVC，查看mvc 的自动配置类： 打开WebMvcAutoConfiguration： 我们看到这个类上的4个注解： @Configuration：声明这个类是一个配置类 @ConditionalOnWebApplication(type = Type.SERVLET) ConditionalOn，翻译就是在某个条件下，此处就是满足项目的类是是Type.SERVLET类型，也就是一个普通web工程，显然我们就是 @ConditionalOnClass({ Servlet.class, DispatcherServlet.class, WebMvcConfigurer.class }) 这里的条件是OnClass，也就是满足以下类存在：Servlet、DispatcherServlet、WebMvcConfigurer，其中Servlet只要引入了tomcat依赖自然会有，后两个需要引入SpringMVC才会有。这里就是判断你是否引入了相关依赖，引入依赖后该条件成立，当前类的配置才会生效！ @ConditionalOnMissingBean(WebMvcConfigurationSupport.class) 这个条件与上面不同，OnMissingBean，是说环境中没有指定的Bean这个才生效。其实这就是自定义配置的入口，也就是说，如果我们自己配置了一个WebMVCConfigurationSupport的类，那么这个默认配置就会失效！ 接着，我们查看该类中定义了什么： 视图解析器： 处理器适配器（HandlerAdapter）： 还有很多，这里就不一一截图了。 4.2.2.默认配置属性另外，这些默认配置的属性来自哪里呢？ 我们看到，这里通过@EnableAutoConfiguration注解引入了两个属性：WebMvcProperties和ResourceProperties。这不正是SpringBoot的属性注入玩法嘛。 我们查看这两个属性类： 找到了内部资源视图解析器的prefix和suffix属性。 ResourceProperties中主要定义了静态资源（.js,.html,.css等)的路径： 如果我们要覆盖这些默认属性，只需要在application.properties中定义与其前缀prefix和字段名一致的属性即可。 4.3.总结SpringBoot为我们提供了默认配置，而默认配置生效的条件一般有两个： 你引入了相关依赖 你自己没有配置Bean 1）启动器 所以，我们如果不想配置，只需要引入依赖即可，而依赖版本我们也不用操心，因为只要引入了SpringBoot提供的stater（启动器），就会自动管理依赖及版本了。 因此，玩SpringBoot的第一件事情，就是找启动器，SpringBoot提供了大量的默认启动器，参考课前资料中提供的《SpringBoot启动器.txt》 2）全局配置 另外，SpringBoot的默认配置，都会读取默认属性，而这些属性可以通过自定义application.properties文件来进行覆盖。这样虽然使用的还是默认配置，但是配置中的值改成了我们自定义的。 因此，玩SpringBoot的第二件事情，就是通过application.properties来覆盖默认属性值，形成自定义配置。我们需要知道SpringBoot的默认属性key，非常多，参考课前资料提供的：《SpringBoot全局属性.md》 属性文件支持两种格式，application.properties和application.yml yml的语法实例： 12345678jdbc: driverClassName: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/leyou username: root password: 123server: port: 80 5.SpringBoot实践接下来，我们来看看如何用SpringBoot来玩转以前的SSM,我们沿用之前讲解SSM用到的数据库tb_user和实体类User 5.1.整合SpringMVC虽然默认配置已经可以使用SpringMVC了，不过我们有时候需要进行自定义配置。 5.1.1.修改端口查看SpringBoot的全局属性可知，端口通过以下方式配置： 12# 映射端口server.port=80 重启服务后测试： 5.1.2.访问静态资源现在，我们的项目是一个jar工程，那么就没有webapp，我们的静态资源该放哪里呢？ 回顾我们上面看的源码，有一个叫做ResourceProperties的类，里面就定义了静态资源的默认查找路径： 默认的静态资源路径为： classpath:/META-INF/resources/ classpath:/resources/ classpath:/static/ classpath:/public 只要静态资源放在这些目录中任何一个，SpringMVC都会帮我们处理。 我们习惯会把静态资源放在classpath:/static/目录下。我们创建目录，并且添加一些静态资源： 重启项目后测试 5.1.3.添加拦截器拦截器也是我们经常需要使用的，在SpringBoot中该如何配置呢？ 拦截器不是一个普通属性，而是一个类，所以就要用到java配置方式了。在SpringBoot官方文档中有这么一段说明： If you want to keep Spring Boot MVC features and you want to add additional MVC configuration (interceptors, formatters, view controllers, and other features), you can add your own @Configuration class of type WebMvcConfigurer but without @EnableWebMvc. If you wish to provide custom instances of RequestMappingHandlerMapping, RequestMappingHandlerAdapter, or ExceptionHandlerExceptionResolver, you can declare a WebMvcRegistrationsAdapter instance to provide such components. If you want to take complete control of Spring MVC, you can add your own @Configuration annotated with @EnableWebMvc. 翻译： 如果你想要保持Spring Boot 的一些默认MVC特征，同时又想自定义一些MVC配置（包括：拦截器，格式化器, 视图控制器、消息转换器 等等），你应该让一个类实现WebMvcConfigurer，并且添加@Configuration注解，但是千万不要加@EnableWebMvc注解。如果你想要自定义HandlerMapping、HandlerAdapter、ExceptionResolver等组件，你可以创建一个WebMvcRegistrationsAdapter实例 来提供以上组件。 如果你想要完全自定义SpringMVC，不保留SpringBoot提供的一切特征，你可以自己定义类并且添加@Configuration注解和@EnableWebMvc注解 总结：通过实现WebMvcConfigurer并添加@Configuration注解来实现自定义部分SpringMvc配置。 首先我们定义一个拦截器： 12345678910111213141516171819public class LoginInterceptor implements HandlerInterceptor &#123; private Logger logger = LoggerFactory.getLogger(LoginInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; logger.debug(\"preHandle method is now running!\"); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) &#123; logger.debug(\"postHandle method is now running!\"); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) &#123; logger.debug(\"afterCompletion method is now running!\"); &#125;&#125; 然后，我们定义配置类，注册拦截器： 123456789101112131415161718192021@Configurationpublic class MvcConfig implements WebMvcConfigurer&#123; /** * 通过@Bean注解，将我们定义的拦截器注册到Spring容器 * @return */ @Bean public LoginInterceptor loginInterceptor()&#123; return new LoginInterceptor(); &#125; /** * 重写接口中的addInterceptors方法，添加自定义拦截器 * @param registry */ @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 通过registry来注册拦截器，通过addPathPatterns来添加拦截路径 registry.addInterceptor(this.loginInterceptor()).addPathPatterns(\"/**\"); &#125;&#125; 结构如下： 接下来运行并查看日志： 你会发现日志中什么都没有，因为我们记录的log级别是debug，默认是显示info以上，我们需要进行配置。 SpringBoot通过logging.level.*=debug来配置日志级别，*填写包名 12# 设置com.leyou包的日志级别为debuglogging.level.com.leyou=debug 再次运行查看： 1232018-05-05 17:50:01.811 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : preHandle method is now running!2018-05-05 17:50:01.854 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : postHandle method is now running!2018-05-05 17:50:01.854 DEBUG 4548 --- [p-nio-80-exec-1] com.leyou.interceptor.LoginInterceptor : afterCompletion method is now running! 5.2.整合jdbc和事务spring中的jdbc连接和事务是配置中的重要一环，在SpringBoot中该如何处理呢？ 答案是不需要处理，我们只要找到SpringBoot提供的启动器即可： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt; 当然，不要忘了数据库驱动，SpringBoot并不知道我们用的什么数据库，这里我们选择MySQL： 1234&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 至于事务，SpringBoot中通过注解来控制。就是我们熟知的@Transactional 123456789101112131415@Servicepublic class UserService &#123; @Autowired private UserMapper userMapper; public User queryById(Long id)&#123; return this.userMapper.selectByPrimaryKey(id); &#125; @Transactional public void deleteById(Long id)&#123; this.userMapper.deleteByPrimaryKey(id); &#125;&#125; 5.3.整合连接池其实，在刚才引入jdbc启动器的时候，SpringBoot已经自动帮我们引入了一个连接池：HikariCP应该是目前速度最快的连接池了，我们看看它与c3p0的对比： 因此，我们只需要指定连接池参数即可： 123456spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/heima30 username: root password: 123 当然，如果你更喜欢Druid连接池，也可以使用Druid官方提供的启动器： 123456&lt;!-- Druid连接池 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt; 而连接信息的配置与上面是类似的，只不过在连接池特有属性上，方式略有不同： 12345678910#初始化连接数spring.datasource.druid.initial-size=1#最小空闲连接spring.datasource.druid.min-idle=1#最大活动连接spring.datasource.druid.max-active=20#获取连接时测试是否可用spring.datasource.druid.test-on-borrow=true#监控页面启动spring.datasource.druid.stat-view-servlet.allow=true 5.4.整合mybatis5.4.1.mybatisSpringBoot官方并没有提供Mybatis的启动器，不过Mybatis官网自己实现了： 123456&lt;!--mybatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 配置，基本没有需要配置的： 1234# mybatis 别名扫描mybatis.type-aliases-package=com.heima.pojo# mapper.xml文件位置,如果没有映射文件，请注释掉mybatis.mapper-locations=classpath:mappers/*.xml 需要注意，这里没有配置mapper接口扫描包，因此我们需要给每一个Mapper接口添加@Mapper注解，才能被识别。 123@Mapperpublic interface UserMapper &#123;&#125; 或者，我们也可以不加注解，而是在启动类上添加扫描包注解： 12345678@SpringBootApplication@MapperScan(\"cn.itcast.demo.mapper\")public class Application &#123; public static void main(String[] args) &#123; // 启动代码 SpringApplication.run(Application.class, args); &#125;&#125; 5.4.2.通用mapper通用Mapper的作者也为自己的插件编写了启动器，我们直接引入即可： 123456&lt;!-- 通用mapper --&gt;&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt;&lt;/dependency&gt; 不需要做任何配置就可以使用了。 123@Mapperpublic interface UserMapper extends tk.mybatis.mapper.common.Mapper&lt;User&gt;&#123;&#125; 5.5.启动测试将controller进行简单改造： 123456789101112@RestControllerpublic class HelloController &#123; @Autowired private UserService userService; @GetMapping(\"/hello\") public User hello() &#123; User user = this.userService.queryById(8L); return user; &#125;&#125; 我们启动项目，查看： 6.JDK1.8参考课前资料：JDK1.8新特性.md","categories":[{"name":"随堂笔记","slug":"随堂笔记","permalink":"https://zem12345678.github.io/categories/随堂笔记/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://zem12345678.github.io/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zem12345678.github.io/tags/SpringBoot/"},{"name":"Spring","slug":"Spring","permalink":"https://zem12345678.github.io/tags/Spring/"}]},{"title":"RabbitMQ简介","slug":"RabbitMQ简介","date":"2018-10-19T03:29:16.775Z","updated":"2018-10-19T04:28:44.379Z","comments":true,"path":"2018/10/19/RabbitMQ简介/","link":"","permalink":"https://zem12345678.github.io/2018/10/19/RabbitMQ简介/","excerpt":"","text":"RabbitMQ是一个由erlang开发的AMQP（Advanced Message Queue ）的开源实现。AMQP 的出现其实也是应了广大人民群众的需求，虽然在同步消息通讯的世界里有很多公开标准（如 COBAR的 IIOP ，或者是 SOAP 等），但是在异步消息处理中却不是这样，只有大企业有一些商业实现（如微软的 MSMQ ，IBM 的 Websphere MQ 等），因此，在 2006 年的 6 月，Cisco 、Redhat、iMatix 等联合制定了 AMQP 的公开标准。 RabbitMQ是由RabbitMQ Technologies Ltd开发并且提供商业支持的。该公司在2010年4月被SpringSource（VMWare的一个部门）收购。在2013年5月被并入Pivotal。其实VMWare，Pivotal和EMC本质上是一家的。不同的是VMWare是独立上市子公司，而Pivotal是整合了EMC的某些资源，现在并没有上市。 RabbitMQ的官网是http://www.rabbitmq.comRabbitMQ 最初起源于金融系统，用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。具体特点包括： 什么叫消息队列(MQ)消息（Message）是指在应用间传送的数据。消息可以非常简单，比如只包含文本字符串，也可以更复杂，可能包含嵌入对象。 消息队列（Message Queue）是一种应用程序对应用程序的通信方法，消息发送后可以立即返回，由消息系统来确保消息的可靠传递。消息发布者只管把消息发布到 MQ 中而不用管谁来取，消息使用者只管从 MQ 中取消息而不管是谁发布的。这样发布者和使用者都不用知道对方的存在。通过消息队列通信，让A，B两个服务指间保持低耦合，实现业务的灵活拓展。 为何用消息队列(MQ)从上面的描述中可以看出消息队列是一种应用间的异步协作机制，那什么时候需要使用 MQ 呢？ 以常见的订单系统为例，用户点击【下单】按钮之后的业务逻辑可能包括：扣减库存、生成相应单据、发红包、发短信通知。在业务发展初期这些逻辑可能放在一起同步执行，随着业务的发展订单量增长，需要提升系统服务的性能，这时可以将一些不需要立即生效的操作拆分出来异步执行，比如发放红包、发短信通知等。这种场景下就可以用 MQ ，在下单的主流程（比如扣减库存、生成相应单据）完成之后发送一条消息到 MQ 让主流程快速完结，而由另外的单独线程拉取MQ的消息（或者由 MQ 推送消息），当发现 MQ 中有发红包或发短信之类的消息时，执行相应的业务逻辑。 以上是用于业务解耦的情况，其它常见场景包括最终一致性、广播、错峰流控等等。 RabbitMQ 特点 可靠性（Reliability）RabbitMQ 使用一些机制来保证可靠性，如持久化、传输确认、发布确认。 灵活的路由（Flexible Routing）在消息进入队列之前，通过 Exchange 来路由消息的。对于典型的路由功能，RabbitMQ 已经提供了一些内置的 Exchange 来实现。针对更复杂的路由功能，可以将多个 Exchange 绑定在一起，也通过插件机制实现自己的 Exchange 。 消息集群（Clustering）多个 RabbitMQ 服务器可以组成一个集群，形成一个逻辑 Broker 。 高可用（Highly Available Queues）队列可以在集群中的机器上进行镜像，使得在部分节点出问题的情况下队列仍然可用。 多种协议（Multi-protocol）RabbitMQ 支持多种消息队列协议，比如 STOMP、MQTT 等等。 多语言客户端（Many Clients）RabbitMQ 几乎支持所有常用语言，比如 Java、.NET、Ruby ,python,等等。 管理界面（Management UI）RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息 Broker 的许多方面。 跟踪机制（Tracing）如果消息异常，RabbitMQ 提供了消息跟踪机制，使用者可以找出发生了什么。 插件机制（Plugin System）RabbitMQ 提供了许多插件，来从多方面进行扩展，也可以编写自己的插件。 RabbitMQ 中的概念模型消息模型所有 MQ 产品从模型抽象上来说都是一样的过程：消费者（consumer）订阅某个队列。生产者（producer）创建消息，然后发布到队列（queue）中，最后将消息发送到监听的消费者。 RabbitMQ 基本概念上面只是最简单抽象的描述，具体到 RabbitMQ 则有更详细的概念需要解释。上面介绍过 RabbitMQ 是 AMQP 协议的一个开源实现，所以其内部实际上也是 AMQP 中的基本概念： Message消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher消息的生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。 Binding绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Queue消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Connection网络连接，比如一个TCP连接。 Channel信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker表示消息队列服务器实体。AMQP 中的消息路由AMQP 中消息的路由过程和 Java 开发者熟悉的 JMS 存在一些差别，AMQP 中增加了 Exchange 和 Binding 的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。 Exchange 类型Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： direct 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routing key 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配、单播的模式。 fanout 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 topic topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。它同样也会识别两个通配符：符号“#”和符号“”。#匹配0个或多个单词，匹配不多不少一个单词。","categories":[{"name":"异步处理","slug":"异步处理","permalink":"https://zem12345678.github.io/categories/异步处理/"}],"tags":[{"name":"消息队列","slug":"消息队列","permalink":"https://zem12345678.github.io/tags/消息队列/"},{"name":"分布式","slug":"分布式","permalink":"https://zem12345678.github.io/tags/分布式/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"https://zem12345678.github.io/tags/RabbitMQ/"}]},{"title":"Hadoop、Storm、Spark","slug":"Hadoop、Storm、Spark","date":"2018-10-18T07:28:29.350Z","updated":"2018-10-18T07:32:03.219Z","comments":true,"path":"2018/10/18/Hadoop、Storm、Spark/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/Hadoop、Storm、Spark/","excerpt":"","text":"Storm与Spark、Hadoop这三种框架，各有各的优点，每个框架都有自己的最佳应用场景。所以，在不同的应用场景下，应该选择不同的框架。 StormStorm是最佳的流式计算框架，Storm由Java和Clojure写成，Storm的优点是全内存计算，所以它的定位是分布式实时计算系统，按照Storm作者的说法，Storm对于实时计算的意义类似于Hadoop对于批处理的意义。 Storm的适用场景：1）流数据处理Storm可以用来处理源源不断流进来的消息，处理之后将结果写入到某个存储中去。2）分布式RPC。由于Storm的处理组件是分布式的，而且处理延迟极低，所以可以作为一个通用的分布式RPC框架来使用。 sparkSparkSpark是一个基于内存计算的开源集群计算系统，目的是更快速的进行数据分析。Spark由加州伯克利大学AMP实验室Matei为主的小团队使用Scala开发开发，类似于Hadoop MapReduce的通用并行计算框架，Spark基于Map Reduce算法实现的分布式计算，拥有Hadoop MapReduce所具有的优点，但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的Map Reduce的算法。 Spark的适用场景：1）多次操作特定数据集的应用场合Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。2）粗粒度更新状态的应用由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如Web服务的存储或者是增量的Web爬虫和索引。就是对于那种增量修改的应用模型不适合。总的来说Spark的适用面比较广泛且比较通用。 hadoopHadoop是实现了MapReduce的思想，将数据切片计算来处理大量的离线数据数据。Hadoop处理的数据必须是已经存放在HDFS上或者类似HBase的数据库中，所以Hadoop实现的时候是通过移动计算到这些存放数据的机器上来提高效率。 Hadoop的适用场景：1）海量数据的离线分析处理2）大规模Web信息搜索3）数据密集型并行计算 简单来说：Hadoop适合于离线的批量数据处理适用于对实时性要求极低的场景Storm适合于实时流数据处理，实时性方面做得极好Spark是内存分布式计算框架，试图吞并Hadoop的Map-Reduce批处理框架和Storm的流处理框架，但是Spark已经做得很不错了，批处理方面性能优于Map-Reduce，但是流处理目前还是弱于Storm，产品仍在改进之中","categories":[{"name":"大数据","slug":"大数据","permalink":"https://zem12345678.github.io/categories/大数据/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://zem12345678.github.io/tags/大数据/"},{"name":"Hadoop","slug":"Hadoop","permalink":"https://zem12345678.github.io/tags/Hadoop/"}]},{"title":"python基础小谈","slug":"python基础小谈","date":"2018-10-18T06:55:13.351Z","updated":"2018-10-18T07:07:25.385Z","comments":true,"path":"2018/10/18/python基础小谈/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/python基础小谈/","excerpt":"","text":"python语音是动态解释类型的，被称为胶水语言，再python的底层函数我们会经常看到两个形参*args,**kwargs，那么它们的本质是什么，什么使用它们呢？ 一 .*args 和 **kwargs 是什么？*args本质是一个tuple（元组），**kwargs本质是一个dict（字典）。 二.怎么用 *args 和 **kwargs?def my_fun(*args, **kwargs ): print (‘args = ‘, args) print (‘kwargs = ‘, kwargs) 调用就比较有意思了，传统的比如，c, c++, Java, C#，基本都是一对一传参，但是python靠这两个参数，可以实现多参的灵活传入。如下所示，我完全可以这么调用： my_fun(1,3,5,9, a=2, b=4) 这样打印的结果： args = (1,3,5,9) # 是一个元组 kwargs = { ‘a’: 2 , ‘b’:4 } #是一个字典 注意事项：上述函数 my_fun，如果这么调用就会有问题： my_fun( a=2, b=4, 1,3,5,9 ) 报错：SyntaxError: non-keyword arg after keyword arg” 意思是：关键字参数后面不能有非关键字参数，言外之意，关键字参数 * kwargs 必须位于 args 之后！","categories":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"}]},{"title":"Thinking In Python Language","slug":"Thinking In Python Language","date":"2018-10-18T05:54:54.070Z","updated":"2018-11-14T03:10:47.353Z","comments":true,"path":"2018/10/18/Thinking In Python Language/","link":"","permalink":"https://zem12345678.github.io/2018/10/18/Thinking In Python Language/","excerpt":"","text":"1.前言本文诞生于利用 Topic Reading 方法读 Python 若干本技术书籍这个过程中结合自己的开发常见场景记录下来的一些笔记。 2.简介1. 为什么是 Python Python, 很大程度上是因为 Python 的快速开发。 当然，快速开发（这里的开发包含部署）这个词也往往会被误解。什么叫做快速？我用一个 CMS 框架快速搭建出一个网站这是否叫做快速？ 每一次部署的时候，如果使用 Java 或者是 Go, 部署的时候直接 maven 编译打包，接着把 War 包直接上传到 Tomcat 就结束了。而用 Python 则需要各种虚拟环境，各种稀里哗啦的配置。这种情况下是哪一种快速呢？Python 有什么好处呢？ 写代码效率高。 生态圈好。 写代码效率高，这指的是写 Python 代码，而不是运行时。3.写代码效率高，这指的是写 Python 代码，而不是运行时。 生态圈好，Web 开发用 Django/Flask , 数据抓取用 Requests , 数据分析清洗用 Pandas, 机器学习。 2. 工具链 Anaconda工具：https://www.anaconda.com/download/ 3. 文档 官方文档：https://docs.python.org/3/ 4. 社区 官方社区：https://www.python.org/community/ 4. 书籍 《python核心编程》，《python编程从入门到实践》 3. 基本概念 程序 = 算法 + 数据结构 这句话当然是不全面的，但并不影响这句话在计算机世界里面的地位。依我看来，对我的启发大致是：我会把 API 的调用和数据结构以及算法想清楚，然后才动手把代码分解成伪代码。 1.数据类型数据类型按照不同的划分标准可以进行不同的划分： 按照复杂性可以这么划分： 简单类型 复杂类型】 按照复杂性可以这么划分： 基本类型 引用类型 按照数据结构可以这么划分： 集合结构 : 串 线性结构 : 线性表 （单链表，静态链表，循环链表，双向链表，栈，队列) 树形结构 : 树（二叉树，B+ 树，红黑树） 图形结构 : 图 2. 操作对于一些基本的数据类型，操作为 加减乘除取余数位运算等等 对于复杂的一些数据类型，则需要对数据结构多一些了解。 比如，对队列而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对 hash 而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对字典而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？比如，对字符串而言，增删改查在算法复杂度上意味着什么？对机器的性能会不会有很多影响呢？ 那字符串来说，Java 推荐使用 StringBuilder 来合并多个字符串，Python 推荐 join 多个字符串等等。 4.1.函数2.作用域3.模块模块，这个概念，可大可小，大的时候，把一个程序说成是模块，小的时候，可以把一个文件，甚至你说这一个函数是一个模块，也行。 这里的模块指的是一个包下的函数。 4.面向对象面向对象有三大概念： 封装 继承 多态5.错误 / 调试测试异常处理实际上可以考验一个程序员编写代码的健壮性。 事实上来说，代码写的健壮是一个程序员必备的素养。但其实在开发过程中，出于对项目进行赶工上线，需要对程序的健壮性做出一定的取舍。并且，在编写客户端，服务端，网页前端的时候基本上都会遇到这个问题。什么时候选择健壮的程序，什么时候选择是还可以的程序。需要自己的经验。 6. IO 编程7.进程和线程1.多线程 Python 多线程约等于并发。 2.多进程3.GILGlobal Interpreter Lock 并不是所有的解释器语言都有 GIL （尽管 Python 和 Ruby 里面都有）, 也并不是没有尝试过去除 GIL, 但是每次去除都会导致单线程性能的下降。所以暂时保留。 GIL 对程序中的影响： 一个线程运行 Python , 而其他 N 个睡眠或者等待 I/O - 同一时刻只有一个线程对共享资源进行存取 , Python 线程也可以等待 threading.Lock 或者线程模块中的其他同步对象； 协同式多任务处理如果有两个线程，同时进行 IO 请求，当其中一个线程连接之后，立即会主动让出 GIL, 其他线程就可以运行。 当N 个线程在网络 I/O 堵塞，或等待重新获取 GIL，而一个线程运行 Python。 让出之后还要执行代码呀，所以要有个收回 GIL 的动作。 抢占式多任务处理Python 2 GIL , 尝试收回 GIL 为 执行 1000 字节码。Python 3 GIL , 尝试收回 GIL 检测间隔为 15ms 线程安全原子操作：sort 之类不需要非原子操作：n=n+2 的字节码分为 加载 n , 加载 2 , 相加，存储 n, 四个步骤，由于不是原子性，很可能被由于 15 ms 而被打断。 当然，懒人一向是 : 优先级不决加括号，线程不决加 lock 对于 Java, 程序员努力在尽可能短的时间内加锁存取共享数据，减轻线程的争夺，实现最大并行。但 Python 中，线程无法并行运行，细粒度的锁就没有了优势。 8.正则表达式5.高级技巧6.标准库常用内建模块系统化模块IntroductionBuilt-in FunctionsBuilt-in ConstantsBuilt-in TypesBuilt-in ExceptionsText Processing ServicesBinary Data ServicesData TypesNumeric and Mathematical ModulesFunctional Programming ModulesFile and Directory AccessData PersistenceData Compression and ArchivingFile FormatsCryptographic ServicesGeneric Operating System ServicesConcurrent ExecutionInterprocess Communication and NetworkingInternet Data HandlingStructured Markup Processing ToolsInternet Protocols and SupportMultimedia ServicesInternationalizationProgram FrameworksGraphical User Interfaces with TkDevelopment ToolsDebugging and ProfilingSoftware Packaging and DistributionPython Runtime ServicesCustom Python InterpretersImporting ModulesPython Language ServicesMiscellaneous ServicesMS Windows Specific ServicesUnix Specific ServicesSuperseded ModulesUndocumented Modules 7.第三方库Requests : API 人性化 8.代码质量1.正确性 外部不该引用 protected member （单下划线）lambda 为一次使用，最好不要赋值。不要给 buildin 函数赋值py3 直接 super()for in else 如果不内置 break 则出会在最后 for in 为 empty 的时候再执行 else 中的语句context exit 如果不 catch 掉异常让其自然向上一级抛出错误的话，必须为 (self, exception_type, exception_value, traceback):不要在 init 里面 return 数据不要混用 tab 和 space4 个 space 缩进staticmethod 直接是 参数，classmethod 第一个参数为 cls可变的 default value 是不能作为 参数的。（可能是解释器在确定函数的定义的时候完成赋值？)遵循 exception hierachy https://docs.python.org/3/library/exceptions.html#exception-hierarchydefaultdict defaultdict(lambda : 6) , 必须 callable尽量 unpack 赋值字典用获取用 get(“myk”,None) , 赋值用 dictionary.setdefault(“list”, []).append(“list_item”) 2.可维护性 避免使用 import * , 我觉得这点值得商榷 , 如果是某个模块下，完全可以先把模块拆分成多个，最后 import 进来，接着使用 all.getxxx 获取实际值，如果不为实际值，返回 None 显然不如 try catch 来的实在。避免使用 global命名要注意动态创建方法 , 我觉得这点值得商榷。 3.可读性 不要检查，如果可能有异常，尽量抛出异常来 trycatch 解决。a is None , if flagisinstance , not type(r) is types.ListType“{name}{city}”.format(**info_dict)for k , v in infodict.items()使用 poiinfo = namedtuple(“poiinfo”,[“name”,”lng”,”lat”]) 返回 poiinfo[‘上海’,121.00,23] 最后返回值打印 poi.name , poi.lng , poi latfor numbers_value, letters_value in zip(numbers, letters):enumerate如果能用 listcomp 则不使用 map 和 filter 4.安全性5.性能","categories":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://zem12345678.github.io/tags/Python/"},{"name":"杂谈","slug":"杂谈","permalink":"https://zem12345678.github.io/tags/杂谈/"}]},{"title":"Jquery ajax, Axios, Fetch区别浅谈","slug":"Jquery ajax, Axios, Fetch区别浅谈","date":"2018-10-16T10:57:03.472Z","updated":"2018-10-16T12:26:10.813Z","comments":true,"path":"2018/10/16/Jquery ajax, Axios, Fetch区别浅谈/","link":"","permalink":"https://zem12345678.github.io/2018/10/16/Jquery ajax, Axios, Fetch区别浅谈/","excerpt":"","text":"前端技术是一个发展飞快的领域,JQuery ajax早已不能专美于前，axios和fetch都已经开始分别抢占“请求”这个前端高地。 1 JQuery ajax：廉颇老矣。尚能饭，但总有饭不动的一天。1234567891011$.ajax(&#123; type: &apos;POST&apos;, url: url, data: data, dataType: dataType, success: function () &#123;&#125;, error: function () &#123;&#125;&#125;); 这个我就不用多言了把，是对原生XHR的封装，除此以外还增添了对JSONP的支持。有一说一的说一句，JQuery ajax经过多年的更新维护，真的已经是非常的方便了，优点无需多言；如果是硬要举出几个缺点，那可能只有: 本身是针对MVC的编程,不符合现在前端MVVM的浪潮 基于原生的XHR开发，XHR本身的架构不清晰，已经有了fetch的替代方案 JQuery整个项目太大，单纯使用ajax却要引入整个JQuery非常的不合理（采取个性化打包的方案又不能享受CDN服务） 尽管JQuery对我们前端的开发工作曾有着（现在也仍然有着）深远的影响，但是我们可以看到随着VUE，REACT新一代框架的兴起，以及ES规范的完善，更多API的更新，JQuery这种大而全的JS库，未来的路会越走越窄。 2 Axios： 谁敢横刀立马，唯我Axios大将军！1234567891011121314axios(&#123; method: &apos;post&apos;, url: &apos;/user/12345&apos;, data: &#123; firstName: &apos;Fred&apos;, lastName: &apos;Flintstone&apos; &#125;&#125;).then(function (response) &#123; console.log(response);&#125;).catch(function (error) &#123; console.log(error);&#125;); Vue2.0之后，尤雨溪推荐大家用axios替换JQuery ajax，想必让Axios进入了很多人的目光中。Axios本质上也是对原生XHR的封装，只不过它是Promise的实现版本，符合最新的ES规范，从它的官网上可以看到它有以下几条特性： 从 node.js 创建 http 请求 支持 Promise API ； 客户端支持防止CSRF 提供了一些并发请求的接口（重要，方便了很多的操作） 这个支持防止CSRF其实挺好玩的，是怎么做到的呢，就是让你的每个请求都带一个从cookie中拿到的key, 根据浏览器同源策略，假冒的网站是拿不到你cookie中得key的，这样，后台就可以轻松辨别出这个请求是否是用户在假冒网站上的误导输入，从而采取正确的策略。Axios既提供了并发的封装，也没有下文会提到的fetch的各种问题，而且体积也较小，当之无愧现在最应该选用的请求的方式。 3 Fetch ：酋长的孩子,还需成长fetch号称是AJAX的替代品，它的好处在《传统 Ajax 已死，Fetch 永生》中提到有以下几点： 符合关注分离，没有将输入、输出和用事件来跟踪的状态混杂在一个对象里 更好更方便的写法，诸如： 1234try &#123; let response = await fetch(url); let data = response.json(); console.log(data);&#125; catch(e) &#123; console.log(&quot;Oops, error&quot;, e);&#125; 坦白说，上面的理由对我来说完全没有什么说服力，因为不管是Jquery还是Axios都已经帮我们把xhr封装的足够好，使用起来也足够方便，为什么我们还要花费大力气去学习fetch？我认为fetch的优势主要优势就是： 更加底层，提供的API丰富（request, response） 脱离了XHR，是ES规范里新的实现方式 偶尔觉得写的丑陋，但是在使用了JQuery和axios之后，已经对这一块完全无所谓了。当然，如果新的fetch能做的同样好，我为了不掉队也会选择使用fetch。这个道理其实很好理解：你有一架歼8，魔改了N次，性能达到了歼10的水准，但是要是有个人给你拿来一架新的歼10，你也会毫不犹豫的选择新的歼10——不仅仅是新，也代表了还有新的魔改潜力。但是我最近在使用fetch的时候，也遇到了不少的问题 fetch是一个低层次的API，你可以把它考虑成原生的XHR，所以使用起来并不是那么舒服，需要进行封装 例如： 1）fetch只对网络请求报错，对400，500都当做成功的请求，需要封装去处理2）fetch默认不会带cookie，需要添加配置项3）fetch不支持abort，不支持超时控制，使用setTimeout及Promise.reject的实现的超时控制并不能阻止请求过程继续在后台运行，造成了流量的浪费4）fetch没有办法原生监测请求的进度，而XHR可以 PS: fetch的具体问题大家可以参考：《fetch没有你想象的那么美》《fetch使用的常见问题及解决方法》 看到这里，你心里一定有个疑问，这鬼东西就是个半拉子工程嘛，我还是回去用Jquery或者Axios算了——其实我就是这么打算的。但是，必须要提出的是，我发现fetch在前端的应用上有一项xhr怎么也比不上的能力：跨域的处理。 我们都知道因为同源策略的问题，浏览器的请求是可能随便跨域的——一定要有跨域头或者借助JSONP，但是，fetch中可以设置mode为”no-cors”（不跨域），如下所示： 123fetch(&apos;/users.json&apos;, &#123; method: &apos;post&apos;, mode: &apos;no-cors&apos;, data: &#123;&#125;&#125;).then(function() &#123; /* handle response */ &#125;); 这样之后我们会得到一个type为“opaque”的返回。需要指出的是，这个请求是真正抵达过后台的，所以我们可以使用这种方法来进行信息上报，在我们之前的image.src方法中多出了一种选择，另外，我们在network中可以看到这个请求后台设置跨域头之后的实际返回，有助于我们提前调试接口（当然，通过chrome插件我们也可以做的到）。总之，fetch现在还不是很好用，我尝试过几个fetch封装的包，都还不尽如人意。 总结现在只需要知道无脑使用axios即可，Jquery老迈笨拙，fetch年轻稚嫩，只有Axios正当其年！","categories":[{"name":"前端","slug":"前端","permalink":"https://zem12345678.github.io/categories/前端/"}],"tags":[{"name":"前端","slug":"前端","permalink":"https://zem12345678.github.io/tags/前端/"},{"name":"跨域","slug":"跨域","permalink":"https://zem12345678.github.io/tags/跨域/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-10-16T03:24:15.311Z","updated":"2018-10-16T03:24:15.311Z","comments":true,"path":"2018/10/16/hello-world/","link":"","permalink":"https://zem12345678.github.io/2018/10/16/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}