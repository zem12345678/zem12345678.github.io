<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Scrapy 深入 | Enmin`s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="Enmin,ZEnmin's Blog" />
  
  <meta name="description" content="Item Pipeline当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。 每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：  验证爬取的数据(检查item包含某些字段，比如说name字段) 查重(并丢弃) 将">
<meta name="keywords" content="爬虫,Scrapy">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy 深入">
<meta property="og:url" content="https://zem12345678.github.io/2019/05/17/Scrapy深入/index.html">
<meta property="og:site_name" content="Enmin`s blog">
<meta property="og:description" content="Item Pipeline当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。 每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：  验证爬取的数据(检查item包含某些字段，比如说name字段) 查重(并丢弃) 将">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://s2.ax1x.com/2019/05/17/ELasQ1.jpg">
<meta property="og:updated_time" content="2019-05-17T12:16:32.493Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scrapy 深入">
<meta name="twitter:description" content="Item Pipeline当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。 每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：  验证爬取的数据(检查item包含某些字段，比如说name字段) 查重(并丢弃) 将">
<meta name="twitter:image" content="https://s2.ax1x.com/2019/05/17/ELasQ1.jpg">
  
  
    <link rel="icon" href="/favicon1.ico">
  
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  <script src="/js/pace.min.js"></script>
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-127786403-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?4802669a9f01ae631292334aca36a944";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

  
  <div style="display: none;">
    <script src="//s22.cnzz.com/z_stat.php?id=1275093534&web_id=1275093534" language="JavaScript"></script>
  </div>


</head>

<body>
  <div id="container">
      <header id="header">
    <div id="banner"></div>
    <div id="header-outer">
        <div id="header-menu" class="header-menu-pos animated">
            <div class="header-menu-container">
                <a href="/" class="left">
                    <span class="site-title">Enmin&#39;s Blog</span>
                </a>
                <nav id="header-menu-nav" class="right">
                    
                    <a  href="/">
                        <i class="fa fa-home"></i>
                        <span>主页</span>
                    </a>
                    
                    <a  href="/archives">
                        <i class="fa fa-archive"></i>
                        <span>文章</span>
                    </a>
                    
                    <a  href="/about">
                        <i class="fa fa-user"></i>
                        <span>关于我</span>
                    </a>
                    
                </nav>
                <a class="mobile-header-menu-button">
                    <i class="fa fa-bars"></i>
                </a>
            </div>
        </div>
        <div id="header-row">
            <div id="logo">
                <a href="/">
                    <img src="/images/logo.png" alt="logo">
                </a>
            </div>
            <div class="header-info">
                <div id="header-title">
                    
                    <h2>
                        Enmin&#39;s Blog
                    </h2>
                    
                </div>
                <div id="header-description">
                    
                    <h3>
                        漫宅万岁，bilibili (゜-゜)つロ 乾杯~-
                    </h3>
                    
                </div>
            </div>
            <nav class="header-nav">
                <div class="social">
                    
                        <a title="zem12345678" target="_blank" href="//https://zem12345678.github.io/">
                            <i class="fa fa-home fa-2x"></i></a>
                    
                        <a title="Github" target="_blank" href="//github.com/zem12345678">
                            <i class="fa fa-github fa-2x"></i></a>
                    
                        <a title="Weibo" target="_blank" href="//weibo.com/zem12345678">
                            <i class="fa fa-weibo fa-2x"></i></a>
                    
                        <a title="Twitter" target="_blank" href="//twitter.com/Enmin_zhang">
                            <i class="fa fa-twitter fa-2x"></i></a>
                    
                </div>
            </nav>
        </div>
    </div>
</header>
      <div class="outer">
        <section id="main" class="body-wrap"><article id="post-Scrapy深入" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="post-title" itemprop="name">
      Scrapy 深入
    </h1>
    <div class="post-title-bar">
      <ul>
          
              <li>
                  <i class="fa fa-book"></i>
                  
                      <a href="/categories/数据挖掘/">数据挖掘</a>
                  
              </li>
          
        <li>
          <i class="fa fa-calendar"></i>  2019-05-17
        </li>
        <li>
          <i class="fa fa-eye"></i>
          <span id="busuanzi_value_page_pv"></span>
        </li>
      </ul>
    </div>
  

          
      </header>
    
    <div class="article-entry post-content" itemprop="articleBody">
      
            
            <h1 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h1><p>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，这些Item Pipeline组件按定义的顺序处理Item。</p>
<p>每个Item Pipeline都是实现了简单方法的Python类，比如决定此Item是丢弃而存储。以下是item pipeline的一些典型应用：</p>
<ul>
<li>验证爬取的数据(检查item包含某些字段，比如说name字段)</li>
<li>查重(并丢弃)</li>
<li>将爬取结果保存到文件或者数据库中</li>
</ul>
<h2 id="编写item-pipeline"><a href="#编写item-pipeline" class="headerlink" title="编写item pipeline"></a>编写item pipeline</h2><p>编写item pipeline很简单，item pipiline组件是一个独立的Python类，其中process_item()方法必须实现:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import something</span><br><span class="line"></span><br><span class="line">class SomethingPipeline(object):</span><br><span class="line">    def __init__(self):    </span><br><span class="line">        # 可选实现，做参数初始化等</span><br><span class="line">        # doing something</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        # item (Item 对象) – 被爬取的item</span><br><span class="line">        # spider (Spider 对象) – 爬取该item的spider</span><br><span class="line">        # 这个方法必须实现，每个item pipeline组件都需要调用该方法，</span><br><span class="line">        # 这个方法必须返回一个 Item 对象，被丢弃的item将不会被之后的pipeline组件所处理。</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        # spider (Spider 对象) – 被开启的spider</span><br><span class="line">        # 可选实现，当spider被开启时，这个方法被调用。</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        # spider (Spider 对象) – 被关闭的spider</span><br><span class="line">        # 可选实现，当spider被关闭时，这个方法被调用</span><br></pre></td></tr></table></figure></p>
<h2 id="完善之前的案例"><a href="#完善之前的案例" class="headerlink" title="完善之前的案例"></a>完善之前的案例</h2><h3 id="item写入JSON文件"><a href="#item写入JSON文件" class="headerlink" title="item写入JSON文件"></a>item写入JSON文件</h3><p>以下pipeline将所有(从所有’spider’中)爬取到的item，存储到一个独立地items.json 文件，每行包含一个序列化为’JSON’格式的’item’。</p>
<p>打开 pipelines.py 文件，写入下面代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># pipelines.py</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">class ItcastJsonPipeline(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.file = open(&apos;teacher.json&apos;, &apos;wb&apos;)</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        content = json.dumps(dict(item), ensure_ascii=False) + &quot;\n&quot;</span><br><span class="line">        self.file.write(content)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure></p>
<h3 id="启用一个Item-Pipeline组件"><a href="#启用一个Item-Pipeline组件" class="headerlink" title="启用一个Item Pipeline组件"></a>启用一个Item Pipeline组件</h3><p>为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置，就像下面这个例子:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Configure item pipelines</span><br><span class="line"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    #&apos;mySpider.pipelines.SomePipeline&apos;: 300,</span><br><span class="line">    &quot;mySpider.pipelines.ItcastJsonPipeline&quot;:300</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）</p>
<h3 id="重新启动爬虫"><a href="#重新启动爬虫" class="headerlink" title="重新启动爬虫"></a>重新启动爬虫</h3><p>将parse()方法改为4.2中最后思考中的代码，然后执行下面的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl itcast</span><br></pre></td></tr></table></figure></p>
<p>查看当前目录是否生成teacher.json</p>
<h1 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h1><p>Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。</p>
<figure class="highlight plain"><figcaption><span>scrapy.Spider```是最基本的类，所有编写的爬虫必须继承这个类。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">主要用到的函数及调用顺序为：</span><br><span class="line"></span><br><span class="line">```__init__()``` : 初始化爬虫名字和start_urls列表</span><br><span class="line"></span><br><span class="line">```start_requests()``` 调用```make_requests_from``` url():生成Requests对象交给Scrapy下载并返回response</span><br><span class="line"></span><br><span class="line">```parse() ```: 解析response，并返回Item或Requests（需指定回调函数）。Item传给```Item pipline```持久化 ， 而Requests交由Scrapy下载，并由指定的回调函数处理（默认parse())，一直进行循环，直到处理完所有的数据为止。</span><br><span class="line">## 源码参考</span><br></pre></td></tr></table></figure>
<p>#所有爬虫的基类，用户定义的爬虫必须从这个类继承<br>class Spider(object_ref):</p>
<pre><code>#定义spider名字的字符串(string)。spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。
#name是spider最重要的属性，而且是必须的。
#一般做法是以该网站(domain)(加或不加 后缀 )来命名spider。 例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite
name = None

#初始化，提取爬虫名字，start_ruls
def __init__(self, name=None, **kwargs):
    if name is not None:
        self.name = name
    # 如果爬虫没有名字，中断后续操作则报错
    elif not getattr(self, &apos;name&apos;, None):
        raise ValueError(&quot;%s must have a name&quot; % type(self).__name__)

    # python 对象或类型通过内置成员__dict__来存储成员信息
    self.__dict__.update(kwargs)

    #URL列表。当没有指定的URL时，spider将从该列表中开始进行爬取。 因此，第一个被获取到的页面的URL将是该列表之一。 后续的URL将会从获取到的数据中提取。
    if not hasattr(self, &apos;start_urls&apos;):
        self.start_urls = []

# 打印Scrapy执行后的log信息
def log(self, message, level=log.DEBUG, **kw):
    log.msg(message, spider=self, level=level, **kw)

# 判断对象object的属性是否存在，不存在做断言处理
def set_crawler(self, crawler):
    assert not hasattr(self, &apos;_crawler&apos;), &quot;Spider already bounded to %s&quot; % crawler
    self._crawler = crawler

@property
def crawler(self):
    assert hasattr(self, &apos;_crawler&apos;), &quot;Spider not bounded to any crawler&quot;
    return self._crawler

@property
def settings(self):
    return self.crawler.settings

#该方法将读取start_urls内的地址，并为每一个地址生成一个Request对象，交给Scrapy下载并返回Response
#该方法仅调用一次
def start_requests(self):
    for url in self.start_urls:
        yield self.make_requests_from_url(url)

#start_requests()中调用，实际生成Request的函数。
#Request对象默认的回调函数为parse()，提交的方式为get
def make_requests_from_url(self, url):
    return Request(url, dont_filter=True)

#默认的Request对象回调函数，处理返回的response。
#生成Item或者Request对象。用户必须实现这个类
def parse(self, response):
    raise NotImplementedError

@classmethod
def handles_request(cls, request):
    return url_is_from_spider(request.url, cls)

def __str__(self):
    return &quot;&lt;%s %r at 0x%0x&gt;&quot; % (type(self).__name__, self.name, id(self))

__repr__ = __str__
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">## 主要属性和方法</span><br><span class="line">* name</span><br><span class="line"></span><br><span class="line">&gt;定义spider名字的字符串。</span><br><span class="line"></span><br><span class="line">&gt;例如，如果spider爬取 mywebsite.com ，该spider通常会被命名为 mywebsite</span><br><span class="line"></span><br><span class="line">* allowed_domains</span><br><span class="line"></span><br><span class="line">&gt;包含了spider允许爬取的域名(domain)的列表，可选。</span><br><span class="line"></span><br><span class="line">* start_urls</span><br><span class="line"></span><br><span class="line">&gt;初始URL元祖/列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。</span><br><span class="line"></span><br><span class="line">* start_requests(self)</span><br><span class="line"></span><br><span class="line">&gt;该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取（默认实现是使用 start_urls 的url）的第一个Request。</span><br><span class="line"></span><br><span class="line">&gt;当spider启动爬取并且未指定start_urls时，该方法被调用。</span><br><span class="line"></span><br><span class="line">* parse(self, response)</span><br><span class="line"></span><br><span class="line">&gt;当请求url返回网页没有指定回调函数时，默认的Request对象回调函数。用来处理网页返回的response，以及生成Item或者Request对象。</span><br><span class="line"></span><br><span class="line">* log(self, message[, level, component])</span><br><span class="line"></span><br><span class="line">&gt;使用 scrapy.log.msg() 方法记录(log)message。 更多数据请参见 logging</span><br><span class="line"></span><br><span class="line">## 案例：腾讯招聘网自动翻页采集</span><br><span class="line">* 创建一个新的爬虫：</span><br></pre></td></tr></table></figure>
<p>scrapy genspider tencent “tencent.com”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 编写items.py</span><br><span class="line">获取职位名称、详细信息、</span><br></pre></td></tr></table></figure></p>
<p>class TencentItem(scrapy.Item):<br>    name = scrapy.Field()<br>    detailLink = scrapy.Field()<br>    positionInfo = scrapy.Field()<br>    peopleNumber = scrapy.Field()<br>    workLocation = scrapy.Field()<br>    publishTime = scrapy.Field()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* 编写tencent.py</span><br></pre></td></tr></table></figure></p>
<h1 id="tencent-py"><a href="#tencent-py" class="headerlink" title="tencent.py"></a>tencent.py</h1><p>from mySpider.items import TencentItem<br>import scrapy<br>import re</p>
<p>class TencentSpider(scrapy.Spider):<br>    name = “tencent”<br>    allowed_domains = [“hr.tencent.com”]<br>    start_urls = [<br>        “<a href="http://hr.tencent.com/position.php?&amp;start=0#a&quot;" target="_blank" rel="noopener">http://hr.tencent.com/position.php?&amp;start=0#a&quot;</a><br>    ]</p>
<pre><code>def parse(self, response):
items = response.xpath(&apos;//*[contains(@class,&quot;odd&quot;) or contains(@class,&quot;even&quot;)]&apos;)
for item in items:
    temp = dict(
        name=item.xpath(&quot;./td[1]/a/text()&quot;).extract()[0],
        detailLink=&quot;http://hr.tencent.com/&quot;+item.xpath(&quot;./td[1]/a/@href&quot;).extract()[0],
        positionInfo=item.xpath(&apos;./td[2]/text()&apos;).extract()[0] if len(item.xpath(&apos;./td[2]/text()&apos;).extract())&gt;0 else None,
        peopleNumber=item.xpath(&apos;./td[3]/text()&apos;).extract()[0],
        workLocation=item.xpath(&apos;./td[4]/text()&apos;).extract()[0],
        publishTime=item.xpath(&apos;./td[5]/text()&apos;).extract()[0]
    )
    yield temp

now_page = int(re.search(r&quot;\d+&quot;, response.url).group(0))
print(&quot;*&quot; * 100)
if now_page &lt; 216:
    url = re.sub(r&quot;\d+&quot;, str(now_page + 10), response.url)
    print(&quot;this is next page url:&quot;, url)
    print(&quot;*&quot; * 100)
    yield scrapy.Request(url, callback=self.parse)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* 编写pipeline.py文件</span><br></pre></td></tr></table></figure>
<p>import json</p>
<p>#class ItcastJsonPipeline(object):<br>class TencentJsonPipeline(object):</p>
<pre><code>def __init__(self):
    #self.file = open(&apos;teacher.json&apos;, &apos;wb&apos;)
    self.file = open(&apos;tencent.json&apos;, &apos;wb&apos;)

def process_item(self, item, spider):
    content = json.dumps(dict(item), ensure_ascii=False) + &quot;\n&quot;
    self.file.write(content)
    return item

def close_spider(self, spider):
    self.file.close()
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 在 setting.py 里设置ITEM_PIPELINES</span><br></pre></td></tr></table></figure>
<p>ITEM_PIPELINES = {</p>
<pre><code>#&apos;mySpider.pipelines.SomePipeline&apos;: 300,
#&quot;mySpider.pipelines.ItcastJsonPipeline&quot;:300
&quot;mySpider.pipelines.TencentJsonPipeline&quot;:300
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* 执行爬虫：```scrapy crawl tencent</span><br></pre></td></tr></table></figure></p>
<h2 id="parse-方法的工作机制："><a href="#parse-方法的工作机制：" class="headerlink" title="parse()方法的工作机制："></a>parse()方法的工作机制：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. 因为使用的yield，而不是return。parse函数将会被当做一个生成器使用。scrapy会逐一获取parse方法中生成的结果，并判断该结果是一个什么样的类型；</span><br><span class="line">2. 如果是request则加入爬取队列，如果是item类型则使用pipeline处理，其他类型则返回错误信息。</span><br><span class="line">3. scrapy取到第一部分的request不会立马就去发送这个request，只是把这个request放到队列里，然后接着从生成器里获取；</span><br><span class="line">4. 取尽第一部分的request，然后再获取第二部分的item，取到item了，就会放到对应的pipeline里处理；</span><br><span class="line">5. parse()方法作为回调函数(callback)赋值给了Request，指定parse()方法来处理这些请求 scrapy.Request(url, callback=self.parse)</span><br><span class="line">6. Request对象经过调度，执行生成 scrapy.http.response()的响应对象，并送回给parse()方法，直到调度器中没有Request（递归的思路）</span><br><span class="line">7. 取尽之后，parse()工作结束，引擎再根据队列和pipelines中的内容去执行相应的操作；</span><br><span class="line">8. 程序在取得各个页面的items前，会先处理完之前所有的request队列里的请求，然后再提取items。</span><br><span class="line">7. 这一切的一切，Scrapy引擎和调度器将负责到底。</span><br></pre></td></tr></table></figure>
<h2 id="常见bug"><a href="#常见bug" class="headerlink" title="常见bug"></a>常见bug</h2><blockquote>
<p>[scrapy.spidermiddlewares.offsite] DEBUG: Filtered offsite request to ‘hr.tencent.com’:</p>
</blockquote>
<p>解决方式：</p>
<blockquote>
<p>domain错误 修改domain为：hr.tencent.com</p>
</blockquote>
<h1 id="CrawlSpiders"><a href="#CrawlSpiders" class="headerlink" title="CrawlSpiders"></a>CrawlSpiders</h1><p>通过下面的命令可以快速创建 CrawlSpider模板 的代码：</p>
<blockquote>
<p>scrapy genspider -t crawl tencent tencent.com</p>
</blockquote>
<p>上一个案例中，我们通过正则表达式，制作了新的url作为Request请求参数，现在我们可以换个花样…</p>
<blockquote>
<p>class scrapy.spiders.CrawlSpider</p>
</blockquote>
<p>它是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的机制，从爬取的网页中获取link并继续爬取的工作更适合。</p>
<h2 id="源码参考"><a href="#源码参考" class="headerlink" title="源码参考"></a>源码参考</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">class CrawlSpider(Spider):</span><br><span class="line">    rules = ()</span><br><span class="line">    def __init__(self, *a, **kw):</span><br><span class="line">        super(CrawlSpider, self).__init__(*a, **kw)</span><br><span class="line">        self._compile_rules()</span><br><span class="line"></span><br><span class="line">    #首先调用parse()来处理start_urls中返回的response对象</span><br><span class="line">    #parse()则将这些response对象传递给了_parse_response()函数处理，并设置回调函数为parse_start_url()</span><br><span class="line">    #设置了跟进标志位True</span><br><span class="line">    #parse将返回item和跟进了的Request对象    </span><br><span class="line">    def parse(self, response):</span><br><span class="line">        return self._parse_response(response, self.parse_start_url, cb_kwargs=&#123;&#125;, follow=True)</span><br><span class="line"></span><br><span class="line">    #处理start_url中返回的response，需要重写</span><br><span class="line">    def parse_start_url(self, response):</span><br><span class="line">        return []</span><br><span class="line"></span><br><span class="line">    def process_results(self, response, results):</span><br><span class="line">        return results</span><br><span class="line"></span><br><span class="line">    #从response中抽取符合任一用户定义&apos;规则&apos;的链接，并构造成Resquest对象返回</span><br><span class="line">    def _requests_to_follow(self, response):</span><br><span class="line">        if not isinstance(response, HtmlResponse):</span><br><span class="line">            return</span><br><span class="line">        seen = set()</span><br><span class="line">        #抽取之内的所有链接，只要通过任意一个&apos;规则&apos;，即表示合法</span><br><span class="line">        for n, rule in enumerate(self._rules):</span><br><span class="line">            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]</span><br><span class="line">            #使用用户指定的process_links处理每个连接</span><br><span class="line">            if links and rule.process_links:</span><br><span class="line">                links = rule.process_links(links)</span><br><span class="line">            #将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded()</span><br><span class="line">            for link in links:</span><br><span class="line">                seen.add(link)</span><br><span class="line">                #构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数</span><br><span class="line">                r = Request(url=link.url, callback=self._response_downloaded)</span><br><span class="line">                r.meta.update(rule=n, link_text=link.text)</span><br><span class="line">                #对每个Request调用process_request()函数。该函数默认为indentify，即不做任何处理，直接返回该Request.</span><br><span class="line">                yield rule.process_request(r)</span><br><span class="line"></span><br><span class="line">    #处理通过rule提取出的连接，并返回item以及request</span><br><span class="line">    def _response_downloaded(self, response):</span><br><span class="line">        rule = self._rules[response.meta[&apos;rule&apos;]]</span><br><span class="line">        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)</span><br><span class="line"></span><br><span class="line">    #解析response对象，会用callback解析处理他，并返回request或Item对象</span><br><span class="line">    def _parse_response(self, response, callback, cb_kwargs, follow=True):</span><br><span class="line">        #首先判断是否设置了回调函数。（该回调函数可能是rule中的解析函数，也可能是 parse_start_url函数）</span><br><span class="line">        #如果设置了回调函数（parse_start_url()），那么首先用parse_start_url()处理response对象，</span><br><span class="line">        #然后再交给process_results处理。返回cb_res的一个列表</span><br><span class="line">        if callback:</span><br><span class="line">            #如果是parse调用的，则会解析成Request对象</span><br><span class="line">            #如果是rule callback，则会解析成Item</span><br><span class="line">            cb_res = callback(response, **cb_kwargs) or ()</span><br><span class="line">            cb_res = self.process_results(response, cb_res)</span><br><span class="line">            for requests_or_item in iterate_spider_output(cb_res):</span><br><span class="line">                yield requests_or_item</span><br><span class="line"></span><br><span class="line">        #如果需要跟进，那么使用定义的Rule规则提取并返回这些Request对象</span><br><span class="line">        if follow and self._follow_links:</span><br><span class="line">            #返回每个Request对象</span><br><span class="line">            for request_or_item in self._requests_to_follow(response):</span><br><span class="line">                yield request_or_item</span><br><span class="line"></span><br><span class="line">    def _compile_rules(self):</span><br><span class="line">        def get_method(method):</span><br><span class="line">            if callable(method):</span><br><span class="line">                return method</span><br><span class="line">            elif isinstance(method, basestring):</span><br><span class="line">                return getattr(self, method, None)</span><br><span class="line"></span><br><span class="line">        self._rules = [copy.copy(r) for r in self.rules]</span><br><span class="line">        for rule in self._rules:</span><br><span class="line">            rule.callback = get_method(rule.callback)</span><br><span class="line">            rule.process_links = get_method(rule.process_links)</span><br><span class="line">            rule.process_request = get_method(rule.process_request)</span><br><span class="line"></span><br><span class="line">    def set_crawler(self, crawler):</span><br><span class="line">        super(CrawlSpider, self).set_crawler(crawler)</span><br><span class="line">        self._follow_links = crawler.settings.getbool(&apos;CRAWLSPIDER_FOLLOW_LINKS&apos;, True)</span><br></pre></td></tr></table></figure>
<p>CrawlSpider继承于Spider类，除了继承过来的属性外（name、allow_domains），还提供了新的属性和方法:</p>
<h2 id="rules"><a href="#rules" class="headerlink" title="rules"></a>rules</h2><p>CrawlSpider使用rules来决定爬虫的爬取规则，并将匹配后的url请求提交给引擎。所以在正常情况下，CrawlSpider不需要单独手动返回请求了。</p>
<p>在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了某种特定操作，比如提取当前相应内容里的特定链接，是否对提取的链接跟进爬取，对提交的请求设置回调函数等。</p>
<p>如果多个rule匹配了相同的链接，则根据规则在本集合中被定义的顺序，第一个会被使用。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class scrapy.spiders.Rule(</span><br><span class="line">        link_extractor,</span><br><span class="line">        callback = None,</span><br><span class="line">        cb_kwargs = None,</span><br><span class="line">        follow = None,</span><br><span class="line">        process_links = None,</span><br><span class="line">        process_request = None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<ul>
<li><p>link_extractor：是一个Link Extractor对象，用于定义需要提取的链接。</p>
</li>
<li><p>callback： 从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。</p>
</li>
</ul>
<blockquote>
<p>注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</p>
</blockquote>
<ul>
<li><p>follow：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。</p>
</li>
<li><p>process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。</p>
</li>
<li><p>process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。 (用来过滤request)</p>
</li>
</ul>
<h2 id="LinkExtractors"><a href="#LinkExtractors" class="headerlink" title="LinkExtractors"></a>LinkExtractors</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class scrapy.linkextractors.LinkExtractor</span><br></pre></td></tr></table></figure>
<p>Link Extractors 的目的很简单: 提取链接｡</p>
<p>每个LinkExtractor有唯一的公共方法是 extract_links()，它接收一个 Response 对象，并返回一个 scrapy.link.Link 对象。</p>
<p>Link Extractors要实例化一次，并且 extract_links 方法会根据不同的 response 调用多次提取链接｡<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class scrapy.linkextractors.LinkExtractor(</span><br><span class="line">    allow = (),</span><br><span class="line">    deny = (),</span><br><span class="line">    allow_domains = (),</span><br><span class="line">    deny_domains = (),</span><br><span class="line">    deny_extensions = None,</span><br><span class="line">    restrict_xpaths = (),</span><br><span class="line">    tags = (&apos;a&apos;,&apos;area&apos;),</span><br><span class="line">    attrs = (&apos;href&apos;),</span><br><span class="line">    canonicalize = True,</span><br><span class="line">    unique = True,</span><br><span class="line">    process_value = None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>主要参数：</p>
<ul>
<li><p>allow：满足括号中“正则表达式”的URL会被提取，如果为空，则全部匹配。</p>
</li>
<li><p>deny：满足括号中“正则表达式”的URL一定不提取（优先级高于allow）。</p>
</li>
<li><p>allow_domains：会被提取的链接的domains。</p>
</li>
<li><p>deny_domains：一定不会被提取链接的domains。</p>
</li>
<li><p>restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。</p>
<h2 id="爬取规则-Crawling-rules"><a href="#爬取规则-Crawling-rules" class="headerlink" title="爬取规则(Crawling rules)"></a>爬取规则(Crawling rules)</h2><p>继续用腾讯招聘为例，给出配合rule使用CrawlSpider的例子:</p>
</li>
</ul>
<ol>
<li><p>首先运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &quot;http://hr.tencent.com/position.php?&amp;start=0#a&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入LinkExtractor，创建LinkExtractor实例对象。：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line"></span><br><span class="line">page_lx = LinkExtractor(allow=(&apos;position.php?&amp;start=\d+&apos;))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<blockquote>
<p>allow : LinkExtractor对象最重要的参数之一，这是一个正则表达式，必须要匹配这个正则表达式(或正则表达式列表)的URL才会被提取，如果没有给出(或为空), 它会匹配所有的链接｡</p>
</blockquote>
<blockquote>
<p>deny : 用法同allow，只不过与这个正则表达式匹配的URL不会被提取)｡它的优先级高于 allow 的参数，如果没有给出(或None), 将不排除任何链接｡</p>
</blockquote>
<ol start="3">
<li><p>调用LinkExtractor实例的extract_links()方法查询匹配结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">page_lx.extract_links(response)</span><br></pre></td></tr></table></figure>
</li>
<li><p>没有查到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[]</span><br></pre></td></tr></table></figure>
</li>
<li><p>注意转义字符的问题，继续重新匹配：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">page_lx = LinkExtractor(allow=(&apos;position\.php\?&amp;start=\d+&apos;))</span><br><span class="line"># page_lx = LinkExtractor(allow = (&apos;start=\d+&apos;))</span><br><span class="line"></span><br><span class="line">page_lx.extract_links(response)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><img src="https://s2.ax1x.com/2019/05/17/ELasQ1.jpg" alt="enter image description here"></p>
<h2 id="CrawlSpider-版本"><a href="#CrawlSpider-版本" class="headerlink" title="CrawlSpider 版本"></a>CrawlSpider 版本</h2><p>那么，scrapy shell测试完成之后，修改以下代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#提取匹配 &apos;http://hr.tencent.com/position.php?&amp;start=\d+&apos;的链接</span><br><span class="line">page_lx = LinkExtractor(allow = (&apos;start=\d+&apos;))</span><br><span class="line"></span><br><span class="line">rules = [</span><br><span class="line">    #提取匹配,并使用spider的parse方法进行分析;并跟进链接(没有callback意味着follow默认为True)</span><br><span class="line">    Rule(page_lx, callback = &apos;parse&apos;, follow = True)</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<p><strong>这么写对吗？</strong></p>
<p><strong>不对！千万记住 callback 千万不能写 parse，再次强调：由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TecentSpider(CrawlSpider):</span><br><span class="line">    name = &apos;tecent&apos;</span><br><span class="line">    allowed_domains = [&apos;hr.tencent.com&apos;]</span><br><span class="line">    start_urls = [&apos;http://hr.tencent.com/position.php?&amp;start=0&apos;]</span><br><span class="line">    page_lx = LinkExtractor(allow=r&apos;start=\d+&apos;)</span><br><span class="line">    #position.php?&amp;start=10#a</span><br><span class="line">    rules = (</span><br><span class="line">        Rule(page_lx, callback=&apos;parse_item&apos;, follow=True),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    def parse_item(self, response):</span><br><span class="line">        items = response.xpath(&apos;//*[contains(@class,&quot;odd&quot;) or contains(@class,&quot;even&quot;)]&apos;)</span><br><span class="line">        for item in items:</span><br><span class="line">            temp = dict(</span><br><span class="line">                position=item.xpath(&quot;./td[1]/a/text()&quot;).extract()[0],</span><br><span class="line">                detailLink=&quot;http://hr.tencent.com/&quot; + item.xpath(&quot;./td[1]/a/@href&quot;).extract()[0],</span><br><span class="line">                type=item.xpath(&apos;./td[2]/text()&apos;).extract()[0] if len(</span><br><span class="line">                    item.xpath(&apos;./td[2]/text()&apos;).extract()) &gt; 0 else None,</span><br><span class="line">                need_num=item.xpath(&apos;./td[3]/text()&apos;).extract()[0],</span><br><span class="line">                location=item.xpath(&apos;./td[4]/text()&apos;).extract()[0],</span><br><span class="line">                publish_time=item.xpath(&apos;./td[5]/text()&apos;).extract()[0]</span><br><span class="line">            )</span><br><span class="line">            print(temp)</span><br><span class="line">            yield temp</span><br><span class="line"></span><br><span class="line">    # parse() 方法不需要重写     </span><br><span class="line">    # def parse(self, response):                                              </span><br><span class="line">    #     pass</span><br></pre></td></tr></table></figure></p>
<p>运行：<figure class="highlight plain"><figcaption><span>crawl tencent```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## Logging</span><br><span class="line">Scrapy提供了log功能，可以通过 logging 模块使用。</span><br><span class="line"></span><br><span class="line">&gt;可以修改配置文件settings.py，任意位置添加下面两行，效果会清爽很多。</span><br></pre></td></tr></table></figure></p>
<p>LOG_FILE = “TencentSpider.log”<br>LOG_LEVEL = “INFO”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">### Log levels</span><br><span class="line">* Scrapy提供5层logging级别:</span><br><span class="line"></span><br><span class="line">* CRITICAL - 严重错误(critical)</span><br><span class="line"></span><br><span class="line">* ERROR - 一般错误(regular errors)</span><br><span class="line">* WARNING - 警告信息(warning messages)</span><br><span class="line">* INFO - 一般信息(informational messages)</span><br><span class="line">* DEBUG - 调试信息(debugging messages)</span><br><span class="line">### logging设置</span><br><span class="line">通过在setting.py中进行以下设置可以被用来配置logging:</span><br><span class="line"></span><br><span class="line">1. ```LOG_ENABLED``` 默认: True，启用logging</span><br><span class="line">2. ```LOG_ENCODING``` 默认: &apos;utf-8&apos;，logging使用的编码</span><br><span class="line">3. ```LOG_FILE``` 默认: None，在当前目录里创建logging输出文件的文件名</span><br><span class="line">4. ```LOG_LEVEL``` 默认: &apos;DEBUG&apos;，log的最低级别</span><br><span class="line">5. ```LOG_STDOUT``` 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print &quot;hello&quot; ，其将会在Scrapy log中显示。</span><br><span class="line"></span><br><span class="line"># Request</span><br><span class="line">Request 部分源码：</span><br></pre></td></tr></table></figure></p>
<h1 id="部分代码"><a href="#部分代码" class="headerlink" title="部分代码"></a>部分代码</h1><p>class Request(object_ref):</p>
<pre><code>def __init__(self, url, callback=None, method=&apos;GET&apos;, headers=None, body=None, 
             cookies=None, meta=None, encoding=&apos;utf-8&apos;, priority=0,
             dont_filter=False, errback=None):

    self._encoding = encoding  # this one has to be set first
    self.method = str(method).upper()
    self._set_url(url)
    self._set_body(body)
    assert isinstance(priority, int), &quot;Request priority not an integer: %r&quot; % priority
    self.priority = priority

    assert callback or not errback, &quot;Cannot use errback without a callback&quot;
    self.callback = callback
    self.errback = errback

    self.cookies = cookies or {}
    self.headers = Headers(headers or {}, encoding=encoding)
    self.dont_filter = dont_filter

    self._meta = dict(meta) if meta else None

@property
def meta(self):
    if self._meta is None:
        self._meta = {}
    return self._meta
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其中，比较常用的参数：</span><br></pre></td></tr></table></figure>
<p>url: 就是需要请求，并进行下一步处理的url</p>
<p>callback: 指定该请求返回的Response，由那个函数来处理。</p>
<p>method: 请求一般不需要指定，默认GET方法，可设置为”GET”, “POST”, “PUT”等，且保证字符串大写</p>
<p>headers: 请求时，包含的头文件。一般不需要。内容一般如下：</p>
<pre><code># 自己写过爬虫的肯定知道
Host: media.readthedocs.org
User-Agent: Mozilla/5.0 (Windows NT 6.2; WOW64; rv:33.0) Gecko/20100101 Firefox/33.0
Accept: text/css,*/*;q=0.1
Accept-Language: zh-cn,zh;q=0.8,en-us;q=0.5,en;q=0.3
Accept-Encoding: gzip, deflate
Referer: http://scrapy-chs.readthedocs.org/zh_CN/0.24/
Cookie: _ga=GA1.2.1612165614.1415584110;
Connection: keep-alive
If-Modified-Since: Mon, 25 Aug 2014 21:59:35 GMT
Cache-Control: max-age=0
</code></pre><p>meta: 比较常用，在不同的请求之间传递数据使用的。字典dict型</p>
<pre><code>request_with_cookies = Request(
    url=&quot;http://www.example.com&quot;,
    cookies={&apos;currency&apos;: &apos;USD&apos;, &apos;country&apos;: &apos;UY&apos;},
    meta={&apos;dont_merge_cookies&apos;: True}
)
</code></pre><p>encoding: 使用默认的 ‘utf-8’ 就行。</p>
<p>dont_filter: 表明该请求不由调度器过滤。这是当你想使用多次执行相同的请求,忽略重复的过滤器。默认为False。</p>
<p>errback: 指定错误处理函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># Response</span><br></pre></td></tr></table></figure></p>
<h1 id="部分代码-1"><a href="#部分代码-1" class="headerlink" title="部分代码"></a>部分代码</h1><p>class Response(object_ref):<br>    def <strong>init</strong>(self, url, status=200, headers=None, body=’’, flags=None, request=None):<br>        self.headers = Headers(headers or {})<br>        self.status = int(status)<br>        self._set_body(body)<br>        self._set_url(url)<br>        self.request = request<br>        self.flags = [] if flags is None else list(flags)</p>
<pre><code>@property
def meta(self):
    try:
        return self.request.meta
    except AttributeError:
        raise AttributeError(&quot;Response.meta not available, this response &quot; \
            &quot;is not tied to any request&quot;)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">大部分参数和上面的差不多：</span><br></pre></td></tr></table></figure>
<p>status: 响应码<br>_set_body(body)： 响应体<br>_set_url(url)：响应url<br>self.request = request<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">## 发送POST请求</span><br><span class="line">* 可以使用 yield scrapy.FormRequest(url, formdata, callback)方法发送POST请求。</span><br><span class="line"></span><br><span class="line">* 如果希望程序执行一开始就发送POST请求，可以重写Spider类的start_requests(self) 方法，并且不再调用start_urls里的url。</span><br></pre></td></tr></table></figure></p>
<p>class mySpider(scrapy.Spider):</p>
<pre><code># start_urls = [&quot;http://www.example.com/&quot;]

def start_requests(self):
    url = &apos;http://www.renren.com/PLogin.do&apos;

    # FormRequest 是Scrapy发送POST请求的方法
    yield scrapy.FormRequest(
        url = url,
        formdata = {&quot;email&quot; : &quot;mr_mao_hacker@163.com&quot;, &quot;password&quot; : &quot;axxxxxxxe&quot;},
        callback = self.parse_page
    )
def parse_page(self, response):
    # do something
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 模拟登陆</span><br><span class="line">使用FormRequest.from_response()方法模拟用户登录</span><br><span class="line"></span><br><span class="line">&gt;通常网站通过 实现对某些表单字段（如数据或是登录界面中的认证令牌等）的预填充。</span><br><span class="line"></span><br><span class="line">&gt;使用Scrapy抓取网页时，如果想要预填充或重写像用户名、用户密码这些表单字段， 可以使用 FormRequest.from_response() 方法实现。</span><br><span class="line"></span><br><span class="line">下面是使用这种方法的爬虫例子:</span><br></pre></td></tr></table></figure>
<p>import scrapy</p>
<p>class LoginSpider(scrapy.Spider):<br>    name = ‘example.com’<br>    start_urls = [‘<a href="http://www.example.com/users/login.php&#39;]" target="_blank" rel="noopener">http://www.example.com/users/login.php&#39;]</a></p>
<pre><code>def parse(self, response):
    return scrapy.FormRequest.from_response(
        response,
        formdata={&apos;username&apos;: &apos;john&apos;, &apos;password&apos;: &apos;secret&apos;},
        callback=self.after_login
    )

def after_login(self, response):
    # check login succeed before going on
    if &quot;authentication failed&quot; in response.body:
        self.log(&quot;Login failed&quot;, level=log.ERROR)
        return

    # continue scraping with authenticated session...
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 知乎爬虫案例参考：</span><br><span class="line">### zhihuSpider.py爬虫代码</span><br></pre></td></tr></table></figure>
<p>#!/usr/bin/env python</p>
<h1 id="coding-utf-8"><a href="#coding-utf-8" class="headerlink" title="-- coding:utf-8 --"></a>-<em>- coding:utf-8 -</em>-</h1><p>from scrapy.spiders import CrawlSpider, Rule<br>from scrapy.selector import Selector<br>from scrapy.linkextractors import LinkExtractor<br>from scrapy import Request, FormRequest<br>from zhihu.items import ZhihuItem</p>
<p>class ZhihuSipder(CrawlSpider) :<br>    name = “zhihu”<br>    allowed_domains = [“<a href="http://www.zhihu.com&quot;]" target="_blank" rel="noopener">www.zhihu.com&quot;]</a><br>    start_urls = [<br>        “<a href="http://www.zhihu.com&quot;" target="_blank" rel="noopener">http://www.zhihu.com&quot;</a><br>    ]<br>    rules = (<br>        Rule(LinkExtractor(allow = (‘/question/\d+#.*?’, )), callback = ‘parse_page’, follow = True),<br>        Rule(LinkExtractor(allow = (‘/question/\d+’, )), callback = ‘parse_page’, follow = True),<br>    )</p>
<pre><code>headers = {
    &quot;Accept&quot;: &quot;*/*&quot;,
    &quot;Accept-Language&quot;: &quot;en-US,en;q=0.8,zh-TW;q=0.6,zh;q=0.4&quot;,
    &quot;Connection&quot;: &quot;keep-alive&quot;,
    &quot;Content-Type&quot;:&quot; application/x-www-form-urlencoded; charset=UTF-8&quot;,
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2125.111 Safari/537.36&quot;,
    &quot;Referer&quot;: &quot;http://www.zhihu.com/&quot;
}

#重写了爬虫类的方法, 实现了自定义请求, 运行成功后会调用callback回调函数
def start_requests(self):
    return [Request(&quot;https://www.zhihu.com/login&quot;, meta = {&apos;cookiejar&apos; : 1}, callback = self.post_login)]

def post_login(self, response):
    print &apos;Preparing login&apos;
    #下面这句话用于抓取请求网页后返回网页中的_xsrf字段的文字, 用于成功提交表单
    xsrf = response.xpath(&apos;//input[@name=&quot;_xsrf&quot;]/@value&apos;).extract()[0]
    print xsrf
    #FormRequeset.from_response是Scrapy提供的一个函数, 用于post表单
    #登陆成功后, 会调用after_login回调函数
    return [FormRequest.from_response(response,   #&quot;http://www.zhihu.com/login&quot;,
                        meta = {&apos;cookiejar&apos; : response.meta[&apos;cookiejar&apos;]},
                        headers = self.headers,  #注意此处的headers
                        formdata = {
                        &apos;_xsrf&apos;: xsrf,
                        &apos;email&apos;: &apos;123456@qq.com&apos;,
                        &apos;password&apos;: &apos;123456&apos;
                        },
                        callback = self.after_login,
                        dont_filter = True
                        )]

def after_login(self, response) :
    for url in self.start_urls :
        yield self.make_requests_from_url(url)

def parse_page(self, response):
    problem = Selector(response)
    item = ZhihuItem()
    item[&apos;url&apos;] = response.url
    item[&apos;name&apos;] = problem.xpath(&apos;//span[@class=&quot;name&quot;]/text()&apos;).extract()
    print item[&apos;name&apos;]
    item[&apos;title&apos;] = problem.xpath(&apos;//h2[@class=&quot;zm-item-title zm-editable-content&quot;]/text()&apos;).extract()
    item[&apos;description&apos;] = problem.xpath(&apos;//div[@class=&quot;zm-editable-content&quot;]/text()&apos;).extract()
    item[&apos;answer&apos;]= problem.xpath(&apos;//div[@class=&quot; zm-editable-content clearfix&quot;]/text()&apos;).extract()
    return item
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">### Item类设置</span><br></pre></td></tr></table></figure>
<p>from scrapy.item import Item, Field</p>
<p>class ZhihuItem(Item):</p>
<pre><code># define the fields for your item here like:
# name = scrapy.Field()
url = Field()  #保存抓取问题的url
title = Field()  #抓取问题的标题
description = Field()  #抓取问题的描述
answer = Field()  #抓取问题的答案
name = Field()  #个人用户的名称
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">### setting.py 设置抓取间隔</span><br></pre></td></tr></table></figure>
<p>BOT_NAME = ‘zhihu’</p>
<p>SPIDER_MODULES = [‘zhihu.spiders’]<br>NEWSPIDER_MODULE = ‘zhihu.spiders’<br>DOWNLOAD_DELAY = 0.25   #设置下载间隔为250ms<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 反反爬虫相关机制</span><br><span class="line">**Some websites implement certain measures to prevent bots from crawling them, with varying degrees of sophistication. Getting around those measures can be difficult and tricky, and may sometimes require special infrastructure. Please consider contacting commercial support if in doubt.**</span><br><span class="line">**(有些些网站使用特定的不同程度的复杂性规则防止爬虫访问，绕过这些规则是困难和复杂的，有时可能需要特殊的基础设施，如果有疑问，请联系商业支持。)**</span><br><span class="line">&gt;来自于Scrapy官方文档描述：http://doc.scrapy.org/en/master/topics/practices.html#avoiding-getting-banned</span><br><span class="line"></span><br><span class="line">## 通常防止爬虫被反主要有以下几个策略：</span><br><span class="line">* 动态设置User-Agent（随机切换User-Agent，模拟不同用户的浏览器信息）</span><br><span class="line"></span><br><span class="line">* 禁用Cookies（也就是不启用cookies middleware，不向Server发送cookies，有些网站通过cookie的使用发现爬虫行为）</span><br><span class="line"></span><br><span class="line"> * 可以通过COOKIES_ENABLED 控制 CookiesMiddleware 开启或关闭</span><br><span class="line">* 设置延迟下载（防止访问过于频繁，设置为 2秒 或更高）</span><br><span class="line"></span><br><span class="line">* Google Cache 和 Baidu Cache：如果可能的话，使用谷歌/百度等搜索引擎服务器页面缓存获取页面数据。</span><br><span class="line"></span><br><span class="line">* 使用IP地址池：VPN和代理IP，现在大部分网站都是根据IP来ban的。</span><br><span class="line"></span><br><span class="line">* 使用 Crawlera（专用于爬虫的代理组件），正确配置和设置下载中间件后，项目所有的request都是通过crawlera发出。</span><br></pre></td></tr></table></figure></p>
<p>  DOWNLOADER_MIDDLEWARES = {<br>      ‘scrapy_crawlera.CrawleraMiddleware’: 600<br>  }</p>
<p>  CRAWLERA_ENABLED = True<br>  CRAWLERA_USER = ‘注册/购买的UserKey’<br>  CRAWLERA_PASS = ‘注册/购买的Password’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 设置下载中间件（Downloader Middlewares）</span><br><span class="line">下载中间件是处于引擎(crawler.engine)和下载器(crawler.engine.download())之间的一层组件，可以有多个下载中间件被加载运行。</span><br><span class="line"></span><br><span class="line">1. 当引擎传递请求给下载器的过程中，下载中间件可以对请求进行处理 （例如增加http header信息，增加proxy信息等）；</span><br><span class="line"></span><br><span class="line">2. 在下载器完成http请求，传递响应给引擎的过程中， 下载中间件可以对响应进行处理（例如进行gzip的解压等）</span><br><span class="line"></span><br><span class="line">要激活下载器中间件组件，将其加入到 DOWNLOADER_MIDDLEWARES 设置中。 该设置是一个字典(dict)，键为中间件类的路径，值为其中间件的顺序(order)。</span><br><span class="line"></span><br><span class="line">这里是一个例子:</span><br></pre></td></tr></table></figure></p>
<p>DOWNLOADER_MIDDLEWARES = {<br>    ‘mySpider.middlewares.MyDownloaderMiddleware’: 543,<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">编写下载器中间件十分简单。每个中间件组件是一个定义了以下一个或多个方法的Python类:</span><br></pre></td></tr></table></figure></p>
<p>class scrapy.contrib.downloadermiddleware.DownloaderMiddleware<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## process_request(self, request, spider)</span><br><span class="line">* 当每个request通过下载中间件时，该方法被调用。</span><br><span class="line"></span><br><span class="line">* process_request() 必须返回以下其中之一：一个 None 、一个 Response 对象、一个 Request 对象或 raise IgnoreRequest:</span><br><span class="line"></span><br><span class="line"> * 如果其返回 None ，Scrapy将继续处理该request，执行其他的中间件的相应方法，直到合适的下载器处理函数(download handler)被调用， 该request被执行(其response被下载)。</span><br><span class="line"></span><br><span class="line"> * 如果其返回 Response 对象，Scrapy将不会调用 任何 其他的 process_request() 或 process_exception() 方法，或相应地下载函数； 其将返回该response。 已安装的中间件的 process_response() 方法则会在每个response返回时被调用。</span><br><span class="line"></span><br><span class="line"> * 如果其返回 Request 对象，Scrapy则停止调用 process_request方法并重新调度返回的request。当新返回的request被执行后， 相应地中间件链将会根据下载的response被调用。</span><br><span class="line"></span><br><span class="line"> * 如果其raise一个 IgnoreRequest 异常，则安装的下载中间件的 process_exception() 方法会被调用。如果没有任何一个方法处理该异常， 则request的errback(Request.errback)方法会被调用。如果没有代码处理抛出的异常， 则该异常被忽略且不记录(不同于其他异常那样)。</span><br><span class="line"></span><br><span class="line">* 参数:</span><br><span class="line"></span><br><span class="line"> * request (Request 对象) – 处理的request</span><br><span class="line"> * spider (Spider 对象) – 该request对应的spider</span><br><span class="line"> </span><br><span class="line"> ## process_response(self, request, response, spider)</span><br><span class="line">当下载器完成http请求，传递响应给引擎的时候调用</span><br><span class="line"></span><br><span class="line">* process_request() 必须返回以下其中之一: 返回一个 Response 对象、 返回一个 Request 对象或raise一个 IgnoreRequest 异常。</span><br><span class="line"></span><br><span class="line"> * 如果其返回一个 Response (可以与传入的response相同，也可以是全新的对象)， 该response会被在链中的其他中间件的 process_response() 方法处理。</span><br><span class="line"></span><br><span class="line"> * 如果其返回一个 Request 对象，则中间件链停止， 返回的request会被重新调度下载。处理类似于 process_request() 返回request所做的那样。</span><br><span class="line"></span><br><span class="line"> * 如果其抛出一个 IgnoreRequest 异常，则调用request的errback(Request.errback)。 如果没有代码处理抛出的异常，则该异常被忽略且不记录(不同于其他异常那样)。</span><br><span class="line"></span><br><span class="line">* 参数:</span><br><span class="line"></span><br><span class="line"> * request (Request 对象) – response所对应的request</span><br><span class="line"> * response (Response 对象) – 被处理的response</span><br><span class="line"> * spider (Spider 对象) – response所对应的spider</span><br><span class="line"></span><br><span class="line">## 使用案例：</span><br><span class="line">### 1. 创建```middlewares.py```文件。</span><br><span class="line">Scrapy代理IP、Uesr-Agent的切换都是通过```DOWNLOADER_MIDDLEWARES```进行控制，我们在```settings.py```同级目录下创建```middlewares.py```文件，包装所有请求。</span><br></pre></td></tr></table></figure></p>
<h1 id="middlewares-py"><a href="#middlewares-py" class="headerlink" title="middlewares.py"></a>middlewares.py</h1><p>#!/usr/bin/env python</p>
<h1 id="coding-utf-8-1"><a href="#coding-utf-8-1" class="headerlink" title="-- coding:utf-8 --"></a>-<em>- coding:utf-8 -</em>-</h1><p>import random<br>import base64</p>
<p>from settings import USER_AGENTS<br>from settings import PROXIES</p>
<h1 id="随机的User-Agent"><a href="#随机的User-Agent" class="headerlink" title="随机的User-Agent"></a>随机的User-Agent</h1><p>class RandomUserAgent(object):<br>    def process_request(self, request, spider):<br>        useragent = random.choice(USER_AGENTS)</p>
<pre><code>request.headers.setdefault(&quot;User-Agent&quot;, useragent)
</code></pre><p>class RandomProxy(object):<br>    def process_request(self, request, spider):<br>        proxy = random.choice(PROXIES)</p>
<pre><code>if proxy[&apos;user_passwd&apos;] is None:
    # 没有代理账户验证的代理使用方式
    request.meta[&apos;proxy&apos;] = &quot;http://&quot; + proxy[&apos;ip_port&apos;]
else:
    # 对账户密码进行base64编码转换
    base64_userpasswd = base64.b64encode(proxy[&apos;user_passwd&apos;])
    # 对应到代理服务器的信令格式里
    request.headers[&apos;Proxy-Authorization&apos;] = &apos;Basic &apos; + base64_userpasswd
    request.meta[&apos;proxy&apos;] = &quot;http://&quot; + proxy[&apos;ip_port&apos;]
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;为什么HTTP代理要使用base64编码：</span><br><span class="line"></span><br><span class="line">&gt;HTTP代理的原理很简单，就是通过HTTP协议与代理服务器建立连接，协议信令中包含要连接到的远程主机的IP和端口号，如果有需要身份验证的话还需要加上授权信息，服务器收到信令后首先进行身份验证，通过后便与远程主机建立连接，连接成功之后会返回给客户端200，表示验证通过，就这么简单，下面是具体的信令格式：</span><br></pre></td></tr></table></figure>
<p>CONNECT 59.64.128.198:21 HTTP/1.1<br>Host: 59.64.128.198:21<br>Proxy-Authorization: Basic bGV2I1TU5OTIz<br>User-Agent: OpenFetion<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;其中Proxy-Authorization是身份验证信息，Basic后面的字符串是用户名和密码组合后进行base64编码的结果，也就是对username:password进行base64编码。</span><br></pre></td></tr></table></figure></p>
<p>HTTP/1.0 200 Connection established<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;OK，客户端收到收面的信令后表示成功建立连接，接下来要发送给远程主机的数据就可以发送给代理服务器了，代理服务器建立连接后会在根据IP地址和端口号对应的连接放入缓存，收到信令后再根据IP地址和端口号从缓存中找到对应的连接，将数据通过该连接转发出去。</span><br><span class="line"></span><br><span class="line">### 2. 修改settings.py配置USER_AGENTS和PROXIES</span><br><span class="line">* 添加USER_AGENTS：</span><br></pre></td></tr></table></figure></p>
<p>　USER_AGENTS = [<br>    “Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)”,<br>    “Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)”,<br>    “Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)”,<br>    “Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1”,<br>    “Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0”,<br>    “Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5”<br>    ]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* 添加代理IP设置PROXIES：</span><br><span class="line"></span><br><span class="line">免费代理IP可以网上搜索，或者付费购买一批可用的私密代理IP：</span><br></pre></td></tr></table></figure></p>
<p>PROXIES = [<br>    {‘ip_port’: ‘111.8.60.9:8123’, ‘user_passwd’: ‘user1:pass1’},<br>    {‘ip_port’: ‘101.71.27.120:80’, ‘user_passwd’: ‘user2:pass2’},<br>    {‘ip_port’: ‘122.96.59.104:80’, ‘user_passwd’: ‘user3:pass3’},<br>    {‘ip_port’: ‘122.224.249.122:8088’, ‘user_passwd’: ‘user4:pass4’},<br>]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* 除非特殊需要，禁用cookies，防止某些网站根据Cookie来封锁爬虫。</span><br></pre></td></tr></table></figure></p>
<p>COOKIES_ENABLED = False<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* 设置下载延迟</span><br></pre></td></tr></table></figure></p>
<p>DOWNLOAD_DELAY = 3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* 最后设置setting.py里的DOWNLOADER_MIDDLEWARES，添加自己编写的下载中间件类。</span><br></pre></td></tr></table></figure></p>
<p>DOWNLOADER_MIDDLEWARES = {</p>
<pre><code>#&apos;mySpider.middlewares.MyCustomDownloaderMiddleware&apos;: 543,
&apos;mySpider.middlewares.RandomUserAgent&apos;: 1,
&apos;mySpider.middlewares.ProxyMiddleware&apos;: 100
</code></pre><p>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># Settings</span><br><span class="line">Scrapy设置(settings)提供了定制Scrapy组件的方法。可以控制包括核心(core)，插件(extension)，pipeline及spider组件。比如 设置Json Pipeliine、LOG_LEVEL等。</span><br><span class="line"></span><br><span class="line">参考文档：http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/settings.html#topics-settings-ref</span><br><span class="line">## 内置设置参考手册</span><br><span class="line">* BOT_NAME</span><br><span class="line"></span><br><span class="line"> * 默认: &apos;scrapybot&apos;</span><br><span class="line"></span><br><span class="line"> * 当您使用 startproject 命令创建项目时其也被自动赋值。</span><br><span class="line"></span><br><span class="line">* CONCURRENT_ITEMS</span><br><span class="line"></span><br><span class="line">  * 默认: 100</span><br><span class="line"></span><br><span class="line"> * Item Processor(即 Item Pipeline) 同时处理(每个response的)item的最大值。</span><br><span class="line"></span><br><span class="line">* CONCURRENT_REQUESTS</span><br><span class="line"> * 默认: 16</span><br><span class="line"></span><br><span class="line"> * Scrapy downloader 并发请求(concurrent requests)的最大值。</span><br><span class="line"></span><br><span class="line">* DEFAULT_REQUEST_HEADERS</span><br><span class="line">默认: 如下</span><br></pre></td></tr></table></figure></p>
<p>{<br>‘Accept’: ‘text/html,application/xhtml+xml,application/xml;q=0.9,<em>/</em>;q=0.8’,<br>‘Accept-Language’: ‘en’,<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Scrapy HTTP Request使用的默认header。</span><br><span class="line">* DEPTH_LIMIT</span><br><span class="line"></span><br><span class="line"> * 默认: 0</span><br><span class="line"></span><br><span class="line"> * 爬取网站最大允许的深度(depth)值。如果为0，则没有限制。</span><br><span class="line"></span><br><span class="line">* DOWNLOAD_DELAY</span><br><span class="line"> * 默认: 0</span><br><span class="line"></span><br><span class="line"> * 下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度， 减轻服务器压力。同时也支持小数:</span><br></pre></td></tr></table></figure></p>
<p> DOWNLOAD_DELAY = 0.25 # 250 ms of delay<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">  * 默认情况下，Scrapy在两个请求间不等待一个固定的值， 而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。</span><br><span class="line"> </span><br><span class="line">* DOWNLOAD_TIMEOUT</span><br><span class="line"></span><br><span class="line"> * 默认: 180</span><br><span class="line"></span><br><span class="line"> * 下载器超时时间(单位: 秒)。</span><br><span class="line"></span><br><span class="line">* ITEM_PIPELINES</span><br><span class="line"> * 默认: &#123;&#125;</span><br><span class="line"></span><br><span class="line"> * 保存项目中启用的pipeline及其顺序的字典。该字典默认为空，值(value)任意，不过值(value)习惯设置在0-1000范围内，值越小优先级越高。</span><br></pre></td></tr></table></figure></p>
<p>ITEM_PIPELINES = {<br>‘mySpider.pipelines.SomethingPipeline’: 300,<br>‘mySpider.pipelines.ItcastJsonPipeline’: 800,<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* LOG_ENABLED</span><br><span class="line"></span><br><span class="line"> * 默认: True</span><br><span class="line"></span><br><span class="line"> * 是否启用logging。</span><br><span class="line"></span><br><span class="line">* LOG_ENCODING</span><br><span class="line"></span><br><span class="line"> * 默认: &apos;utf-8&apos;</span><br><span class="line"></span><br><span class="line"> * logging使用的编码。</span><br><span class="line"></span><br><span class="line">* LOG_LEVEL</span><br><span class="line"></span><br><span class="line"> * 默认: &apos;DEBUG&apos;</span><br><span class="line"></span><br><span class="line"> * log的最低级别。可选的级别有: CRITICAL、 ERROR、WARNING、INFO、DEBUG 。</span><br><span class="line"></span><br><span class="line">* USER_AGENT</span><br><span class="line"> * 默认: &quot;Scrapy/VERSION (+http://scrapy.org)&quot;</span><br><span class="line"></span><br><span class="line"> * 爬取的默认User-Agent，除非被覆盖。</span><br><span class="line"></span><br><span class="line">* PROXIES： 代理设置</span><br><span class="line"> * 示例：</span><br></pre></td></tr></table></figure></p>
<p> PROXIES = [<br>  {‘ip_port’: ‘111.11.228.75:80’, ‘password’: ‘’},<br>  {‘ip_port’: ‘120.198.243.22:80’, ‘password’: ‘’},<br>  {‘ip_port’: ‘111.8.60.9:8123’, ‘password’: ‘’},<br>  {‘ip_port’: ‘101.71.27.120:80’, ‘password’: ‘’},<br>  {‘ip_port’: ‘122.96.59.104:80’, ‘password’: ‘’},<br>  {‘ip_port’: ‘122.224.249.122:8088’, ‘password’:’’},<br>]<br> <code>`</code></p>
<ul>
<li>COOKIES_ENABLED = False<ul>
<li>禁用Cookies</li>
</ul>
</li>
</ul>

            <div class="post-copyright">
    <div class="content">
        <p>最后更新： 2019年05月17日 20:16</p>
        <p>原始链接： <a class="post-url" href="/2019/05/17/Scrapy深入/" title="Scrapy 深入">https://zem12345678.github.io/2019/05/17/Scrapy深入/</a></p>
        <footer>
            <a href="https://zem12345678.github.io">
                <img src="/images/logo.png" alt="Enmin">
                Enmin
            </a>
        </footer>
    </div>
</div>

      
        
            
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;">赏</a>
</div>

<div id="reward" class="post-modal reward-lay">
    <a class="close" href="javascript:;" id="reward-close">×</a>
    <span class="reward-title">
        <i class="icon icon-quote-left"></i>
        请我吃糖~
        <i class="icon icon-quote-right"></i>
    </span>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/images/wx.png" alt="打赏二维码">
        </div>
        <div class="reward-select">
            
            <label class="reward-select-item checked" data-id="wechat" data-wechat="/images/wx.png">
                <img class="reward-select-item-wechat" src="/images/wechat.png" alt="微信">
            </label>
            
            
            <label class="reward-select-item" data-id="alipay" data-alipay="/images/zfb.png">
                <img class="reward-select-item-alipay" src="/images/alipay.png" alt="支付宝">
            </label>
            
        </div>
    </div>
</div>


        
    </div>
    <footer class="article-footer">
        
        
<div class="post-share">
    <a href="javascript:;" id="share-sub" class="post-share-fab">
        <i class="fa fa-share-alt"></i>
    </a>
    <div class="post-share-list" id="share-list">
        <ul class="share-icons">
          <li>
            <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://zem12345678.github.io/2019/05/17/Scrapy深入/&title=《Scrapy 深入》 — Enmin`s blog&pic=images/28.jpg" data-title="微博">
              <i class="fa fa-weibo"></i>
            </a>
          </li>
          <li>
            <a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
          <li>
            <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://zem12345678.github.io/2019/05/17/Scrapy深入/&title=《Scrapy 深入》 — Enmin`s blog&source=" data-title="QQ">
              <i class="fa fa-qq"></i>
            </a>
          </li>
          <li>
            <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://zem12345678.github.io/2019/05/17/Scrapy深入/" data-title="Facebook">
              <i class="fa fa-facebook"></i>
            </a>
          </li>
          <li>
            <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Scrapy 深入》 — Enmin`s blog&url=https://zem12345678.github.io/2019/05/17/Scrapy深入/&via=https://zem12345678.github.io" data-title="Twitter">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
          <li>
            <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://zem12345678.github.io/2019/05/17/Scrapy深入/" data-title="Google+">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        </ul>
     </div>
</div>
<div class="post-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;" id="wxShare-close">×</a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=https://zem12345678.github.io/2019/05/17/Scrapy深入/" alt="微信分享二维码">
</div>

<div class="mask"></div>

        
        <ul class="article-footer-menu">
            
            
  <li class="article-footer-tags">
    <i class="fa fa-tags"></i>
      
    <a href="/tags/爬虫/" class="color3">爬虫</a>
      
    <a href="/tags/Scrapy/" class="color2">Scrapy</a>
      
  </li>

        </ul>
        
    </footer>
  </div>
</article>


    <aside class="post-toc-pos post-toc-top" id="post-toc">
        <nav class="post-toc-wrap">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Item-Pipeline"><span class="post-toc-text">Item Pipeline</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#编写item-pipeline"><span class="post-toc-text">编写item pipeline</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#完善之前的案例"><span class="post-toc-text">完善之前的案例</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#item写入JSON文件"><span class="post-toc-text">item写入JSON文件</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#启用一个Item-Pipeline组件"><span class="post-toc-text">启用一个Item Pipeline组件</span></a></li><li class="post-toc-item post-toc-level-3"><a class="post-toc-link" href="#重新启动爬虫"><span class="post-toc-text">重新启动爬虫</span></a></li></ol></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Spider"><span class="post-toc-text">Spider</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#tencent-py"><span class="post-toc-text">tencent.py</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#parse-方法的工作机制："><span class="post-toc-text">parse()方法的工作机制：</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#常见bug"><span class="post-toc-text">常见bug</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#CrawlSpiders"><span class="post-toc-text">CrawlSpiders</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#源码参考"><span class="post-toc-text">源码参考</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#rules"><span class="post-toc-text">rules</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#LinkExtractors"><span class="post-toc-text">LinkExtractors</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#爬取规则-Crawling-rules"><span class="post-toc-text">爬取规则(Crawling rules)</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#CrawlSpider-版本"><span class="post-toc-text">CrawlSpider 版本</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#部分代码"><span class="post-toc-text">部分代码</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#部分代码-1"><span class="post-toc-text">部分代码</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#coding-utf-8"><span class="post-toc-text">-- coding:utf-8 --</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#middlewares-py"><span class="post-toc-text">middlewares.py</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#coding-utf-8-1"><span class="post-toc-text">-- coding:utf-8 --</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#随机的User-Agent"><span class="post-toc-text">随机的User-Agent</span></a></li></ol>
        </nav>
    </aside>
    

<nav id="article-nav">
  
  
    <a href="/2019/05/17/Scrapy Shell/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-title">Scrapy Shell</span>
      <i class="fa fa-hand-o-right" aria-hidden="true"></i>
    </a>
  
</nav>



    
        <div id="SOHUCS" sid="Scrapy深入" ></div>
<script type="text/javascript">
    (function(){
        var appid = 'cytRXLIlH';
        var conf = '983bf9e4c3b1e70e33660721bdb62bb2';
        var width = window.innerWidth || document.documentElement.clientWidth;
        if (width < 960) {
            window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>
    
</section>
        
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：20<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：20<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


      <p>
        Powered by  <a href="http://hexo.io/" target="_blank">Hexo</a>
        Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a>
      &copy; 2019 Enmin<br>
      </p>
    </div>
  </div>
</footer>
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script>
  var mihoConfig = {
      root: "https://zem12345678.github.io",
      animate: true,
      isHome: false,
      share: true,
      reward: 1
  }
</script>
<div class="sidebar">
    <div id="sidebar-search" title="Search">
        <i class="fa fa-search"></i>
    </div>
    <div id="sidebar-category" title="Categories">
        <i class="fa fa-book"></i>
    </div>
    <div id="sidebar-tag" title="Tags">
        <i class="fa fa-tags"></i>
    </div>
    <div id="sidebar-top">
        <span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span>
    </div>
</div>
<div class="sidebar-menu-box" id="sidebar-menu-box">
    <div class="sidebar-menu-box-container">
        <div id="sidebar-menu-box-categories">
            <a class="category-link" href="/categories/Django/">Django</a><a class="category-link" href="/categories/Docker/">Docker</a><a class="category-link" href="/categories/FastDFS/">FastDFS</a><a class="category-link" href="/categories/Flask/">Flask</a><a class="category-link" href="/categories/JWT/">JWT</a><a class="category-link" href="/categories/Machine-learning/">Machine learning</a><a class="category-link" href="/categories/Python/">Python</a><a class="category-link" href="/categories/Redis/">Redis</a><a class="category-link" href="/categories/django/">django</a><a class="category-link" href="/categories/前后端分离/">前后端分离</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/大数据/">大数据</a><a class="category-link" href="/categories/学习笔记/">学习笔记</a><a class="category-link" href="/categories/异步处理/">异步处理</a><a class="category-link" href="/categories/数据库/">数据库</a><a class="category-link" href="/categories/数据挖掘/">数据挖掘</a><a class="category-link" href="/categories/杂谈/">杂谈</a><a class="category-link" href="/categories/算法/">算法</a><a class="category-link" href="/categories/网络原理/">网络原理</a><a class="category-link" href="/categories/随堂笔记/">随堂笔记</a>
        </div>
        <div id="sidebar-menu-box-tags">
            <a href="/tags/BeautifuSoup4/" style="font-size: 10px;">BeautifuSoup4</a> <a href="/tags/BeautifulSoup4/" style="font-size: 10px;">BeautifulSoup4</a> <a href="/tags/C-C/" style="font-size: 10px;">C/C++</a> <a href="/tags/Django/" style="font-size: 18.57px;">Django</a> <a href="/tags/Docker/" style="font-size: 12.86px;">Docker</a> <a href="/tags/ElasticSearch/" style="font-size: 11.43px;">ElasticSearch</a> <a href="/tags/FastDFS/" style="font-size: 12.86px;">FastDFS</a> <a href="/tags/Flask/" style="font-size: 12.86px;">Flask</a> <a href="/tags/Gunicorn/" style="font-size: 10px;">Gunicorn</a> <a href="/tags/HTTP-HTTPS/" style="font-size: 11.43px;">HTTP/HTTPS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 11.43px;">Java</a> <a href="/tags/Jwt/" style="font-size: 11.43px;">Jwt</a> <a href="/tags/LeetCode/" style="font-size: 10px;">LeetCode</a> <a href="/tags/LeetCood/" style="font-size: 11.43px;">LeetCood</a> <a href="/tags/Machine-learning/" style="font-size: 12.86px;">Machine learning</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/Mysql/" style="font-size: 11.43px;">Mysql</a> <a href="/tags/Nginx/" style="font-size: 11.43px;">Nginx</a> <a href="/tags/Python/" style="font-size: 17.14px;">Python</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 15.71px;">Redis</a> <a href="/tags/Requests/" style="font-size: 10px;">Requests</a> <a href="/tags/Restful/" style="font-size: 12.86px;">Restful</a> <a href="/tags/Scrapy/" style="font-size: 12.86px;">Scrapy</a> <a href="/tags/Spring/" style="font-size: 10px;">Spring</a> <a href="/tags/SpringBoot/" style="font-size: 10px;">SpringBoot</a> <a href="/tags/SpringCloud/" style="font-size: 10px;">SpringCloud</a> <a href="/tags/Token/" style="font-size: 11.43px;">Token</a> <a href="/tags/Urllib/" style="font-size: 10px;">Urllib</a> <a href="/tags/Vue/" style="font-size: 11.43px;">Vue</a> <a href="/tags/Web/" style="font-size: 15.71px;">Web</a> <a href="/tags/XPath/" style="font-size: 11.43px;">XPath</a> <a href="/tags/python/" style="font-size: 11.43px;">python</a> <a href="/tags/决策树分类/" style="font-size: 10px;">决策树分类</a> <a href="/tags/分布式/" style="font-size: 12.86px;">分布式</a> <a href="/tags/前后端分离/" style="font-size: 14.29px;">前后端分离</a> <a href="/tags/前端/" style="font-size: 10px;">前端</a> <a href="/tags/回归分类/" style="font-size: 10px;">回归分类</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/容器/" style="font-size: 11.43px;">容器</a> <a href="/tags/富文本/" style="font-size: 10px;">富文本</a> <a href="/tags/微服务/" style="font-size: 10px;">微服务</a> <a href="/tags/搜索引擎/" style="font-size: 11.43px;">搜索引擎</a> <a href="/tags/数据库/" style="font-size: 10px;">数据库</a> <a href="/tags/数据库集群/" style="font-size: 10px;">数据库集群</a> <a href="/tags/数据结构/" style="font-size: 12.86px;">数据结构</a> <a href="/tags/杂谈/" style="font-size: 10px;">杂谈</a> <a href="/tags/权限认证/" style="font-size: 10px;">权限认证</a> <a href="/tags/正则表达式/" style="font-size: 11.43px;">正则表达式</a> <a href="/tags/消息队列/" style="font-size: 10px;">消息队列</a> <a href="/tags/爬虫/" style="font-size: 20px;">爬虫</a> <a href="/tags/算法/" style="font-size: 15.71px;">算法</a> <a href="/tags/编码/" style="font-size: 11.43px;">编码</a> <a href="/tags/虚拟化/" style="font-size: 11.43px;">虚拟化</a> <a href="/tags/跨域/" style="font-size: 10px;">跨域</a> <a href="/tags/随机森林/" style="font-size: 10px;">随机森林</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
        </div>
    </div>
    <a href="javascript:;" class="sidebar-menu-box-close">&times;</a>
</div>
<div class="mobile-header-menu-nav" id="mobile-header-menu-nav">
    <div class="mobile-header-menu-container">
        <span class="title">Menus</span>
        <ul class="mobile-header-menu-navbar">
            
            <li>
                <a  href="/">
                    <i class="fa fa-home"></i><span>主页</span>
                </a>
            </li>
            
            <li>
                <a  href="/archives">
                    <i class="fa fa-archive"></i><span>文章</span>
                </a>
            </li>
            
            <li>
                <a  href="/about">
                    <i class="fa fa-user"></i><span>关于我</span>
                </a>
            </li>
            
        </ul>
    </div>
    <div class="mobile-header-tag-container">
        <span class="title">Tags</span>
        <div id="mobile-header-container-tags">
            <a href="/tags/BeautifuSoup4/" style="font-size: 10px;">BeautifuSoup4</a> <a href="/tags/BeautifulSoup4/" style="font-size: 10px;">BeautifulSoup4</a> <a href="/tags/C-C/" style="font-size: 10px;">C/C++</a> <a href="/tags/Django/" style="font-size: 18.57px;">Django</a> <a href="/tags/Docker/" style="font-size: 12.86px;">Docker</a> <a href="/tags/ElasticSearch/" style="font-size: 11.43px;">ElasticSearch</a> <a href="/tags/FastDFS/" style="font-size: 12.86px;">FastDFS</a> <a href="/tags/Flask/" style="font-size: 12.86px;">Flask</a> <a href="/tags/Gunicorn/" style="font-size: 10px;">Gunicorn</a> <a href="/tags/HTTP-HTTPS/" style="font-size: 11.43px;">HTTP/HTTPS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 11.43px;">Java</a> <a href="/tags/Jwt/" style="font-size: 11.43px;">Jwt</a> <a href="/tags/LeetCode/" style="font-size: 10px;">LeetCode</a> <a href="/tags/LeetCood/" style="font-size: 11.43px;">LeetCood</a> <a href="/tags/Machine-learning/" style="font-size: 12.86px;">Machine learning</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/Mysql/" style="font-size: 11.43px;">Mysql</a> <a href="/tags/Nginx/" style="font-size: 11.43px;">Nginx</a> <a href="/tags/Python/" style="font-size: 17.14px;">Python</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 15.71px;">Redis</a> <a href="/tags/Requests/" style="font-size: 10px;">Requests</a> <a href="/tags/Restful/" style="font-size: 12.86px;">Restful</a> <a href="/tags/Scrapy/" style="font-size: 12.86px;">Scrapy</a> <a href="/tags/Spring/" style="font-size: 10px;">Spring</a> <a href="/tags/SpringBoot/" style="font-size: 10px;">SpringBoot</a> <a href="/tags/SpringCloud/" style="font-size: 10px;">SpringCloud</a> <a href="/tags/Token/" style="font-size: 11.43px;">Token</a> <a href="/tags/Urllib/" style="font-size: 10px;">Urllib</a> <a href="/tags/Vue/" style="font-size: 11.43px;">Vue</a> <a href="/tags/Web/" style="font-size: 15.71px;">Web</a> <a href="/tags/XPath/" style="font-size: 11.43px;">XPath</a> <a href="/tags/python/" style="font-size: 11.43px;">python</a> <a href="/tags/决策树分类/" style="font-size: 10px;">决策树分类</a> <a href="/tags/分布式/" style="font-size: 12.86px;">分布式</a> <a href="/tags/前后端分离/" style="font-size: 14.29px;">前后端分离</a> <a href="/tags/前端/" style="font-size: 10px;">前端</a> <a href="/tags/回归分类/" style="font-size: 10px;">回归分类</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/容器/" style="font-size: 11.43px;">容器</a> <a href="/tags/富文本/" style="font-size: 10px;">富文本</a> <a href="/tags/微服务/" style="font-size: 10px;">微服务</a> <a href="/tags/搜索引擎/" style="font-size: 11.43px;">搜索引擎</a> <a href="/tags/数据库/" style="font-size: 10px;">数据库</a> <a href="/tags/数据库集群/" style="font-size: 10px;">数据库集群</a> <a href="/tags/数据结构/" style="font-size: 12.86px;">数据结构</a> <a href="/tags/杂谈/" style="font-size: 10px;">杂谈</a> <a href="/tags/权限认证/" style="font-size: 10px;">权限认证</a> <a href="/tags/正则表达式/" style="font-size: 11.43px;">正则表达式</a> <a href="/tags/消息队列/" style="font-size: 10px;">消息队列</a> <a href="/tags/爬虫/" style="font-size: 20px;">爬虫</a> <a href="/tags/算法/" style="font-size: 15.71px;">算法</a> <a href="/tags/编码/" style="font-size: 11.43px;">编码</a> <a href="/tags/虚拟化/" style="font-size: 11.43px;">虚拟化</a> <a href="/tags/跨域/" style="font-size: 10px;">跨域</a> <a href="/tags/随机森林/" style="font-size: 10px;">随机森林</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
        </div>
    </div>
</div>
<div class="search-wrap">
    <span class="search-close">&times;</span>
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
            <i class="icon icon-lg icon-chevron-left"></i>
        </a>
        <input class="search-field" placeholder="Search..." id="keywords">
        <a id="search-submit" href="javascript:;">
            <i class="fa fa-search"></i>
        </a>
    <div class="search-container" id="search-container">
        <ul class="search-result" id="search-result">
        </ul>
    </div>
</div>

<div id="search-tpl">
    <li class="search-result-item">
        <a href="{url}" class="search-item-li">
            <span class="search-item-li-title" title="{title}">{title}</span>
        </a>
    </li>
</div>
<script src="/js/search.js"></script>
<script src="/js/main.js"></script>


  <script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script>
  <div id="particles"></div>
  <script src="/js/particles.js"></script>







  <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">
  <script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script>
  <script src="/js/animate.js"></script>


  <script src="/js/pop-img.js"></script>
  <script>
     $(".article-entry p img").popImg();
  </script>

  </div>
</body>
</html>