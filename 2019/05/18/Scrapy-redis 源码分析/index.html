<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Scrapy-redis 源码分析 | Enmin`s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
    <meta name="keywords" content="Enmin,ZEnmin's Blog" />
  
  <meta name="description" content="Scrapy-redis 源码分析官方站点：https://github.com/rolando/scrapy-redis scrapy-redis的官方文档写的比较简洁，没有提及其运行原理，所以如果想全面的理解分布式爬虫的运行原理，还是得看scrapy-redis的源代码才行。 scrapy-redis工程的主体还是是redis和scrapy两个库，工程本身实现的东西不是很多，这个工程就像胶水一">
<meta name="keywords" content="爬虫,Scrapy">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy-redis 源码分析">
<meta property="og:url" content="https://zem12345678.github.io/2019/05/18/Scrapy-redis 源码分析/index.html">
<meta property="og:site_name" content="Enmin`s blog">
<meta property="og:description" content="Scrapy-redis 源码分析官方站点：https://github.com/rolando/scrapy-redis scrapy-redis的官方文档写的比较简洁，没有提及其运行原理，所以如果想全面的理解分布式爬虫的运行原理，还是得看scrapy-redis的源代码才行。 scrapy-redis工程的主体还是是redis和scrapy两个库，工程本身实现的东西不是很多，这个工程就像胶水一">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-05-18T12:51:50.511Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scrapy-redis 源码分析">
<meta name="twitter:description" content="Scrapy-redis 源码分析官方站点：https://github.com/rolando/scrapy-redis scrapy-redis的官方文档写的比较简洁，没有提及其运行原理，所以如果想全面的理解分布式爬虫的运行原理，还是得看scrapy-redis的源代码才行。 scrapy-redis工程的主体还是是redis和scrapy两个库，工程本身实现的东西不是很多，这个工程就像胶水一">
  
  
    <link rel="icon" href="/favicon1.ico">
  
  <link href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
  <script src="/js/pace.min.js"></script>
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-127786403-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?4802669a9f01ae631292334aca36a944";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

  
  <div style="display: none;">
    <script src="//s22.cnzz.com/z_stat.php?id=1275093534&web_id=1275093534" language="JavaScript"></script>
  </div>


</head>

<body>
  <div id="container">
      <header id="header">
    <div id="banner"></div>
    <div id="header-outer">
        <div id="header-menu" class="header-menu-pos animated">
            <div class="header-menu-container">
                <a href="/" class="left">
                    <span class="site-title">Enmin&#39;s Blog</span>
                </a>
                <nav id="header-menu-nav" class="right">
                    
                    <a  href="/">
                        <i class="fa fa-home"></i>
                        <span>主页</span>
                    </a>
                    
                    <a  href="/archives">
                        <i class="fa fa-archive"></i>
                        <span>文章</span>
                    </a>
                    
                    <a  href="/about">
                        <i class="fa fa-user"></i>
                        <span>关于我</span>
                    </a>
                    
                </nav>
                <a class="mobile-header-menu-button">
                    <i class="fa fa-bars"></i>
                </a>
            </div>
        </div>
        <div id="header-row">
            <div id="logo">
                <a href="/">
                    <img src="/images/logo.png" alt="logo">
                </a>
            </div>
            <div class="header-info">
                <div id="header-title">
                    
                    <h2>
                        Enmin&#39;s Blog
                    </h2>
                    
                </div>
                <div id="header-description">
                    
                    <h3>
                        漫宅万岁，bilibili (゜-゜)つロ 乾杯~-
                    </h3>
                    
                </div>
            </div>
            <nav class="header-nav">
                <div class="social">
                    
                        <a title="zem12345678" target="_blank" href="//https://zem12345678.github.io/">
                            <i class="fa fa-home fa-2x"></i></a>
                    
                        <a title="Github" target="_blank" href="//github.com/zem12345678">
                            <i class="fa fa-github fa-2x"></i></a>
                    
                        <a title="Weibo" target="_blank" href="//weibo.com/zem12345678">
                            <i class="fa fa-weibo fa-2x"></i></a>
                    
                        <a title="Twitter" target="_blank" href="//twitter.com/Enmin_zhang">
                            <i class="fa fa-twitter fa-2x"></i></a>
                    
                </div>
            </nav>
        </div>
    </div>
</header>
      <div class="outer">
        <section id="main" class="body-wrap"><article id="post-Scrapy-redis 源码分析" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="post-title" itemprop="name">
      Scrapy-redis 源码分析
    </h1>
    <div class="post-title-bar">
      <ul>
          
              <li>
                  <i class="fa fa-book"></i>
                  
                      <a href="/categories/数据挖掘/">数据挖掘</a>
                  
              </li>
          
        <li>
          <i class="fa fa-calendar"></i>  2019-05-18
        </li>
        <li>
          <i class="fa fa-eye"></i>
          <span id="busuanzi_value_page_pv"></span>
        </li>
      </ul>
    </div>
  

          
      </header>
    
    <div class="article-entry post-content" itemprop="articleBody">
      
            
            <h1 id="Scrapy-redis-源码分析"><a href="#Scrapy-redis-源码分析" class="headerlink" title="Scrapy-redis 源码分析"></a>Scrapy-redis 源码分析</h1><p>官方站点：<a href="https://github.com/rolando/scrapy-redis" target="_blank" rel="noopener">https://github.com/rolando/scrapy-redis</a></p>
<p>scrapy-redis的官方文档写的比较简洁，没有提及其运行原理，所以如果想全面的理解分布式爬虫的运行原理，还是得看scrapy-redis的源代码才行。</p>
<p>scrapy-redis工程的主体还是是redis和scrapy两个库，工程本身实现的东西不是很多，这个工程就像胶水一样，把这两个插件粘结了起来。下面我们来看看，scrapy-redis的每一个源代码文件都实现了什么功能，最后如何实现分布式的爬虫系统：</p>
<h2 id="1-connection-py"><a href="#1-connection-py" class="headerlink" title="1. connection.py"></a>1. connection.py</h2><p>负责根据setting中配置实例化redis连接。被dupefilter和scheduler调用，总之涉及到redis存取的都要使用到这个模块。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"> 这里引入了redis模块，这个是redis-python库的接口，用于通过python访问redis数据库，</span><br><span class="line"># 这个文件主要是实现连接redis数据库的功能，这些连接接口在其他文件中经常被用到</span><br><span class="line"></span><br><span class="line">import redis</span><br><span class="line">import six</span><br><span class="line"></span><br><span class="line">from scrapy.utils.misc import load_object</span><br><span class="line"></span><br><span class="line">DEFAULT_REDIS_CLS = redis.StrictRedis</span><br><span class="line"></span><br><span class="line"># 可以在settings文件中配置套接字的超时时间、等待时间等</span><br><span class="line"># Sane connection defaults.</span><br><span class="line">DEFAULT_PARAMS = &#123;</span><br><span class="line">    &apos;socket_timeout&apos;: 30,</span><br><span class="line">    &apos;socket_connect_timeout&apos;: 30,</span><br><span class="line">    &apos;retry_on_timeout&apos;: True,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 要想连接到redis数据库，和其他数据库差不多，需要一个ip地址、端口号、用户名密码（可选）和一个整形的数据库编号</span><br><span class="line"># Shortcut maps &apos;setting name&apos; -&gt; &apos;parmater name&apos;.</span><br><span class="line">SETTINGS_PARAMS_MAP = &#123;</span><br><span class="line">    &apos;REDIS_URL&apos;: &apos;url&apos;,</span><br><span class="line">    &apos;REDIS_HOST&apos;: &apos;host&apos;,</span><br><span class="line">    &apos;REDIS_PORT&apos;: &apos;port&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_redis_from_settings(settings):</span><br><span class="line">    &quot;&quot;&quot;Returns a redis client instance from given Scrapy settings object.</span><br><span class="line">    This function uses ``get_client`` to instantiate the client and uses</span><br><span class="line">    ``DEFAULT_PARAMS`` global as defaults values for the parameters. You can</span><br><span class="line">    override them using the ``REDIS_PARAMS`` setting.</span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    settings : Settings</span><br><span class="line">        A scrapy settings object. See the supported settings below.</span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line">    server</span><br><span class="line">        Redis client instance.</span><br><span class="line">    Other Parameters</span><br><span class="line">    ----------------</span><br><span class="line">    REDIS_URL : str, optional</span><br><span class="line">        Server connection URL.</span><br><span class="line">    REDIS_HOST : str, optional</span><br><span class="line">        Server host.</span><br><span class="line">    REDIS_PORT : str, optional</span><br><span class="line">        Server port.</span><br><span class="line">    REDIS_PARAMS : dict, optional</span><br><span class="line">        Additional client parameters.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    params = DEFAULT_PARAMS.copy()</span><br><span class="line">    params.update(settings.getdict(&apos;REDIS_PARAMS&apos;))</span><br><span class="line">    # XXX: Deprecate REDIS_* settings.</span><br><span class="line">    for source, dest in SETTINGS_PARAMS_MAP.items():</span><br><span class="line">        val = settings.get(source)</span><br><span class="line">        if val:</span><br><span class="line">            params[dest] = val</span><br><span class="line"></span><br><span class="line">    # Allow ``redis_cls`` to be a path to a class.</span><br><span class="line">    if isinstance(params.get(&apos;redis_cls&apos;), six.string_types):</span><br><span class="line">        params[&apos;redis_cls&apos;] = load_object(params[&apos;redis_cls&apos;])</span><br><span class="line"></span><br><span class="line">    # 返回的是redis库的Redis对象，可以直接用来进行数据操作的对象</span><br><span class="line">    return get_redis(**params)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Backwards compatible alias.</span><br><span class="line">from_settings = get_redis_from_settings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_redis(**kwargs):</span><br><span class="line">    &quot;&quot;&quot;Returns a redis client instance.</span><br><span class="line">    Parameters</span><br><span class="line">    ----------</span><br><span class="line">    redis_cls : class, optional</span><br><span class="line">        Defaults to ``redis.StrictRedis``.</span><br><span class="line">    url : str, optional</span><br><span class="line">        If given, ``redis_cls.from_url`` is used to instantiate the class.</span><br><span class="line">    **kwargs</span><br><span class="line">        Extra parameters to be passed to the ``redis_cls`` class.</span><br><span class="line">    Returns</span><br><span class="line">    -------</span><br><span class="line">    server</span><br><span class="line">        Redis client instance.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    redis_cls = kwargs.pop(&apos;redis_cls&apos;, DEFAULT_REDIS_CLS)</span><br><span class="line">    url = kwargs.pop(&apos;url&apos;, None)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if url:</span><br><span class="line">        return redis_cls.from_url(url, **kwargs)</span><br><span class="line">    else:</span><br><span class="line">        return redis_cls(**kwargs)</span><br></pre></td></tr></table></figure></p>
<h2 id="2-dupefilter-py"><a href="#2-dupefilter-py" class="headerlink" title="2. dupefilter.py"></a>2. dupefilter.py</h2><p>负责执行requst的去重，实现的很有技巧性，使用redis的set数据结构。但是注意scheduler并不使用其中用于在这个模块中实现的dupefilter键做request的调度，而是使用queue.py模块中实现的queue。</p>
<p>当request不重复时，将其存入到queue中，调度时将其弹出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">from scrapy.dupefilters import BaseDupeFilter</span><br><span class="line">from scrapy.utils.request import request_fingerprint</span><br><span class="line"></span><br><span class="line">from .connection import get_redis_from_settings</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DEFAULT_DUPEFILTER_KEY = &quot;dupefilter:%(timestamp)s&quot;</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger(__name__)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: Rename class to RedisDupeFilter.</span><br><span class="line">class RFPDupeFilter(BaseDupeFilter):</span><br><span class="line">    &quot;&quot;&quot;Redis-based request duplicates filter.</span><br><span class="line">    This class can also be used with default Scrapy&apos;s scheduler.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    logger = logger</span><br><span class="line"></span><br><span class="line">    def __init__(self, server, key, debug=False):</span><br><span class="line">        &quot;&quot;&quot;Initialize the duplicates filter.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        server : redis.StrictRedis</span><br><span class="line">            The redis server instance.</span><br><span class="line">        key : str</span><br><span class="line">            Redis key Where to store fingerprints.</span><br><span class="line">        debug : bool, optional</span><br><span class="line">            Whether to log filtered requests.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.server = server</span><br><span class="line">        self.key = key</span><br><span class="line">        self.debug = debug</span><br><span class="line">        self.logdupes = True</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_settings(cls, settings):</span><br><span class="line">        &quot;&quot;&quot;Returns an instance from given settings.</span><br><span class="line">        This uses by default the key ``dupefilter:&lt;timestamp&gt;``. When using the</span><br><span class="line">        ``scrapy_redis.scheduler.Scheduler`` class, this method is not used as</span><br><span class="line">        it needs to pass the spider name in the key.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        settings : scrapy.settings.Settings</span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        RFPDupeFilter</span><br><span class="line">            A RFPDupeFilter instance.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        server = get_redis_from_settings(settings)</span><br><span class="line">        # XXX: This creates one-time key. needed to support to use this</span><br><span class="line">        # class as standalone dupefilter with scrapy&apos;s default scheduler</span><br><span class="line">        # if scrapy passes spider on open() method this wouldn&apos;t be needed</span><br><span class="line">        # TODO: Use SCRAPY_JOB env as default and fallback to timestamp.</span><br><span class="line">        key = DEFAULT_DUPEFILTER_KEY % &#123;&apos;timestamp&apos;: int(time.time())&#125;</span><br><span class="line">        debug = settings.getbool(&apos;DUPEFILTER_DEBUG&apos;)</span><br><span class="line">        return cls(server, key=key, debug=debug)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        &quot;&quot;&quot;Returns instance from crawler.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        crawler : scrapy.crawler.Crawler</span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        RFPDupeFilter</span><br><span class="line">            Instance of RFPDupeFilter.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return cls.from_settings(crawler.settings)</span><br><span class="line"></span><br><span class="line">    def request_seen(self, request):</span><br><span class="line">        &quot;&quot;&quot;Returns True if request was already seen.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        request : scrapy.http.Request</span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        bool</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        fp = self.request_fingerprint(request)</span><br><span class="line">        # This returns the number of values added, zero if already exists.</span><br><span class="line">        added = self.server.sadd(self.key, fp)</span><br><span class="line">        return added == 0</span><br><span class="line"></span><br><span class="line">    def request_fingerprint(self, request):</span><br><span class="line">        &quot;&quot;&quot;Returns a fingerprint for a given request.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        request : scrapy.http.Request</span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        str</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return request_fingerprint(request)</span><br><span class="line"></span><br><span class="line">    def close(self, reason=&apos;&apos;):</span><br><span class="line">        &quot;&quot;&quot;Delete data on close. Called by Scrapy&apos;s scheduler.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        reason : str, optional</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.clear()</span><br><span class="line"></span><br><span class="line">    def clear(self):</span><br><span class="line">        &quot;&quot;&quot;Clears fingerprints data.&quot;&quot;&quot;</span><br><span class="line">        self.server.delete(self.key)</span><br><span class="line"></span><br><span class="line">    def log(self, request, spider):</span><br><span class="line">        &quot;&quot;&quot;Logs given request.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        request : scrapy.http.Request</span><br><span class="line">        spider : scrapy.spiders.Spider</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if self.debug:</span><br><span class="line">            msg = &quot;Filtered duplicate request: %(request)s&quot;</span><br><span class="line">            self.logger.debug(msg, &#123;&apos;request&apos;: request&#125;, extra=&#123;&apos;spider&apos;: spider&#125;)</span><br><span class="line">        elif self.logdupes:</span><br><span class="line">            msg = (&quot;Filtered duplicate request %(request)s&quot;</span><br><span class="line">                   &quot; - no more duplicates will be shown&quot;</span><br><span class="line">                   &quot; (see DUPEFILTER_DEBUG to show all duplicates)&quot;)</span><br><span class="line">            msg = &quot;Filtered duplicate request: %(request)s&quot;</span><br><span class="line">            self.logger.debug(msg, &#123;&apos;request&apos;: request&#125;, extra=&#123;&apos;spider&apos;: spider&#125;)</span><br><span class="line">            self.logdupes = False</span><br></pre></td></tr></table></figure></p>
<p>这个文件看起来比较复杂，重写了scrapy本身已经实现的request判重功能。因为本身scrapy单机跑的话，只需要读取内存中的request队列或者持久化的request队列（scrapy默认的持久化似乎是json格式的文件，不是数据库）就能判断这次要发出的request url是否已经请求过或者正在调度（本地读就行了）。而分布式跑的话，就需要各个主机上的scheduler都连接同一个数据库的同一个request池来判断这次的请求是否是重复的了。</p>
<p>在这个文件中，通过继承BaseDupeFilter重写他的方法，实现了基于redis的判重。根据源代码来看，scrapy-redis使用了scrapy本身的一个fingerprint接request_fingerprint，这个接口很有趣，根据scrapy文档所说，他通过hash来判断两个url是否相同（相同的url会生成相同的hash结果），但是当两个url的地址相同，get型参数相同但是顺序不同时，也会生成相同的hash结果（这个真的比较神奇。。。）所以scrapy-redis依旧使用url的fingerprint来判断request请求是否已经出现过。</p>
<p>这个类通过连接redis，使用一个key来向redis的一个set中插入fingerprint（这个key对于同一种spider是相同的，redis是一个key-value的数据库，如果key是相同的，访问到的值就是相同的，这里使用spider名字+DupeFilter的key就是为了在不同主机上的不同爬虫实例，只要属于同一种spider，就会访问到同一个set，而这个set就是他们的url判重池），如果返回值为0，说明该set中该fingerprint已经存在（因为集合是没有重复值的），则返回False，如果返回值为1，说明添加了一个fingerprint到set中，则说明这个request没有重复，于是返回True，还顺便把新fingerprint加入到数据库中了。 DupeFilter判重会在scheduler类中用到，每一个request在进入调度之前都要进行判重，如果重复就不需要参加调度，直接舍弃就好了，不然就是白白浪费资源。</p>
<h2 id="3-picklecompat-py"><a href="#3-picklecompat-py" class="headerlink" title="3. picklecompat.py"></a>3. picklecompat.py</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;A pickle wrapper module with protocol=-1 by default.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    import cPickle as pickle  # PY2</span><br><span class="line">except ImportError:</span><br><span class="line">    import pickle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def loads(s):</span><br><span class="line">    return pickle.loads(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def dumps(obj):</span><br><span class="line">    return pickle.dumps(obj, protocol=-1)</span><br></pre></td></tr></table></figure>
<p>这里实现了loads和dumps两个函数，其实就是实现了一个序列化器。</p>
<p>因为redis数据库不能存储复杂对象（key部分只能是字符串，value部分只能是字符串，字符串列表，字符串集合和hash），所以我们存啥都要先串行化成文本才行。</p>
<p>这里使用的就是python的pickle模块，一个兼容py2和py3的串行化工具。这个serializer主要用于一会的scheduler存reuqest对象。</p>
<h2 id="4-pipelines-py"><a href="#4-pipelines-py" class="headerlink" title="4.pipelines.py"></a>4.pipelines.py</h2><p>这是是用来实现分布式处理的作用。它将Item存储在redis中以实现分布式处理。由于在这里需要读取配置，所以就用到了from_crawler()函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.utils.misc import load_object</span><br><span class="line">from scrapy.utils.serialize import ScrapyJSONEncoder</span><br><span class="line">from twisted.internet.threads import deferToThread</span><br><span class="line"></span><br><span class="line">from . import connection</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">default_serialize = ScrapyJSONEncoder().encode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RedisPipeline(object):</span><br><span class="line">    &quot;&quot;&quot;Pushes serialized item into a redis list/queue&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, server,</span><br><span class="line">                 key=&apos;%(spider)s:items&apos;,</span><br><span class="line">                 serialize_func=default_serialize):</span><br><span class="line">        self.server = server</span><br><span class="line">        self.key = key</span><br><span class="line">        self.serialize = serialize_func</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_settings(cls, settings):</span><br><span class="line">        params = &#123;</span><br><span class="line">            &apos;server&apos;: connection.from_settings(settings),</span><br><span class="line">        &#125;</span><br><span class="line">        if settings.get(&apos;REDIS_ITEMS_KEY&apos;):</span><br><span class="line">            params[&apos;key&apos;] = settings[&apos;REDIS_ITEMS_KEY&apos;]</span><br><span class="line">        if settings.get(&apos;REDIS_ITEMS_SERIALIZER&apos;):</span><br><span class="line">            params[&apos;serialize_func&apos;] = load_object(</span><br><span class="line">                settings[&apos;REDIS_ITEMS_SERIALIZER&apos;]</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        return cls(**params)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        return cls.from_settings(crawler.settings)</span><br><span class="line"></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        return deferToThread(self._process_item, item, spider)</span><br><span class="line"></span><br><span class="line">    def _process_item(self, item, spider):</span><br><span class="line">        key = self.item_key(item, spider)</span><br><span class="line">        data = self.serialize(item)</span><br><span class="line">        self.server.rpush(key, data)</span><br><span class="line">        return item</span><br><span class="line"></span><br><span class="line">    def item_key(self, item, spider):</span><br><span class="line">        &quot;&quot;&quot;Returns redis key based on given spider.</span><br><span class="line">        Override this function to use a different key depending on the item</span><br><span class="line">        and/or spider.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return self.key % &#123;&apos;spider&apos;: spider.name&#125;</span><br></pre></td></tr></table></figure></p>
<p>pipelines文件实现了一个item pipieline类，和scrapy的item pipeline是同一个对象，通过从settings中拿到我们配置的REDIS_ITEMS_KEY作为key，把item串行化之后存入redis数据库对应的value中（这个value可以看出出是个list，我们的每个item是这个list中的一个结点），这个pipeline把提取出的item存起来，主要是为了方便我们延后处理数据。</p>
<h2 id="5-queue-py"><a href="#5-queue-py" class="headerlink" title="5.queue.py"></a>5.queue.py</h2><p>该文件实现了几个容器类，可以看这些容器和redis交互频繁，同时使用了我们上边picklecompat中定义的序列化器。这个文件实现的几个容器大体相同，只不过一个是队列，一个是栈，一个是优先级队列，这三个容器到时候会被scheduler对象实例化，来实现request的调度。比如我们使用SpiderQueue最为调度队列的类型，到时候request的调度方法就是先进先出，而实用SpiderStack就是先进后出了。</p>
<p>从SpiderQueue的实现看出来，他的push函数就和其他容器的一样，只不过push进去的request请求先被scrapy的接口request_to_dict变成了一个dict对象（因为request对象实在是比较复杂，有方法有属性不好串行化），之后使用picklecompat中的serializer串行化为字符串，然后使用一个特定的key存入redis中（该key在同一种spider中是相同的）。而调用pop时，其实就是从redis用那个特定的key去读其值（一个list），从list中读取最早进去的那个，于是就先进先出了。 这些容器类都会作为scheduler调度request的容器，scheduler在每个主机上都会实例化一个，并且和spider一一对应，所以分布式运行时会有一个spider的多个实例和一个scheduler的多个实例存在于不同的主机上，但是，因为scheduler都是用相同的容器，而这些容器都连接同一个redis服务器，又都使用spider名加queue来作为key读写数据，所以不同主机上的不同爬虫实例公用一个request调度池，实现了分布式爬虫之间的统一调度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.utils.reqser import request_to_dict, request_from_dict</span><br><span class="line"></span><br><span class="line">from . import picklecompat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Base(object):</span><br><span class="line">    &quot;&quot;&quot;Per-spider queue/stack base class&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, server, spider, key, serializer=None):</span><br><span class="line">        &quot;&quot;&quot;Initialize per-spider redis queue.</span><br><span class="line">        Parameters:</span><br><span class="line">            server -- redis connection</span><br><span class="line">            spider -- spider instance</span><br><span class="line">            key -- key for this queue (e.g. &quot;%(spider)s:queue&quot;)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if serializer is None:</span><br><span class="line">            # Backward compatibility.</span><br><span class="line">            # TODO: deprecate pickle.</span><br><span class="line">            serializer = picklecompat</span><br><span class="line">        if not hasattr(serializer, &apos;loads&apos;):</span><br><span class="line">            raise TypeError(&quot;serializer does not implement &apos;loads&apos; function: %r&quot;</span><br><span class="line">                            % serializer)</span><br><span class="line">        if not hasattr(serializer, &apos;dumps&apos;):</span><br><span class="line">            raise TypeError(&quot;serializer &apos;%s&apos; does not implement &apos;dumps&apos; function: %r&quot;</span><br><span class="line">                            % serializer)</span><br><span class="line"></span><br><span class="line">        self.server = server</span><br><span class="line">        self.spider = spider</span><br><span class="line">        self.key = key % &#123;&apos;spider&apos;: spider.name&#125;</span><br><span class="line">        self.serializer = serializer</span><br><span class="line"></span><br><span class="line">    def _encode_request(self, request):</span><br><span class="line">        &quot;&quot;&quot;Encode a request object&quot;&quot;&quot;</span><br><span class="line">        obj = request_to_dict(request, self.spider)</span><br><span class="line">        return self.serializer.dumps(obj)</span><br><span class="line"></span><br><span class="line">    def _decode_request(self, encoded_request):</span><br><span class="line">        &quot;&quot;&quot;Decode an request previously encoded&quot;&quot;&quot;</span><br><span class="line">        obj = self.serializer.loads(encoded_request)</span><br><span class="line">        return request_from_dict(obj, self.spider)</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        &quot;&quot;&quot;Return the length of the queue&quot;&quot;&quot;</span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    def push(self, request):</span><br><span class="line">        &quot;&quot;&quot;Push a request&quot;&quot;&quot;</span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    def pop(self, timeout=0):</span><br><span class="line">        &quot;&quot;&quot;Pop a request&quot;&quot;&quot;</span><br><span class="line">        raise NotImplementedError</span><br><span class="line"></span><br><span class="line">    def clear(self):</span><br><span class="line">        &quot;&quot;&quot;Clear queue/stack&quot;&quot;&quot;</span><br><span class="line">        self.server.delete(self.key)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SpiderQueue(Base):</span><br><span class="line">    &quot;&quot;&quot;Per-spider FIFO queue&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        &quot;&quot;&quot;Return the length of the queue&quot;&quot;&quot;</span><br><span class="line">        return self.server.llen(self.key)</span><br><span class="line"></span><br><span class="line">    def push(self, request):</span><br><span class="line">        &quot;&quot;&quot;Push a request&quot;&quot;&quot;</span><br><span class="line">        self.server.lpush(self.key, self._encode_request(request))</span><br><span class="line"></span><br><span class="line">    def pop(self, timeout=0):</span><br><span class="line">        &quot;&quot;&quot;Pop a request&quot;&quot;&quot;</span><br><span class="line">        if timeout &gt; 0:</span><br><span class="line">            data = self.server.brpop(self.key, timeout)</span><br><span class="line">            if isinstance(data, tuple):</span><br><span class="line">                data = data[1]</span><br><span class="line">        else:</span><br><span class="line">            data = self.server.rpop(self.key)</span><br><span class="line">        if data:</span><br><span class="line">            return self._decode_request(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SpiderPriorityQueue(Base):</span><br><span class="line">    &quot;&quot;&quot;Per-spider priority queue abstraction using redis&apos; sorted set&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        &quot;&quot;&quot;Return the length of the queue&quot;&quot;&quot;</span><br><span class="line">        return self.server.zcard(self.key)</span><br><span class="line"></span><br><span class="line">    def push(self, request):</span><br><span class="line">        &quot;&quot;&quot;Push a request&quot;&quot;&quot;</span><br><span class="line">        data = self._encode_request(request)</span><br><span class="line">        score = -request.priority</span><br><span class="line">        # We don&apos;t use zadd method as the order of arguments change depending on</span><br><span class="line">        # whether the class is Redis or StrictRedis, and the option of using</span><br><span class="line">        # kwargs only accepts strings, not bytes.</span><br><span class="line">        self.server.execute_command(&apos;ZADD&apos;, self.key, score, data)</span><br><span class="line"></span><br><span class="line">    def pop(self, timeout=0):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Pop a request</span><br><span class="line">        timeout not support in this queue class</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # use atomic range/remove using multi/exec</span><br><span class="line">        pipe = self.server.pipeline()</span><br><span class="line">        pipe.multi()</span><br><span class="line">        pipe.zrange(self.key, 0, 0).zremrangebyrank(self.key, 0, 0)</span><br><span class="line">        results, count = pipe.execute()</span><br><span class="line">        if results:</span><br><span class="line">            return self._decode_request(results[0])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class SpiderStack(Base):</span><br><span class="line">    &quot;&quot;&quot;Per-spider stack&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        &quot;&quot;&quot;Return the length of the stack&quot;&quot;&quot;</span><br><span class="line">        return self.server.llen(self.key)</span><br><span class="line"></span><br><span class="line">    def push(self, request):</span><br><span class="line">        &quot;&quot;&quot;Push a request&quot;&quot;&quot;</span><br><span class="line">        self.server.lpush(self.key, self._encode_request(request))</span><br><span class="line"></span><br><span class="line">    def pop(self, timeout=0):</span><br><span class="line">        &quot;&quot;&quot;Pop a request&quot;&quot;&quot;</span><br><span class="line">        if timeout &gt; 0:</span><br><span class="line">            data = self.server.blpop(self.key, timeout)</span><br><span class="line">            if isinstance(data, tuple):</span><br><span class="line">                data = data[1]</span><br><span class="line">        else:</span><br><span class="line">            data = self.server.lpop(self.key)</span><br><span class="line"></span><br><span class="line">        if data:</span><br><span class="line">            return self._decode_request(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__all__ = [&apos;SpiderQueue&apos;, &apos;SpiderPriorityQueue&apos;, &apos;SpiderStack&apos;]</span><br></pre></td></tr></table></figure>
<h2 id="6-scheduler-py"><a href="#6-scheduler-py" class="headerlink" title="6. scheduler.py"></a>6. scheduler.py</h2><p>此扩展是对scrapy中自带的scheduler的替代（在settings的SCHEDULER变量中指出），正是利用此扩展实现crawler的分布式调度。其利用的数据结构来自于queue中实现的数据结构。</p>
<p>scrapy-redis所实现的两种分布式：爬虫分布式以及item处理分布式就是由模块scheduler和模块pipelines实现。上述其它模块作为为二者辅助的功能模块<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line">import importlib</span><br><span class="line">import six</span><br><span class="line"></span><br><span class="line">from scrapy.utils.misc import load_object</span><br><span class="line"></span><br><span class="line">from . import connection</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># TODO: add SCRAPY_JOB support.</span><br><span class="line">class Scheduler(object):</span><br><span class="line">    &quot;&quot;&quot;Redis-based scheduler&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, server,</span><br><span class="line">                 persist=False,</span><br><span class="line">                 flush_on_start=False,</span><br><span class="line">                 queue_key=&apos;%(spider)s:requests&apos;,</span><br><span class="line">                 queue_cls=&apos;scrapy_redis.queue.SpiderPriorityQueue&apos;,</span><br><span class="line">                 dupefilter_key=&apos;%(spider)s:dupefilter&apos;,</span><br><span class="line">                 dupefilter_cls=&apos;scrapy_redis.dupefilter.RFPDupeFilter&apos;,</span><br><span class="line">                 idle_before_close=0,</span><br><span class="line">                 serializer=None):</span><br><span class="line">        &quot;&quot;&quot;Initialize scheduler.</span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        server : Redis</span><br><span class="line">            The redis server instance.</span><br><span class="line">        persist : bool</span><br><span class="line">            Whether to flush requests when closing. Default is False.</span><br><span class="line">        flush_on_start : bool</span><br><span class="line">            Whether to flush requests on start. Default is False.</span><br><span class="line">        queue_key : str</span><br><span class="line">            Requests queue key.</span><br><span class="line">        queue_cls : str</span><br><span class="line">            Importable path to the queue class.</span><br><span class="line">        dupefilter_key : str</span><br><span class="line">            Duplicates filter key.</span><br><span class="line">        dupefilter_cls : str</span><br><span class="line">            Importable path to the dupefilter class.</span><br><span class="line">        idle_before_close : int</span><br><span class="line">            Timeout before giving up.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if idle_before_close &lt; 0:</span><br><span class="line">            raise TypeError(&quot;idle_before_close cannot be negative&quot;)</span><br><span class="line"></span><br><span class="line">        self.server = server</span><br><span class="line">        self.persist = persist</span><br><span class="line">        self.flush_on_start = flush_on_start</span><br><span class="line">        self.queue_key = queue_key</span><br><span class="line">        self.queue_cls = queue_cls</span><br><span class="line">        self.dupefilter_cls = dupefilter_cls</span><br><span class="line">        self.dupefilter_key = dupefilter_key</span><br><span class="line">        self.idle_before_close = idle_before_close</span><br><span class="line">        self.serializer = serializer</span><br><span class="line">        self.stats = None</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.queue)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_settings(cls, settings):</span><br><span class="line">        kwargs = &#123;</span><br><span class="line">            &apos;persist&apos;: settings.getbool(&apos;SCHEDULER_PERSIST&apos;),</span><br><span class="line">            &apos;flush_on_start&apos;: settings.getbool(&apos;SCHEDULER_FLUSH_ON_START&apos;),</span><br><span class="line">            &apos;idle_before_close&apos;: settings.getint(&apos;SCHEDULER_IDLE_BEFORE_CLOSE&apos;),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        # If these values are missing, it means we want to use the defaults.</span><br><span class="line">        optional = &#123;</span><br><span class="line">            # TODO: Use custom prefixes for this settings to note that are</span><br><span class="line">            # specific to scrapy-redis.</span><br><span class="line">            &apos;queue_key&apos;: &apos;SCHEDULER_QUEUE_KEY&apos;,</span><br><span class="line">            &apos;queue_cls&apos;: &apos;SCHEDULER_QUEUE_CLASS&apos;,</span><br><span class="line">            &apos;dupefilter_key&apos;: &apos;SCHEDULER_DUPEFILTER_KEY&apos;,</span><br><span class="line">            # We use the default setting name to keep compatibility.</span><br><span class="line">            &apos;dupefilter_cls&apos;: &apos;DUPEFILTER_CLASS&apos;,</span><br><span class="line">            &apos;serializer&apos;: &apos;SCHEDULER_SERIALIZER&apos;,</span><br><span class="line">        &#125;</span><br><span class="line">        for name, setting_name in optional.items():</span><br><span class="line">            val = settings.get(setting_name)</span><br><span class="line">            if val:</span><br><span class="line">                kwargs[name] = val</span><br><span class="line"></span><br><span class="line">        # Support serializer as a path to a module.</span><br><span class="line">        if isinstance(kwargs.get(&apos;serializer&apos;), six.string_types):</span><br><span class="line">            kwargs[&apos;serializer&apos;] = importlib.import_module(kwargs[&apos;serializer&apos;])</span><br><span class="line"></span><br><span class="line">        server = connection.from_settings(settings)</span><br><span class="line">        # Ensure the connection is working.</span><br><span class="line">        server.ping()</span><br><span class="line"></span><br><span class="line">        return cls(server=server, **kwargs)</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls, crawler):</span><br><span class="line">        instance = cls.from_settings(crawler.settings)</span><br><span class="line">        # FIXME: for now, stats are only supported from this constructor</span><br><span class="line">        instance.stats = crawler.stats</span><br><span class="line">        return instance</span><br><span class="line"></span><br><span class="line">    def open(self, spider):</span><br><span class="line">        self.spider = spider</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            self.queue = load_object(self.queue_cls)(</span><br><span class="line">                server=self.server,</span><br><span class="line">                spider=spider,</span><br><span class="line">                key=self.queue_key % &#123;&apos;spider&apos;: spider.name&#125;,</span><br><span class="line">                serializer=self.serializer,</span><br><span class="line">            )</span><br><span class="line">        except TypeError as e:</span><br><span class="line">            raise ValueError(&quot;Failed to instantiate queue class &apos;%s&apos;: %s&quot;,</span><br><span class="line">                             self.queue_cls, e)</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            self.df = load_object(self.dupefilter_cls)(</span><br><span class="line">                server=self.server,</span><br><span class="line">                key=self.dupefilter_key % &#123;&apos;spider&apos;: spider.name&#125;,</span><br><span class="line">                debug=spider.settings.getbool(&apos;DUPEFILTER_DEBUG&apos;),</span><br><span class="line">            )</span><br><span class="line">        except TypeError as e:</span><br><span class="line">            raise ValueError(&quot;Failed to instantiate dupefilter class &apos;%s&apos;: %s&quot;,</span><br><span class="line">                             self.dupefilter_cls, e)</span><br><span class="line"></span><br><span class="line">        if self.flush_on_start:</span><br><span class="line">            self.flush()</span><br><span class="line">        # notice if there are requests already in the queue to resume the crawl</span><br><span class="line">        if len(self.queue):</span><br><span class="line">            spider.log(&quot;Resuming crawl (%d requests scheduled)&quot; % len(self.queue))</span><br><span class="line"></span><br><span class="line">    def close(self, reason):</span><br><span class="line">        if not self.persist:</span><br><span class="line">            self.flush()</span><br><span class="line"></span><br><span class="line">    def flush(self):</span><br><span class="line">        self.df.clear()</span><br><span class="line">        self.queue.clear()</span><br><span class="line"></span><br><span class="line">    def enqueue_request(self, request):</span><br><span class="line">        if not request.dont_filter and self.df.request_seen(request):</span><br><span class="line">            self.df.log(request, self.spider)</span><br><span class="line">            return False</span><br><span class="line">        if self.stats:</span><br><span class="line">            self.stats.inc_value(&apos;scheduler/enqueued/redis&apos;, spider=self.spider)</span><br><span class="line">        self.queue.push(request)</span><br><span class="line">        return True</span><br><span class="line"></span><br><span class="line">    def next_request(self):</span><br><span class="line">        block_pop_timeout = self.idle_before_close</span><br><span class="line">        request = self.queue.pop(block_pop_timeout)</span><br><span class="line">        if request and self.stats:</span><br><span class="line">            self.stats.inc_value(&apos;scheduler/dequeued/redis&apos;, spider=self.spider)</span><br><span class="line">        return request</span><br><span class="line"></span><br><span class="line">    def has_pending_requests(self):</span><br><span class="line">        return len(self) &gt; 0</span><br></pre></td></tr></table></figure></p>
<p>这个文件重写了scheduler类，用来代替scrapy.core.scheduler的原有调度器。其实对原有调度器的逻辑没有很大的改变，主要是使用了redis作为数据存储的媒介，以达到各个爬虫之间的统一调度。 scheduler负责调度各个spider的request请求，scheduler初始化时，通过settings文件读取queue和dupefilters的类型（一般就用上边默认的），配置queue和dupefilters使用的key（一般就是spider name加上queue或者dupefilters，这样对于同一种spider的不同实例，就会使用相同的数据块了）。每当一个request要被调度时，enqueue_request被调用，scheduler使用dupefilters来判断这个url是否重复，如果不重复，就添加到queue的容器中（先进先出，先进后出和优先级都可以，可以在settings中配置）。当调度完成时，next_request被调用，scheduler就通过queue容器的接口，取出一个request，把他发送给相应的spider，让spider进行爬取工作。</p>
<h2 id="7-spider-py"><a href="#7-spider-py" class="headerlink" title="7.spider.py"></a>7.spider.py</h2><p>设计的这个spider从redis中读取要爬的url,然后执行爬取，若爬取过程中返回更多的url，那么继续进行直至所有的request完成。之后继续从redis中读取url，循环这个过程。</p>
<p>分析：在这个spider中通过connect signals.spider_idle信号实现对crawler状态的监视。当idle时，返回新的make_requests_from_url(url)给引擎，进而交给调度器调度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">from scrapy import signals</span><br><span class="line">from scrapy.exceptions import DontCloseSpider</span><br><span class="line">from scrapy.spiders import Spider, CrawlSpider</span><br><span class="line"></span><br><span class="line">from . import connection</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Default batch size matches default concurrent requests setting.</span><br><span class="line">DEFAULT_START_URLS_BATCH_SIZE = 16</span><br><span class="line">DEFAULT_START_URLS_KEY = &apos;%(name)s:start_urls&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RedisMixin(object):</span><br><span class="line">    &quot;&quot;&quot;Mixin class to implement reading urls from a redis queue.&quot;&quot;&quot;</span><br><span class="line">    # Per spider redis key, default to DEFAULT_START_URLS_KEY.</span><br><span class="line">    redis_key = None</span><br><span class="line">    # Fetch this amount of start urls when idle. Default to DEFAULT_START_URLS_BATCH_SIZE.</span><br><span class="line">    redis_batch_size = None</span><br><span class="line">    # Redis client instance.</span><br><span class="line">    server = None</span><br><span class="line"></span><br><span class="line">    def start_requests(self):</span><br><span class="line">        &quot;&quot;&quot;Returns a batch of start requests from redis.&quot;&quot;&quot;</span><br><span class="line">        return self.next_requests()</span><br><span class="line"></span><br><span class="line">    def setup_redis(self, crawler=None):</span><br><span class="line">        &quot;&quot;&quot;Setup redis connection and idle signal.</span><br><span class="line">        This should be called after the spider has set its crawler object.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if self.server is not None:</span><br><span class="line">            return</span><br><span class="line"></span><br><span class="line">        if crawler is None:</span><br><span class="line">            # We allow optional crawler argument to keep backwards</span><br><span class="line">            # compatibility.</span><br><span class="line">            # XXX: Raise a deprecation warning.</span><br><span class="line">            crawler = getattr(self, &apos;crawler&apos;, None)</span><br><span class="line"></span><br><span class="line">        if crawler is None:</span><br><span class="line">            raise ValueError(&quot;crawler is required&quot;)</span><br><span class="line"></span><br><span class="line">        settings = crawler.settings</span><br><span class="line"></span><br><span class="line">        if self.redis_key is None:</span><br><span class="line">            self.redis_key = settings.get(</span><br><span class="line">                &apos;REDIS_START_URLS_KEY&apos;, DEFAULT_START_URLS_KEY,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.redis_key = self.redis_key % &#123;&apos;name&apos;: self.name&#125;</span><br><span class="line"></span><br><span class="line">        if not self.redis_key.strip():</span><br><span class="line">            raise ValueError(&quot;redis_key must not be empty&quot;)</span><br><span class="line"></span><br><span class="line">        if self.redis_batch_size is None:</span><br><span class="line">            self.redis_batch_size = settings.getint(</span><br><span class="line">                &apos;REDIS_START_URLS_BATCH_SIZE&apos;, DEFAULT_START_URLS_BATCH_SIZE,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        try:</span><br><span class="line">            self.redis_batch_size = int(self.redis_batch_size)</span><br><span class="line">        except (TypeError, ValueError):</span><br><span class="line">            raise ValueError(&quot;redis_batch_size must be an integer&quot;)</span><br><span class="line"></span><br><span class="line">        self.logger.info(&quot;Reading start URLs from redis key &apos;%(redis_key)s&apos; &quot;</span><br><span class="line">                         &quot;(batch size: %(redis_batch_size)s)&quot;, self.__dict__)</span><br><span class="line"></span><br><span class="line">        self.server = connection.from_settings(crawler.settings)</span><br><span class="line">        # The idle signal is called when the spider has no requests left,</span><br><span class="line">        # that&apos;s when we will schedule new requests from redis queue</span><br><span class="line">        crawler.signals.connect(self.spider_idle, signal=signals.spider_idle)</span><br><span class="line"></span><br><span class="line">    def next_requests(self):</span><br><span class="line">        &quot;&quot;&quot;Returns a request to be scheduled or none.&quot;&quot;&quot;</span><br><span class="line">        use_set = self.settings.getbool(&apos;REDIS_START_URLS_AS_SET&apos;)</span><br><span class="line">        fetch_one = self.server.spop if use_set else self.server.lpop</span><br><span class="line">        # XXX: Do we need to use a timeout here?</span><br><span class="line">        found = 0</span><br><span class="line">        while found &lt; self.redis_batch_size:</span><br><span class="line">            data = fetch_one(self.redis_key)</span><br><span class="line">            if not data:</span><br><span class="line">                # Queue empty.</span><br><span class="line">                break</span><br><span class="line">            req = self.make_request_from_data(data)</span><br><span class="line">            if req:</span><br><span class="line">                yield req</span><br><span class="line">                found += 1</span><br><span class="line">            else:</span><br><span class="line">                self.logger.debug(&quot;Request not made from data: %r&quot;, data)</span><br><span class="line"></span><br><span class="line">        if found:</span><br><span class="line">            self.logger.debug(&quot;Read %s requests from &apos;%s&apos;&quot;, found, self.redis_key)</span><br><span class="line"></span><br><span class="line">    def make_request_from_data(self, data):</span><br><span class="line">        # By default, data is an URL.</span><br><span class="line">        if &apos;://&apos; in data:</span><br><span class="line">            return self.make_requests_from_url(data)</span><br><span class="line">        else:</span><br><span class="line">            self.logger.error(&quot;Unexpected URL from &apos;%s&apos;: %r&quot;, self.redis_key, data)</span><br><span class="line"></span><br><span class="line">    def schedule_next_requests(self):</span><br><span class="line">        &quot;&quot;&quot;Schedules a request if available&quot;&quot;&quot;</span><br><span class="line">        for req in self.next_requests():</span><br><span class="line">            self.crawler.engine.crawl(req, spider=self)</span><br><span class="line"></span><br><span class="line">    def spider_idle(self):</span><br><span class="line">        &quot;&quot;&quot;Schedules a request if available, otherwise waits.&quot;&quot;&quot;</span><br><span class="line">        # XXX: Handle a sentinel to close the spider.</span><br><span class="line">        self.schedule_next_requests()</span><br><span class="line">        raise DontCloseSpider</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RedisSpider(RedisMixin, Spider):</span><br><span class="line">    &quot;&quot;&quot;Spider that reads urls from redis queue when idle.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(self, crawler, *args, **kwargs):</span><br><span class="line">        obj = super(RedisSpider, self).from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        obj.setup_redis(crawler)</span><br><span class="line">        return obj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RedisCrawlSpider(RedisMixin, CrawlSpider):</span><br><span class="line">    &quot;&quot;&quot;Spider that reads urls from redis queue when idle.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(self, crawler, *args, **kwargs):</span><br><span class="line">        obj = super(RedisCrawlSpider, self).from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        obj.setup_redis(crawler)</span><br><span class="line">        return obj</span><br></pre></td></tr></table></figure></p>
<p>spider的改动也不是很大，主要是通过connect接口，给spider绑定了spider_idle信号，spider初始化时，通过setup_redis函数初始化好和redis的连接，之后通过next_requests函数从redis中取出strat url，使用的key是settings中REDIS_START_URLS_AS_SET定义的（注意了这里的初始化url池和我们上边的queue的url池不是一个东西，queue的池是用于调度的，初始化url池是存放入口url的，他们都存在redis中，但是使用不同的key来区分，就当成是不同的表吧），spider使用少量的start url，可以发展出很多新的url，这些url会进入scheduler进行判重和调度。直到spider跑到调度池内没有url的时候，会触发spider_idle信号，从而触发spider的next_requests函数，再次从redis的start url池中读取一些url。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>最后总结一下scrapy-redis的总体思路：这个工程通过重写scheduler和spider类，实现了调度、spider启动和redis的交互。实现新的dupefilter和queue类，达到了判重和调度容器和redis的交互，因为每个主机上的爬虫进程都访问同一个redis数据库，所以调度和判重都统一进行统一管理，达到了分布式爬虫的目的。 当spider被初始化时，同时会初始化一个对应的scheduler对象，这个调度器对象通过读取settings，配置好自己的调度容器queue和判重工具dupefilter。每当一个spider产出一个request的时候，scrapy内核会把这个reuqest递交给这个spider对应的scheduler对象进行调度，scheduler对象通过访问redis对request进行判重，如果不重复就把他添加进redis中的调度池。当调度条件满足时，scheduler对象就从redis的调度池中取出一个request发送给spider，让他爬取。当spider爬取的所有暂时可用url之后，scheduler发现这个spider对应的redis的调度池空了，于是触发信号spider_idle，spider收到这个信号之后，直接连接redis读取strart url池，拿去新的一批url入口，然后再次重复上边的工作。</p>

            <div class="post-copyright">
    <div class="content">
        <p>最后更新： 2019年05月18日 20:51</p>
        <p>原始链接： <a class="post-url" href="/2019/05/18/Scrapy-redis 源码分析/" title="Scrapy-redis 源码分析">https://zem12345678.github.io/2019/05/18/Scrapy-redis 源码分析/</a></p>
        <footer>
            <a href="https://zem12345678.github.io">
                <img src="/images/logo.png" alt="Enmin">
                Enmin
            </a>
        </footer>
    </div>
</div>

      
        
            
<div class="page-reward">
    <a id="rewardBtn" href="javascript:;">赏</a>
</div>

<div id="reward" class="post-modal reward-lay">
    <a class="close" href="javascript:;" id="reward-close">×</a>
    <span class="reward-title">
        <i class="icon icon-quote-left"></i>
        请我吃糖~
        <i class="icon icon-quote-right"></i>
    </span>
    <div class="reward-content">
        
        <div class="reward-code">
            <img id="rewardCode" src="/images/wx.png" alt="打赏二维码">
        </div>
        <div class="reward-select">
            
            <label class="reward-select-item checked" data-id="wechat" data-wechat="/images/wx.png">
                <img class="reward-select-item-wechat" src="/images/wechat.png" alt="微信">
            </label>
            
            
            <label class="reward-select-item" data-id="alipay" data-alipay="/images/zfb.png">
                <img class="reward-select-item-alipay" src="/images/alipay.png" alt="支付宝">
            </label>
            
        </div>
    </div>
</div>


        
    </div>
    <footer class="article-footer">
        
        
<div class="post-share">
    <a href="javascript:;" id="share-sub" class="post-share-fab">
        <i class="fa fa-share-alt"></i>
    </a>
    <div class="post-share-list" id="share-list">
        <ul class="share-icons">
          <li>
            <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=https://zem12345678.github.io/2019/05/18/Scrapy-redis 源码分析/&title=《Scrapy-redis 源码分析》 — Enmin`s blog&pic=images/30.jpg" data-title="微博">
              <i class="fa fa-weibo"></i>
            </a>
          </li>
          <li>
            <a class="weixin share-sns" id="wxFab" href="javascript:;" data-title="微信">
              <i class="fa fa-weixin"></i>
            </a>
          </li>
          <li>
            <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=https://zem12345678.github.io/2019/05/18/Scrapy-redis 源码分析/&title=《Scrapy-redis 源码分析》 — Enmin`s blog&source=" data-title="QQ">
              <i class="fa fa-qq"></i>
            </a>
          </li>
          <li>
            <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://zem12345678.github.io/2019/05/18/Scrapy-redis 源码分析/" data-title="Facebook">
              <i class="fa fa-facebook"></i>
            </a>
          </li>
          <li>
            <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《Scrapy-redis 源码分析》 — Enmin`s blog&url=https://zem12345678.github.io/2019/05/18/Scrapy-redis 源码分析/&via=https://zem12345678.github.io" data-title="Twitter">
              <i class="fa fa-twitter"></i>
            </a>
          </li>
          <li>
            <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=https://zem12345678.github.io/2019/05/18/Scrapy-redis 源码分析/" data-title="Google+">
              <i class="fa fa-google-plus"></i>
            </a>
          </li>
        </ul>
     </div>
</div>
<div class="post-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;" id="wxShare-close">×</a>
    <p>扫一扫，分享到微信</p>
    <img src="//api.qrserver.com/v1/create-qr-code/?data=https://zem12345678.github.io/2019/05/18/Scrapy-redis 源码分析/" alt="微信分享二维码">
</div>

<div class="mask"></div>

        
        <ul class="article-footer-menu">
            
            
  <li class="article-footer-tags">
    <i class="fa fa-tags"></i>
      
    <a href="/tags/爬虫/" class="color3">爬虫</a>
      
    <a href="/tags/Scrapy/" class="color2">Scrapy</a>
      
  </li>

        </ul>
        
    </footer>
  </div>
</article>


    <aside class="post-toc-pos post-toc-top" id="post-toc">
        <nav class="post-toc-wrap">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Scrapy-redis-源码分析"><span class="post-toc-text">Scrapy-redis 源码分析</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#1-connection-py"><span class="post-toc-text">1. connection.py</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#2-dupefilter-py"><span class="post-toc-text">2. dupefilter.py</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#3-picklecompat-py"><span class="post-toc-text">3. picklecompat.py</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-pipelines-py"><span class="post-toc-text">4.pipelines.py</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-queue-py"><span class="post-toc-text">5.queue.py</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-scheduler-py"><span class="post-toc-text">6. scheduler.py</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-spider-py"><span class="post-toc-text">7.spider.py</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#总结"><span class="post-toc-text">总结</span></a></li></ol>
        </nav>
    </aside>
    

<nav id="article-nav">
  
    <a href="/2019/05/18/三种scrapy模拟登陆策略/" id="article-nav-newer" class="article-nav-link-wrap">

      <span class="article-nav-title">
        <i class="fa fa-hand-o-left" aria-hidden="true"></i>
        
          三种scrapy模拟登陆策略
        
      </span>
    </a>
  
  
    <a href="/2019/05/17/Scrapy深入/" id="article-nav-older" class="article-nav-link-wrap">
      <span class="article-nav-title">Scrapy 深入</span>
      <i class="fa fa-hand-o-right" aria-hidden="true"></i>
    </a>
  
</nav>



    
        <div id="SOHUCS" sid="Scrapy-redis 源码分析" ></div>
<script type="text/javascript">
    (function(){
        var appid = 'cytRXLIlH';
        var conf = '983bf9e4c3b1e70e33660721bdb62bb2';
        var width = window.innerWidth || document.documentElement.clientWidth;
        if (width < 960) {
            window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>
    
</section>
        
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：20<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：20<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


      <p>
        Powered by  <a href="http://hexo.io/" target="_blank">Hexo</a>
        Theme <a href="//github.com/wongminho/hexo-theme-miho" target="_blank">MiHo</a>
      &copy; 2019 Enmin<br>
      </p>
    </div>
  </div>
</footer>
    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script>
  var mihoConfig = {
      root: "https://zem12345678.github.io",
      animate: true,
      isHome: false,
      share: true,
      reward: 1
  }
</script>
<div class="sidebar">
    <div id="sidebar-search" title="Search">
        <i class="fa fa-search"></i>
    </div>
    <div id="sidebar-category" title="Categories">
        <i class="fa fa-book"></i>
    </div>
    <div id="sidebar-tag" title="Tags">
        <i class="fa fa-tags"></i>
    </div>
    <div id="sidebar-top">
        <span class="sidebar-top-icon"><i class="fa fa-angle-up"></i></span>
    </div>
</div>
<div class="sidebar-menu-box" id="sidebar-menu-box">
    <div class="sidebar-menu-box-container">
        <div id="sidebar-menu-box-categories">
            <a class="category-link" href="/categories/Django/">Django</a><a class="category-link" href="/categories/Docker/">Docker</a><a class="category-link" href="/categories/FastDFS/">FastDFS</a><a class="category-link" href="/categories/Flask/">Flask</a><a class="category-link" href="/categories/JWT/">JWT</a><a class="category-link" href="/categories/Machine-learning/">Machine learning</a><a class="category-link" href="/categories/Python/">Python</a><a class="category-link" href="/categories/Redis/">Redis</a><a class="category-link" href="/categories/django/">django</a><a class="category-link" href="/categories/前后端分离/">前后端分离</a><a class="category-link" href="/categories/前端/">前端</a><a class="category-link" href="/categories/大数据/">大数据</a><a class="category-link" href="/categories/学习笔记/">学习笔记</a><a class="category-link" href="/categories/并行计算/">并行计算</a><a class="category-link" href="/categories/异步处理/">异步处理</a><a class="category-link" href="/categories/数据库/">数据库</a><a class="category-link" href="/categories/数据挖掘/">数据挖掘</a><a class="category-link" href="/categories/杂谈/">杂谈</a><a class="category-link" href="/categories/算法/">算法</a><a class="category-link" href="/categories/网络原理/">网络原理</a><a class="category-link" href="/categories/随堂笔记/">随堂笔记</a>
        </div>
        <div id="sidebar-menu-box-tags">
            <a href="/tags/BeautifuSoup4/" style="font-size: 10px;">BeautifuSoup4</a> <a href="/tags/BeautifulSoup4/" style="font-size: 10px;">BeautifulSoup4</a> <a href="/tags/C-C/" style="font-size: 10px;">C/C++</a> <a href="/tags/Django/" style="font-size: 18.75px;">Django</a> <a href="/tags/Docker/" style="font-size: 12.5px;">Docker</a> <a href="/tags/ElasticSearch/" style="font-size: 11.25px;">ElasticSearch</a> <a href="/tags/FastDFS/" style="font-size: 12.5px;">FastDFS</a> <a href="/tags/Flask/" style="font-size: 12.5px;">Flask</a> <a href="/tags/Gunicorn/" style="font-size: 10px;">Gunicorn</a> <a href="/tags/HTTP-HTTPS/" style="font-size: 11.25px;">HTTP/HTTPS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 11.25px;">Java</a> <a href="/tags/Jwt/" style="font-size: 11.25px;">Jwt</a> <a href="/tags/LeetCode/" style="font-size: 10px;">LeetCode</a> <a href="/tags/LeetCood/" style="font-size: 11.25px;">LeetCood</a> <a href="/tags/Machine-learning/" style="font-size: 12.5px;">Machine learning</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/Mpi/" style="font-size: 10px;">Mpi</a> <a href="/tags/Mysql/" style="font-size: 11.25px;">Mysql</a> <a href="/tags/Nginx/" style="font-size: 11.25px;">Nginx</a> <a href="/tags/Python/" style="font-size: 17.5px;">Python</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/tags/Requests/" style="font-size: 10px;">Requests</a> <a href="/tags/Restful/" style="font-size: 12.5px;">Restful</a> <a href="/tags/Scrapy/" style="font-size: 16.25px;">Scrapy</a> <a href="/tags/Spring/" style="font-size: 10px;">Spring</a> <a href="/tags/SpringBoot/" style="font-size: 10px;">SpringBoot</a> <a href="/tags/SpringCloud/" style="font-size: 10px;">SpringCloud</a> <a href="/tags/Token/" style="font-size: 11.25px;">Token</a> <a href="/tags/Urllib/" style="font-size: 10px;">Urllib</a> <a href="/tags/Vue/" style="font-size: 11.25px;">Vue</a> <a href="/tags/Web/" style="font-size: 15px;">Web</a> <a href="/tags/XPath/" style="font-size: 11.25px;">XPath</a> <a href="/tags/python/" style="font-size: 11.25px;">python</a> <a href="/tags/决策树分类/" style="font-size: 10px;">决策树分类</a> <a href="/tags/分布式/" style="font-size: 12.5px;">分布式</a> <a href="/tags/前后端分离/" style="font-size: 13.75px;">前后端分离</a> <a href="/tags/前端/" style="font-size: 10px;">前端</a> <a href="/tags/回归分类/" style="font-size: 10px;">回归分类</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/容器/" style="font-size: 11.25px;">容器</a> <a href="/tags/富文本/" style="font-size: 10px;">富文本</a> <a href="/tags/微服务/" style="font-size: 10px;">微服务</a> <a href="/tags/搜索引擎/" style="font-size: 11.25px;">搜索引擎</a> <a href="/tags/数据库/" style="font-size: 10px;">数据库</a> <a href="/tags/数据库集群/" style="font-size: 10px;">数据库集群</a> <a href="/tags/数据结构/" style="font-size: 12.5px;">数据结构</a> <a href="/tags/杂谈/" style="font-size: 10px;">杂谈</a> <a href="/tags/权限认证/" style="font-size: 10px;">权限认证</a> <a href="/tags/正则表达式/" style="font-size: 11.25px;">正则表达式</a> <a href="/tags/消息队列/" style="font-size: 10px;">消息队列</a> <a href="/tags/爬虫/" style="font-size: 20px;">爬虫</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/编码/" style="font-size: 11.25px;">编码</a> <a href="/tags/虚拟化/" style="font-size: 11.25px;">虚拟化</a> <a href="/tags/跨域/" style="font-size: 10px;">跨域</a> <a href="/tags/随机森林/" style="font-size: 10px;">随机森林</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
        </div>
    </div>
    <a href="javascript:;" class="sidebar-menu-box-close">&times;</a>
</div>
<div class="mobile-header-menu-nav" id="mobile-header-menu-nav">
    <div class="mobile-header-menu-container">
        <span class="title">Menus</span>
        <ul class="mobile-header-menu-navbar">
            
            <li>
                <a  href="/">
                    <i class="fa fa-home"></i><span>主页</span>
                </a>
            </li>
            
            <li>
                <a  href="/archives">
                    <i class="fa fa-archive"></i><span>文章</span>
                </a>
            </li>
            
            <li>
                <a  href="/about">
                    <i class="fa fa-user"></i><span>关于我</span>
                </a>
            </li>
            
        </ul>
    </div>
    <div class="mobile-header-tag-container">
        <span class="title">Tags</span>
        <div id="mobile-header-container-tags">
            <a href="/tags/BeautifuSoup4/" style="font-size: 10px;">BeautifuSoup4</a> <a href="/tags/BeautifulSoup4/" style="font-size: 10px;">BeautifulSoup4</a> <a href="/tags/C-C/" style="font-size: 10px;">C/C++</a> <a href="/tags/Django/" style="font-size: 18.75px;">Django</a> <a href="/tags/Docker/" style="font-size: 12.5px;">Docker</a> <a href="/tags/ElasticSearch/" style="font-size: 11.25px;">ElasticSearch</a> <a href="/tags/FastDFS/" style="font-size: 12.5px;">FastDFS</a> <a href="/tags/Flask/" style="font-size: 12.5px;">Flask</a> <a href="/tags/Gunicorn/" style="font-size: 10px;">Gunicorn</a> <a href="/tags/HTTP-HTTPS/" style="font-size: 11.25px;">HTTP/HTTPS</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 11.25px;">Java</a> <a href="/tags/Jwt/" style="font-size: 11.25px;">Jwt</a> <a href="/tags/LeetCode/" style="font-size: 10px;">LeetCode</a> <a href="/tags/LeetCood/" style="font-size: 11.25px;">LeetCood</a> <a href="/tags/Machine-learning/" style="font-size: 12.5px;">Machine learning</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/Mpi/" style="font-size: 10px;">Mpi</a> <a href="/tags/Mysql/" style="font-size: 11.25px;">Mysql</a> <a href="/tags/Nginx/" style="font-size: 11.25px;">Nginx</a> <a href="/tags/Python/" style="font-size: 17.5px;">Python</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 15px;">Redis</a> <a href="/tags/Requests/" style="font-size: 10px;">Requests</a> <a href="/tags/Restful/" style="font-size: 12.5px;">Restful</a> <a href="/tags/Scrapy/" style="font-size: 16.25px;">Scrapy</a> <a href="/tags/Spring/" style="font-size: 10px;">Spring</a> <a href="/tags/SpringBoot/" style="font-size: 10px;">SpringBoot</a> <a href="/tags/SpringCloud/" style="font-size: 10px;">SpringCloud</a> <a href="/tags/Token/" style="font-size: 11.25px;">Token</a> <a href="/tags/Urllib/" style="font-size: 10px;">Urllib</a> <a href="/tags/Vue/" style="font-size: 11.25px;">Vue</a> <a href="/tags/Web/" style="font-size: 15px;">Web</a> <a href="/tags/XPath/" style="font-size: 11.25px;">XPath</a> <a href="/tags/python/" style="font-size: 11.25px;">python</a> <a href="/tags/决策树分类/" style="font-size: 10px;">决策树分类</a> <a href="/tags/分布式/" style="font-size: 12.5px;">分布式</a> <a href="/tags/前后端分离/" style="font-size: 13.75px;">前后端分离</a> <a href="/tags/前端/" style="font-size: 10px;">前端</a> <a href="/tags/回归分类/" style="font-size: 10px;">回归分类</a> <a href="/tags/大数据/" style="font-size: 10px;">大数据</a> <a href="/tags/容器/" style="font-size: 11.25px;">容器</a> <a href="/tags/富文本/" style="font-size: 10px;">富文本</a> <a href="/tags/微服务/" style="font-size: 10px;">微服务</a> <a href="/tags/搜索引擎/" style="font-size: 11.25px;">搜索引擎</a> <a href="/tags/数据库/" style="font-size: 10px;">数据库</a> <a href="/tags/数据库集群/" style="font-size: 10px;">数据库集群</a> <a href="/tags/数据结构/" style="font-size: 12.5px;">数据结构</a> <a href="/tags/杂谈/" style="font-size: 10px;">杂谈</a> <a href="/tags/权限认证/" style="font-size: 10px;">权限认证</a> <a href="/tags/正则表达式/" style="font-size: 11.25px;">正则表达式</a> <a href="/tags/消息队列/" style="font-size: 10px;">消息队列</a> <a href="/tags/爬虫/" style="font-size: 20px;">爬虫</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/编码/" style="font-size: 11.25px;">编码</a> <a href="/tags/虚拟化/" style="font-size: 11.25px;">虚拟化</a> <a href="/tags/跨域/" style="font-size: 10px;">跨域</a> <a href="/tags/随机森林/" style="font-size: 10px;">随机森林</a> <a href="/tags/随笔/" style="font-size: 10px;">随笔</a>
        </div>
    </div>
</div>
<div class="search-wrap">
    <span class="search-close">&times;</span>
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
            <i class="icon icon-lg icon-chevron-left"></i>
        </a>
        <input class="search-field" placeholder="Search..." id="keywords">
        <a id="search-submit" href="javascript:;">
            <i class="fa fa-search"></i>
        </a>
    <div class="search-container" id="search-container">
        <ul class="search-result" id="search-result">
        </ul>
    </div>
</div>

<div id="search-tpl">
    <li class="search-result-item">
        <a href="{url}" class="search-item-li">
            <span class="search-item-li-title" title="{title}">{title}</span>
        </a>
    </li>
</div>
<script src="/js/search.js"></script>
<script src="/js/main.js"></script>


  <script src="//cdn.bootcss.com/particles.js/2.0.0/particles.min.js"></script>
  <div id="particles"></div>
  <script src="/js/particles.js"></script>







  <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">
  <script src="//cdn.bootcss.com/scrollReveal.js/3.0.5/scrollreveal.js"></script>
  <script src="/js/animate.js"></script>


  <script src="/js/pop-img.js"></script>
  <script>
     $(".article-entry p img").popImg();
  </script>

  </div>
</body>
</html>